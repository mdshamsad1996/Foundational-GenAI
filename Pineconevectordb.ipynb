{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5459b05-cbcc-4247-b092-82a9172d9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.30 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.0.31)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.37 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.38)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.37->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n",
      "Downloading pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/215.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/215.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 41.0/215.9 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 61.4/215.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ ------------------- 102.4/215.9 kB 454.0 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 153.6/215.9 kB 538.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 153.6/215.9 kB 538.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 153.6/215.9 kB 538.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 153.6/215.9 kB 538.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  215.0/215.9 kB 485.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 215.9/215.9 kB 454.0 kB/s eta 0:00:00\n",
      "Installing collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.2.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002410877E9F0>: Failed to establish a new connection: [WinError 10065] A socket operation was attempted to an unreachable host')': /simple/pinecone-client/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000241085FA7B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pinecone-client/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000241087D59A0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pinecone-client/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000241087D5A60>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pinecone-client/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000241087D5EB0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/pinecone-client/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: openai in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain\n",
    "! pip install pinecone-client\n",
    "! pip install pypdf\n",
    "! pip install openai\n",
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e4fe92-cbca-42d9-90a1-af7350df0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab3cd9f-d2c9-4e7a-ad65-a9a280140690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb90094-178e-4175-869f-0b42b4a69fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043b7023-ab00-4395-8fea-0e0bf7cd7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79aa847-d125-4040-b0cf-3624f94e2181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Compressing YOLOv7\\nBenjamin van Zwienen\\nMaster of Science Thesis\\nNovember 24, 2023\\nStudent Number: 4471180\\nSupervisor: Prof. Dr. Ir. M. Wisse', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d39e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Compressing YOLOv7\\nBenjamin van Zwienen\\nMaster of Science Thesis\\nNovember 24, 2023\\nStudent Number: 4471180\\nSupervisor: Prof. Dr. Ir. M. Wisse'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a6d7b8-1428-40da-aad5-e369d3f08892",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cacad136-e7de-4a33-b2be-cf59ab2f69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc03bacb-4b0d-4df5-ab26-8a070166b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Compressing YOLOv7\\nBenjamin van Zwienen\\nMaster of Science Thesis\\nNovember 24, 2023\\nStudent Number: 4471180\\nSupervisor: Prof. Dr. Ir. M. Wisse', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 0}),\n",
       " Document(page_content='Abstract\\nIn the literature, neural network compression can significantly reduce the number of floating-\\npoint operations (FLOPs) of a neural network with limited accuracy loss. At the same\\ntime, it is common to manually design smaller networks instead of using modern compres-\\nsion techniques. This thesis will compare the two approaches for the object detection network\\nYOLOv7. YOLOv7 can run in real time on a desktop GPU. F or edge GPUs a smaller version,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 2}),\n",
       " Document(page_content='called YOLOv7-tiny , was manually designed by the authors of YOLOv7. This thesis answers\\nthe question: Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than\\nYOLOv7-tiny at the same number of floating-point operations?\\nFirst, two state-of-the-art compression methods are selected and compared on YOLOv7-\\ntiny . Then the best performing method, GBIP , is used to compress YOLOv7 till it has the\\nsame number of FLOPs as YOLOv7-tiny . F rom the experiments it is determined that GBIP', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 2}),\n",
       " Document(page_content='is not able to achieve higher accuracy than YOLOv7-tiny at the same number of FLOPs.\\niii', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 2}),\n",
       " Document(page_content='Contents\\n1 Introduction 1\\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n2 Background 5\\n2.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='2.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2 YOLOv7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.1 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='2.2.3 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3 Compressing Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.3.1 Compression T echniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.3.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.3.3 Benchmark Datasets and Networks . . . . . . . . . . . . . . . . . . . . . 16\\n3 Methods 19', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='3 Methods 19\\n3.1 Compression Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n3.2 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3.2.1 Output T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n3.2.2 Attention T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n3.2.3 Adversarial Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='3.2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.3 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n3.3.1 Kernel Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n3.3.2 Kernel Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n3.3.3 Kernel Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='3.3.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4 YOLOv7-tiny Experiments 27\\n4.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.2 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.2.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n4.2.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='4.3 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.3.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n4.3.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nv', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 4}),\n",
       " Document(page_content='5 Selecting the Best Method 31\\n5.1 Compression Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.1.1 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n5.1.2 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2 Best Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2.1 Comparing GBIP and KSE . . . . . . . . . . . . . . . . . . . . . . . . . 32', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 5}),\n",
       " Document(page_content='5.2.2 PyT orch optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n5.2.3 Selecting Best Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n6 YOLOv7 Experiments 37\\n6.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n6.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n6.2.1 Attention T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 5}),\n",
       " Document(page_content='6.2.2 Pruning Batch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n6.2.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n6.2.4 Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n7 Results 41\\n7.1 Final Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\n7.2 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n8 Conclusion 45', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 5}),\n",
       " Document(page_content='8 Conclusion 45\\n8.1 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.1.1 Main Research Question . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.1.2 Sub Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n8.2 Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nA Architectures 49\\nA.1 YOLOv7-tiny . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 5}),\n",
       " Document(page_content='A.2 YOLOv7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\nvi', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 5}),\n",
       " Document(page_content='1 Introduction\\n1.1 Motivation\\nIn the last decade, the performance of neural networks has increased dramatically . They\\nhave become state of the art in a wide range of areas, such as image classification, object\\ndetection and localization, and speech recognition [ 1,2]. A lot of the improvements have come\\nthrough creating larger and more complex models. Besides a larger memory footprint, complex\\nneural networks take longer to train and run inference and have higher energy consumption.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='Therefore, neural networks are usually run on servers with high-end GPUs, allowing for the\\nincreased computational demand needed for increased accuracy [ 3].\\nThere are a lot of edge devices, like smartphones, home automation devices and robots,\\nwhich could also benefit from large neural networks. There are two paradigms for providing\\nmachine learning to these devices: cloud computing and edge computing [ 4]. In the first case,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='the edge device sends its data to a server with powerful hardware, which runs neural network\\nand sends back the result [ 5]. In the latter case, all the computation is handled on the edge\\ndevice itself.\\nThere are several drawbacks to cloud computing since all data must be transmitted to\\na server [ 6,7]. Especially for edge devices with a lot of sensors, the network bandwidth will\\nbecome a problem. F or example, an autonomous car is estimated to produce 3 Gbit of data', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='per second [ 8]. F or real time applications, the latency of the connection might be too high to\\nfunction at the desired speed. Also, in safety-critical situations, the internet connection must\\nbe extremely reliable. And lastly , certain edge devices, especially health monitoring devices,\\ncollect sensitive, personal data which people might not want uploaded to a server.\\nAlthough cloud computing has its drawbacks, edge computing is not always the obvious', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='choice. Edge devices are typically constrained in terms of memory , computation and energy\\nusage. This makes running large neural networks on these devices infeasible. Assuming large\\nneural networks are required, upgrading the hardware or using cloud computing are reasonable\\noptions. However, there has also been a lot of research into creating smaller neural networks\\nthat can deal with the constraints of edge devices [ 5,9,10]. The creation of smaller networks', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='can be split into two categories: designing new, compact architectures and compressing existing\\narchitectures.\\nCompression of existing neural networks is based on the observation that large networks\\nonly need a part of their parameters for accurate predictions [ 11]. The amount of compression\\ndepends on the network and dataset, with a reduction in floating-point operations (FLOPs) of\\nmore than 99% for LeNet-5 on MNIST [ 12] and up to 89% for ResNet-56 on CIF AR-10 [ 13],', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='all while the accuracy loss stays below 1%.\\nCompact architectures are diﬀicult to compare to other compressed networks since they\\ntypically do not have a clear baseline. F or example, MobileNet [ 14] loses less than 1% accuracy\\nwith only 4% of FLOPs compared to VGG-16. However, MobileNet is not a smaller version\\nof VGG-16, so it might just as well be compared with any other neural network. On the other\\nhand, there are some architectures that could be compared, like the different ResNets [ 15].', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='Maybe compressing ResNet-110 will lead to a model with higher accuracy and lower FLOPs\\nthan ResNet-50. No such comparison, between compact and compressed networks, has been\\nfound in the literature.\\n1', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 6}),\n",
       " Document(page_content='1.2 Research Questions\\nGiven the promising results of neural network compression, and the lack of comparison between\\ncompressed and compact networks, this thesis will focus on comparing these two for the case\\nof YOLOv7 [ 16]. YOLOv7 is one of the best performing object detection networks. But given\\nits size, it is meant to run on a powerful GPU. The creators of YOLOv7 have hand-designed\\nYOLOv7-tiny especially for the use on edge devices. The goal of this thesis is to see if a', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='state-of-the-art compression method will be able to compress YOLOv7 more eﬀiciently than\\nthe hand-designed YOLOv7-tiny .\\nMain Research Question\\nThe main research question of this thesis is:\\nCan a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-\\ntiny at the same number of floating-point operations?\\nBased on the compression literature, it seems plausible that a compression method should\\nbe able to outperform YOLOv7-tiny .\\nSub Questions', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='Sub Questions\\nT o answer the main research question, several sub questions will be answered throughout this\\nthesis. These sub questions are listed and motivated below:\\n1. What is the state of the art in neural network compression?\\nIt is important to understand what compression techniques exist and how they work, as well\\nas the state-of-the-art performance.\\n2. What are the best networks for comparing compression methods?', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='T o make a fair comparison between compression methods, they should be evaluated on the\\nsame networks. It is, therefore, important to determine which networks are most suited for\\nthis comparison.\\n3. Which compression method is best suited for compressing YOLOv7?\\nBased on the latest research, two of the best methods will be assessed on YOLOv7-tiny to see if\\nthey can be adapted to the YOLOv7 architecture and select the best performing method. An', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='important constraint on the compression methods for this thesis is the amount of finetuning\\nrequired. There is no hard limit, but the compression method should be able to work on\\na standard GPU and complete within hours or days rather than weeks. This is also why\\nYOLOv7-tiny is initially used to compare methods. YOLOv7-tiny runs and trains much faster,\\nwhile the architecture is very similar to YOLOv7. Typically , both the reduction in FLOPs', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='and parameters is reported for the compression of neural networks. This is also the case in\\nthis thesis. However, object detection is usually run in real-time, which makes speed, and thus\\nFLOPs, the key factor.\\n4. What are the optimal hyperparameters to compress YOLOv7?\\nThe best performing method on YOLOv7-tiny will be used to compress YOLOv7 till it reaches\\nthe same speed as YOLOv7-tiny , as measured by the number of FLOPs. T o achieve the best', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='result, the hyperparameters of this method have to be adapted to YOLOv7.\\n2', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 7}),\n",
       " Document(page_content='5. Does one large pruning step work better than several smal ler pruning steps?\\nThe selected compression method runs multiple pruning steps. Therefore, this thesis will also\\ntest if several small pruning steps or one large step achieves better results.\\n6. Why is the selected compression method not able to outperform YOLOv7-tiny?\\nSurprisingly , it turns out the compressed YOLOv7 model is not able to outperform YOLOv7-\\ntiny . The results have to be examined, to see why this is the case.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 8}),\n",
       " Document(page_content='1.3 Outline\\nChapter 2will provide the background knowledge for this thesis: a short introduction to neural\\nnetworks and YOLOv7, is followed by an extensive overview of neural network compression.\\nChapter 3will explain the search method and the selected compression methods. These meth-\\nods will then be adapted for YOLOv7-tiny , which is the topic of Chapter 4, as well as the\\nproposed experiments. In Chapter 5the results of these experiments are given, followed by', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 8}),\n",
       " Document(page_content='the selection of the best method for YOLOv7.\\nThe adaptation of the selected method to YOLOv7, including the optimization of the\\nhyperparameters, and the experiments are discussed in Chapter 6. The results are given in\\nChapter 7. Based on these results, the research questions will be answered in Chapter 8, and\\nrecommendations for future research are given.\\n3', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 8}),\n",
       " Document(page_content='2 Background\\nThis chapter will provide the background for the rest of the thesis. It starts by introducing\\nthe basics of neural networks in Section 2.1 . Section 2.2 will explain YOLOv7, the neural\\nnetwork used in this thesis. A deep dive into the compression of neural networks is presented\\nin Section 2.3 .\\n2.1 Neural Networks\\nThe notation used in the rest of the thesis will be introduced in this section. F ollowed by a\\nbrief overview of object detection and the relevant metrics.\\n2.1.1 Basics', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 10}),\n",
       " Document(page_content='2.1.1 Basics\\nThis thesis assumes a basic understanding of neural networks. Here, the activation functions\\nused in YOLOv7 are introduced, as well as the notation for a convolutional layer. Different\\npapers use slightly different notations, thus it is important to explicitly specify the notation\\nused in this thesis.\\nActivation functions\\nActivation functions introduce nonlinearity to a neural network. They are usually added after', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 10}),\n",
       " Document(page_content='each fully-connected or convolution operation. F or a long time, the most common activation\\nfunction was the Rectified Linear Unit (ReLU) [ 17]. In the past years more complex functions\\nhave been proposed [ 18]. One such activation function, which is used in YOLOv7, is the\\nSigmoid Linear Unit (SiLU) [ 19].\\nAnother common activation function is the sigmoid function. In case of YOLOv7, it is\\nused in the final layer. Given that the output of the sigmoid function is between 0 and 1,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 10}),\n",
       " Document(page_content='it can be interpreted as a probability . The equations for these three activation functions are\\ngiven in Equations 2.1 -2.3 and are plotted in Figure 2.1 .\\nfReLU (x) = max(0, x) (2.1)\\nfsigmoid (x) =σ(x) =1\\n1 +e−x(2.2)\\nfSiLU(x) =xσ(x) (2.3)\\nConvolutional Layer\\nThe operation of a convolutional layer can be expressed as:\\nYn=f CX\\nc=1Wn,c∗Xc+bn!\\n(2.4)\\n5', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 10}),\n",
       " Document(page_content='Figure 2.1 :Activation functions.\\n(a) Example 1\\n (b) Example 2\\nFigure 2.2 :Output of an object detection network for two images from the MS COCO dataset.\\nDetected objects are classified and located with a bounding box.\\nWith output Y∈RN×Hout×Wout , weights W∈RN×C×Kh×Kw, input X∈RC×Hin×Win\\nand bias b∈RN. Where N is the number of output channels, C the number of input channels,\\nW andH the width and height of the channels, KH andKW the height and width of the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 11}),\n",
       " Document(page_content='convolution kernels, and fthe activation function.\\nThe input and output channels can also be referred to as input and output feature maps.\\nThough in some cases the output feature maps only refer to the direct result of the convolution\\nbefore the activation function is applied. In this thesis, the output feature maps will refer to\\nthe post-activation feature maps, unless explicitly mentioned otherwise.\\nThe weights W of a convolutional layer can be split into N 3D filters. Each of these filters', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 11}),\n",
       " Document(page_content='convolved with the input X corresponds to one of the output feature maps Yn.\\n2.1.2 Object Detection\\nObject detection is the task of locating and classifying objects in an image. F or each object,\\nthe network should output a bounding box that fits closely around the object and produce a\\nclassification for that object. Figure 2.2 shows the output of an object detection network for\\ntwo different images.\\nThis subsection provides background on the metrics, datasets and models used for object', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 11}),\n",
       " Document(page_content='detection. The metrics and datasets are used for YOLOv7, while a short overview of the\\nexisting models provides context.\\n6', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 11}),\n",
       " Document(page_content='(a) Intersection\\n (b) Union\\n (c) IoU\\nFigure 2.3 :Intersection over Union (IoU) of a predicted bounding box, bbox pred , and the\\nground truth, bbox gt.\\nMetrics\\nT o determine how well an object detection network is performing, first the proposed bounding\\nboxes must be evaluated. This is done using the Intersection over Union (IoU) metric:\\nIoU =bbox gt∩bbox pred\\nbbox gt∪bbox pred(2.5)\\nThis measures how close the predicted bounding box, bbox pred , matches with the ground', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 12}),\n",
       " Document(page_content='truth, bbox gt. Figure 2.3 visualizes the IoU equation. The IoU is zero when the bounding\\nboxes have no overlap, and one when the bounding boxes are identical.\\nPrecision and recall are defined in terms of T rue Positives (TP), F alse Positives (FP) and F alse\\nNegatives (FN):\\nPrecision =TP\\nTP+FP(2.6)\\nRecall =TP\\nTP+FN(2.7)\\nIt is unrealistic to expect all predicted bounding boxes to match perfectly with the ground\\ntruth. However, they must match to some extent. This extent is measured with the IoU', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 12}),\n",
       " Document(page_content='metric. T o compute the number of true and false positives for the precision and recall, only\\nthe predicted bounding boxes are used that have an IoU above a certain threshold. This\\nthreshold is typically set to a value of 0.5 or higher.\\nThe precision is largest when there are no false positives, while the recall is largest when\\nthere are no false negatives. T o avoid false positives fewer bounding boxes could be proposed,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 12}),\n",
       " Document(page_content='but this creates more false negatives. Thus, there is a tradeoff between precision and recall.\\nThis tradeoff can be captured in a precision-recall curve p(r). This curve is created\\nby ordering all predictions from highest to lowest confidence and computing intermediate\\nprecision and recall from top to bottom. At high confidence, the precision will be high since\\nit is unlikely to be very certain about an incorrect prediction. At the same time, the number', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 12}),\n",
       " Document(page_content='of false negatives will be higher if lower confidence predictions are ignored. An example of a\\nprecision-recall curve is shown in Figure 2.4 .\\nFinally , based on an interpolated version of the precision-recall curve (Figure 2.4 ), the most\\ncommon metric for evaluating object detection networks, namely A verage Precision (AP), can\\nbe computed. The interpolated precision-recall curve pinterp (r)is given as:\\n7', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 12}),\n",
       " Document(page_content='Figure 2.4 :Precision-recall curve p(r)and interpolation pinterp (r). The colored area under\\npinterp (r)is the A verage Precision (AP), which is the metric used in this thesis. Adapted\\nfrom [ 20].\\npinterp (r) = max\\n˜r≥rp(˜r) (2.8)\\nThe AP is simply defined as the area under the interpolated precision-recall curve. The\\narea is computed by sampling pinterp (r)at 101 points:\\nAP=1\\n101100X\\ni=0pinterp (0.01i) (2.9)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 13}),\n",
       " Document(page_content='Since the definition of AP includes precision and recall, it requires an IoU threshold. F or\\nexample, AP 50 refers to the AP at an IoU threshold of 0.5. The AP can also be an average at\\ndifferent IoU thresholds. AP 50:95 refers to the average of AP 50, AP 55, ..., AP 95. Each object\\nclass has its own AP , for example APcaror APperson, but typically only the average over\\nall classes is reported. This is called the mean A verage Precision, mAP . F ollowing common', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 13}),\n",
       " Document(page_content='notation, in this thesis, unless specifically stated otherwise, the mean A verage Precision at\\nIoUs from 0.5 to 0.95, mAP 50:95 , will be referred to as simply AP .\\nDatasets\\nThe two most common datasets for training and benchmarking object detection networks are\\nP ASCAL VOC [ 21] and MS COCO [ 22]. P ASCAL VOC is an older dataset with oﬀicial\\ncompetitions from 2005 to 2012. The 2012 version of the dataset has 20 classes and consists', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 13}),\n",
       " Document(page_content='of 6k training, 6k validation and 11k test images. The best reported result on P ASCAL VOC\\nachieves an AP of 97.2% [ 23], which makes it diﬀicult to show any further improvement. Most\\nnewer methods do no longer report results for P ASCAL VOC.\\nInstead, MS COCO has become the default benchmarking dataset for object detection,\\nwith competitions from 2015 to 2020. The dataset was last updated in 2017 and now con-', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 13}),\n",
       " Document(page_content='tains 118k training, 5k validation and 41k test images. Objects have to be classified into 80\\ncategories, which include the same 20 categories as P ASCAL VOC. Compared to P ASCAL\\nVOC, MS COCO contains objects at all different scales, including a large number of small ob-\\njects [ 24]. This makes MS COCO a more challenging dataset than P ASCAL VOC. Currently\\nthe best reported result on MS COCO is an AP of 66.0% [ 25].\\n8', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 13}),\n",
       " Document(page_content='Models\\nThere are two types of deep learning approaches for object detection: two-stage and one-stage\\nobject detectors [ 26]. T wo-stage networks split the detection of objects into two parts: (i)\\nfind regions in an image where objects may be present, (ii) classify these region proposals.\\nOne-stage object detectors do not use region proposals, but instead predict all bounding boxes\\nand classifications in one go.\\nA common example of a two-stage detector is F aster R-CNN [ 27] which builds on F ast', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='R-CNN [ 28]. With F aster R-CNN the region proposal network and the classifier network\\nare not separated. Firstly , a set of feature maps is generated from an input image using\\nconvolutional layers. Then, based on these feature maps the region proposals are generated.\\nFinally , the classifier takes the feature maps and looks at the regions that were proposed.\\nT wo-stage detectors are usually slower since they must first find region proposals and then run', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='a classifier for all these regions. Usually , the increase in inference time will be compensated by\\nan accuracy increase [ 29]. However, for real-time applications this might not be a beneficial\\ntradeoff.\\nF or this reason, one-stage detectors are more likely to be used in real-time applications.\\nCommonly referenced one-stage detectors are SSD [ 30] and different YOLO versions. Both\\nSSD and F aster R-CNN were introduced in 2015, and are no longer state-of-the-art, achieving', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='28.8% and 34.9% AP on MS COCO, respectively . The original YOLO [ 31] network was also\\nintroduced in 2015, but newer versions, like YOLOv7 [ 16], are still in use. YOLOv7 was\\nspecifically designed for real-time applications and runs 10 times faster than most other non-\\nYOLO methods. It achieves 51.4% AP on MS COCO. As mentioned above, currently the best\\nreported result on MS COCO is 66.0% AP . Thus, YOLOv7 should not be used if very high', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='accuracy is needed, and slower inference time is acceptable.\\n2.2 YOLOv7\\nThis thesis deals with the compression of YOLOv7. An explanation of this object detection\\nneural network is given in this section. Subsection 2.2.1 deals with the structure of YOLOv7\\nand its different versions. Next, the computation of the output and its interpretation is dis-\\ncussed in Subsection 2.2.2 . And lastly , the training of YOLOv7 is described in Subsection 2.2.3 .\\n2.2.1 Structure\\nVersions', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='Versions\\nThere are three different YOLOv7 versions for three different types of GPU: YOLOv7-tiny\\nfor edge GPUs, YOLOv7 for normal desktop GPUs and YOLOv7-W6 for cloud GPUs. They\\nall share the same type of architecture but are specifically designed to run in real-time on\\nthe respective hardware. T able 2.1 shows the difference in number of parameters and FLOPs,\\nthe speed at which each version runs on a NVIDIA V100 GPU, and the achieved AP on MS', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='COCO. The tradeoff between speed and accuracy is clear, with YOLOv7-tiny optimizing for\\nspeed at the cost of accuracy . YOLOv7-tiny and YOLOv7 are evaluated with input images\\nresized to 640x640, while YOLOv7-W6 is designed for input images of 1280x1280. F or the rest\\nof this thesis all reported results are obtained using 640x640 images.\\n9', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 14}),\n",
       " Document(page_content='T able 2.1 :Comparison of\\nthe different YOLOv7\\nversions [ 16]. Reported\\nFPS is achieved on a\\nNVIDIA V100 GPU.V ersion #Params #FLOPs FPS AP\\nYOLOv7-tiny 6.2M 13.8G 286 38.7\\nYOLOv7 36.9M 104.7G 161 51.4\\nYOLOv7-W6 70.4M 360.0G 84 54.9\\nInformation Flow\\nA simplified overview of the YOLOv7 architecture is shown in Figure 2.5 . The numbers on the\\nleft indicate the scale at that level. F or example, at the top (32x) the input image has been', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='downsampled from a high resolution of 640x640 to a low resolution of 20x20. The different\\nscales make it easier to focus on different sized objects. The arrows show how the information\\nflows through the network to the three output layers, it has a bottom-to-top pathway as well\\nas a top-to-bottom and another bottom-to-top pathway . These are called feature pyramids\\nand were introduced in [ 32]. The upper layers contain more semantic information, but due to', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='the lower resolution lack the precise localization. By allowing information to flow between the\\ndifferent scales, the semantic information can be combined with the more precise localization\\nfrom the higher resolution, resulting in better object detection.\\nFigure 2.5 :Simplified overview of the\\nYOLOv7 architecture showing the\\nflow of information. Numbers on\\nthe left indicate the scale: an input\\nimage of 640x640 will be reduced to\\n20x20 at the top (640/32 = 20).\\nELAN', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='ELAN\\nMost of the convolutional layers are built up from ELAN (Eﬀicient Layer Aggregation Net-\\nworks) blocks [ 33]. Stacking more and more layers in a neural network leads to reduced\\naccuracy increase, and after a point, it will actually reduce the overall accuracy . ELAN is\\ndesigned to solve this problem by reducing the length of the shortest gradient path. Fig-\\nure 2.6 shows different configurations of an ELAN block. ELAN 2-4 and ELAN 1-4 are used', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='in YOLOv7, while the smaller ELAN 1-2 is used in YOLOv7-tiny .\\nDetection Head\\nYOLOv7(-tiny) contains a detection head at three different scales (8x, 16x, 32x). These heads\\nconsist of two convolution layers, where the last layer has 255 output channels. Thus, in\\nthe case of a 640x640 input image, the output sizes are: 80x80x255 (8x scale), 40x40x255\\n(16x scale) and 20x20x255 (32x scale). The next subsection will explain how the output is', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='interpreted to get to bounding boxes and classifications of objects in the image.\\n10', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 15}),\n",
       " Document(page_content='(a) ELAN 2-4\\n (b) ELAN 1-4\\n (c) ELAN 1-2\\nFigure 2.6 :Different ELAN block configurations. Numbers under ’Conv’ indicate kernel size\\nand number of output channels.\\n2.2.2 Output\\nAs mentioned in the previous subsection, the output of YOLOv7(-tiny) consists of three tensors\\nwith sizes: 80x80x255, 40x40x255 and 20x20x255. F or now, only 20x20x255 will be considered.\\nThe first two dimensions can be interpreted as a grid on the original input image. F or each', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 16}),\n",
       " Document(page_content='cell in this 20x20 grid three bounding boxes are predicted. Each of these bounding boxes is\\ncharacterized by 85 values (three bounding boxes: 3x85 = 255). The first four of these values\\ndetermine the location of the bounding box, the fifth is the object confidence score, and the\\nlast 80 values are the class probabilities. Thus, if a different dataset is used, with a different\\nnumber of classes, the number of output channels must change to match.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 16}),\n",
       " Document(page_content='At each scale three anchors are defined, that are boxes of a specific width and height.\\nInstead of predicting the width, height and position of the bounding boxes, YOLOv7 predicts\\nhow the anchors should be scaled and translated to achieve the optimal bounding box. Fig-\\nure 2.8 shows an example of three anchors centered inside a cell. These anchors are identical\\nfor all cells at the same scale. The predicted values ( tx, ty, tw, th) can then be transformed into', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 16}),\n",
       " Document(page_content='the actual bounding box with the following equations:\\nbx= 2·σ(tx)−0.5 +cx (2.10)\\nby= 2·σ(ty)−0.5 +cy (2.11)\\nbw=\\x00\\n2·σ(tw)\\x012·pw (2.12)\\nbh=\\x00\\n2·σ(th)\\x012·ph (2.13)\\nHere bxandbygive the center position of the bounding box. cxandcyare the offsets of\\nthis cell from the top left of the grid. F or the third cell of the left at the top row, cx= 2 and\\ncy= 0 . The width and height of the bounding box, bwandbh, are based on the width and', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 16}),\n",
       " Document(page_content='height of the anchor, pwandph. Finally , all these values must be multiplied by the scale, also\\ncalled stride, to go from the grid to input image position. The scale is simply the number of\\npixels of the input image that are represented in one cell.\\nThe 80 class probabilities for each bounding box can be interpreted in exactly the same\\nway as with a standard classification network. The bounding box will be assigned to the class\\nwith the highest probability .\\n11', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 16}),\n",
       " Document(page_content='Now with three predicted bounding boxes at each cell, the total number of predictions\\nis 25200 ( (20×20 + 40 ×40 + 80 ×80)×3). Clearly , most of these bounding boxes will not\\nactually contain an object. That is why for each bounding box an object confidence score is\\npredicted. As the name implies, the confidence score indicates how confident the network is\\nthat this bounding box contains an object. Multiplying the object confidence score with the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='probability of the assigned class, gives the overall confidence score or the estimated probability\\nthat an object with this specific class is present in the bounding box.\\nNon-maximum suppression (NMS) is used to remove all low-confidence predictions and\\npredictions with high overlap. First, all predictions with an overall confidence score below\\na certain threshold (typically 0.25) are removed. Next, for all bounding boxes that have an', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='IoU with another box above a threshold (0.65 for YOLOv7), the lower confidence prediction\\nis removed. This last step is repeated till there are no bounding boxes with an overlap above\\nthe threshold. Thus, NMS takes in 25200 predictions and will typically only return a handful\\n(e.g., two in the case of Figure 2.2a ).\\nFigure 2.7 :Cosine annealing.\\nFigure 2.8 :Anchor boxes\\n(dashed) centered in cell\\n(solid black).\\n2.2.3 T raining\\nData Augmentation', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='Data Augmentation\\nYOLOv7 is trained and evaluated on the MS COCO dataset. Different types of data augmenta-\\ntion are used to avoid overfitting. Firstly , Mosaic augmentation (introduced by YOLOv4 [ 34])\\nplaces 4 or 9 images side by side into one combined input image. Then MixUp [ 35] and\\nCutout [ 36] are applied with a probability of 15%. MixUp combines two images (with Mosaic\\nalready applied) into one using a weighted average of both images. Cutout randomly masks', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='out square sections of the image. Next, 50% of the images is flipped left to right. Finally ,\\nsome noise is applied to the hsv color values.\\nLoss\\nThe loss of YOLOv7 consists of three parts: box, objectness and classification loss. The box\\nloss is the CIoU loss [ 37] between the target bounding box and the prediction. The CIoU loss\\nis similar to IoU but also penalizes the distance between the center points of both boxes and', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='the deviation of the aspect ratios. The objectness score should be zero if there is no object\\nin the bounding box, one if it exactly fits the target, and have the same value as the IoU in\\ncase of partial overlap. The objectness loss is then calculated as the cross-entropy between\\nthis target value and the actual predicted score. The classification loss is also implemented as\\na cross-entropy loss between the predicted distribution and the actual class.\\n12', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 17}),\n",
       " Document(page_content='Evaluation\\nThe main evaluation metric for MS COCO is the AP (abbreviated from mAP 50:95 ). A detailed\\nexplanation of how the AP is calculated is given in Subsection 2.1.2 . F or YOLOv7 the main\\nevaluation metric, which is used to determine if the network accuracy is increasing, is called\\nfitness:\\nfitness = 0.1·AP50+ 0.9·AP (2.14)\\nThis metric is also used in the rest of this thesis to evaluate the compressed models.\\nLearning Rate Schedule', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='The learning rate is slowly decreased during training using cosine annealing. Figure 2.7 shows\\nthe learning rate factor going from 1.0 to 0.01 in 10 epochs. The actual learning rate for each\\nepoch is the start learning rate multiplied by the learning rate factor.\\n2.3 Compressing Neural Networks\\nCompression techniques can be categorized into four different types. Section 2.3.1 will intro-\\nduce these categories, followed by an overview of each category . The evaluation metrics to', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='compare different compression methods are explained in Section 2.3.2 . Section 2.3.3 will give a\\nbrief overview of the different datasets and networks commonly used for benchmarking. These\\ndatasets and networks are needed for comparing and selecting compression methods.\\n2.3.1 Compression T echniques\\nDifferent papers use different ways to categorize compression techniques, or use different sub-\\ncategories [ 5,9,10]. In this thesis they are categorized into: (i) pruning, (ii) quantization,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='(iii) tensor decomposition and (iv) compact architectures. T ensor decomposition includes the\\ndecomposition of both fully-connected and convolutional layers.\\nPruning\\nNetwork pruning is based on the Minimal Description Length (MDL) principle, which states\\nthat the best model for describing a dataset is the one that leads to the highest compression [ 5].\\nT ogether with the observation that large networks only need a fraction of their parameters for', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='accurate prediction [ 11], pruning methods try to find unimportant parameters and remove them\\nfrom the network or set them to zero. Besides compression, pruning also has a regularization\\neffect on the network [ 38].\\nA common criterion for pruning a parameter is magnitude-based, pruning parameters\\nwhose weights are below a certain threshold. This is generally used in combination with L 2\\nor L 1regularization, which will force parameters that have a small impact on the network’s', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='performance close to zero [ 5]. The L 2norm mainly pushes the value of larger weights down,\\nwhile the L 1norm can achieve some sparsity . The L 0norm would induce even more sparsity ,\\nbut since it is not differentiable, cannot be used for neural network training with gradient\\ndescent [ 39].\\nPruning can be categorized into structured and unstructured methods. Unstructured methods\\nwill prune individual weights, while structured methods will prune complete neurons or entire\\n13', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 18}),\n",
       " Document(page_content='(a) shape-wise\\n (b) row-wise\\n (c) column-wise\\n (d) channel-wise\\n(e) stack shape-wise\\n (f) stack row-wise\\n (g) stack column-wise\\n (h) filter-wise\\nFigure 2.9 :Types of structured sparsity in 3D filters. Blue squares indicate weights to be\\npruned. Sparsity types (a)-(d) can also be used for fully-connected layers. Note: each\\nconvolutional layer includes many filters, but only one is shown for simplicity .', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='filters. Unstructured pruning requires specific software to take full advantage of the sparsity ,\\nwhile structured pruning completely removes certain parts of the network, resulting in a more\\ncompact architecture, with fewer FLOPs, which can be run directly without the need for\\nspecialized software [ 40]. Figure 2.9 shows the different kind of structures that can be used\\nfor sparsity in convolutional layers. Figure 2.9a shows no structure at all and 2.9h is fully', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='structured. Figure 2.9b -2.9g could be classified as semi-structured, which does induce some\\nstructure but still requires additional optimization. Sparsity types 2.9a -2.9d can also be applied\\nto the 2D weights of fully-connected layers. In that case, 2.9b -2.9d would be considered fully\\nstructured, which directly reduces the model size.\\nPruning methods using structured sparsity can lead to a more compact architecture, but', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='not necessarily to a smaller file size, since they usually do not prune the individual weights in\\nthe remaining layers. Therefore, methods using structured sparsity may achieve less compres-\\nsion but end up with a model that uses fewer FLOPs and thus runs faster.\\nQuantization\\nQuantization reduces the number of bits used to represent the value of each parameter. [ 41]\\nshows that neural networks are resistant to certain amounts of low precision. There are a lot', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='of different quantization methods. The standard way is to reduce the precision used for each\\nparameter. F or example, when an 8-bit format is used, instead of the standard 32-bit format,\\nthe model size is reduced by a factor of four. In the extreme case the network can be binarized,\\nusing only one bit for each parameter [ 42].\\nThe previous approach uses linear quantization, but the weights of neural networks are', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='not uniformly distributed [ 43]. One way to use nonlinear quantization is k-means clustering,\\nwhich can be done during [ 11] or after [ 44] training. When one of the clusters is located at\\nzero, this effectively prunes all parameters assigned to this cluster.\\n14', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 19}),\n",
       " Document(page_content='An important insight of [ 45] is the fact that pruning and quantization can compress neural\\nnetworks without interfering with each other. Combining the two techniques can therefore lead\\nto higher compression with little to no extra accuracy loss.\\nT ensor Decomposition\\nT ensor decomposition is a technique to approximate the full weight matrix of a fully-connected\\nlayer or the filter kernels of a convolutional layer with a low-rank approximation. T ensor de-', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='composition methods include T runcated Singular V alue Decomposition [ 46], T ucker Decom-\\nposition [ 47], Canonical Polyadic Decomposition [ 48] and T ensor T rain Decomposition [ 49].\\nThere are several disadvantages to this approach [ 40]. Firstly , it is not always obvious which\\nrank should be used. Secondly , the decomposition operation is computationally expensive.\\nAnd finally , the factorized neural network converges slower, meaning that extensive retraining', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='is required. However, most compression methods require fine-tuning and a lot of extra com-\\nputation. Which is usually not a problem since this can be done using powerful computers.\\nCompact Architectures\\nThe above-mentioned techniques change the original network to reduce its size. Another\\ncommon technique is the use of compact architectures. These networks are not changed during\\nor after training but are specifically designed to be compact. The most well-known networks', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='are MobileNet [ 14] and SqueezeNet [ 50]. Another option is using knowledge distillation. With\\nthis approach a large network, or an ensemble of large networks, which acts as teacher is fully\\ntrained without any compression. Then, a much smaller student network is trained using both\\nthe actual dataset the teacher was trained on, as well as the dark knowledge [ 51] (softmax\\noutput) of the teacher. The extra knowledge from the teacher allows the student to minimize\\nthe accuracy loss [ 52].', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='Typically , the softmax output of both the teacher and student are softened using temper-\\nature T before computing the loss [ 51]. The output is then calculated as:\\nqi=exp(zi/T)P\\njexp(zj/T)(2.15)\\nWhere inputs ziare converted to qi. Note that if T= 1 , this results in the standard\\nsoftmax function.\\n2.3.2 Evaluation Metrics\\nThere are a lot of possible evaluation metrics, but most metrics are related to accuracy , number', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='of parameters and number of floating-point operations (FLOPs) [ 5,9,10]. Since the comparison\\nof compression methods is done using similar networks and datasets, it makes sense to use the\\nrelative metrics accuracy loss and compression ratio.\\nThe accuracy loss can be used as a constraint when searching for the right compression\\nmethod. In certain applications an accuracy loss of up to 10% might be acceptable, while in\\nother applications a loss of just 1% may cause significant problems [ 53].', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='It is important to note that the number of parameters is not linearly correlated with the\\nnumber of FLOPs [ 40]. Convolutional filters do not need many parameters, but depending on\\nthe image size, will require a lot of FLOPs. And the opposite is true for fully-connected layers.\\nThese layers take up a lot of the parameters but contribute little to the number of FLOPs.\\n15', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 20}),\n",
       " Document(page_content='Depending on the way the parameters are saved, the model size saved on disk might not\\nbe linearly correlated with the number of parameters. This is obvious when the parameters\\nare saved with different precision but could also happen when a different data structure is used\\nto manage sparse tensors. In the latter case, the differences are usually negligible, but when\\navailable both the number of parameters and the model size should be used for comparison\\nbetween different methods.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='In the literature it is common to calculate the compression ratio for the number of FLOPs\\n(or parameters) as the original number of FLOPs divided by the number of FLOPs in the\\ncompressed network. The higher the compression ratio, the fewer parameters an increase in\\ncompression ratio will prune. F or example, the difference between a compression ratio of 10\\nand 20 is 5% of the parameters, but the difference between a compression ratio of 100 and 110', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='is below 0.1%. Given this nonlinearity , a better way to represent compression ratios seems to\\nbe the percentage of parameters or FLOPs remaining after compression. In this thesis this will\\nbe referred to as the FLOPs Pruning Ratio (FPR) and Parameter Pruning Ratio (PPR). The\\nway it is defined, a FPR of 100% means no compression, so a lower FPR and PPR is better.\\n2.3.3 Benchmark Datasets and Networks\\nT o make fair comparisons between different compression methods, as many external factors as', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='possible should remain constant. The most important factors are the dataset and the network\\nthat are used to evaluate the compression method. F or example, using a large network will\\ngenerally allow for more compression compared to a small network for the same task. Similarly ,\\na simple binary classification task will need a smaller network than a complicated classification\\ntask with a thousand classes, and thus allow for more compression.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='F ortunately , there are several common datasets and networks that are often used for\\nbenchmarking. In this section we discuss the ImageNet [ 54] and CIF AR-10 [ 55] datasets. Be-\\nsides these datasets, we will briefly introduce the following networks: ResNet-50 and ResNet-56\\n[15]. Typically , ResNet-50 is used for ImageNet and ResNet-56 for CIF AR-10.\\nImageNet + ResNet-50\\nImageNet is a large dataset of over 14 million images with almost 22 thousand classes. A', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='popular subset comes from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\\nIt is sometimes called ImageNet-1k or ILSVRC2017, but given its popularity is usually just\\nreferred to as ImageNet.\\nThis subset includes a thousand classes and is already split up in training, validation and\\ntest sets. It contains roughly 1.3 million training, 50 thousand validation and 100 thousand\\ntest images. The size of the images varies, but it is common to resize all images to 224 by', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='224 pixels. Currently , the best accuracy achieved on ImageNet-1k (from now on referred to as\\nImageNet) is 91.1% [ 56].\\nFigure 2.10 :ResNet building block [ 15].The ResNet architecture was introduced\\nin 2015 as a solution to the degradation prob-\\nlem. Namely , the problem that increasing\\nthe depth of a neural network only increases\\nthe accuracy up to a point, after which it de-\\ngrades rapidly [ 15]. ResNets consist of resid-\\nual building blocks. The input of each block', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='is directly added to the output of one or more\\nconvolutional layers (see Figure 2.10 ), this\\nway each block can learn a residual function\\n16', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 21}),\n",
       " Document(page_content='F(x). It is shown that this solves the degra-\\ndation problem, since by pushing the weights\\nof certain layers to zero, it easily mimics a smaller network if necessary . While at the same\\ntime more building blocks allow for more residual refinement of the network output, resulting\\nin better accuracy .\\nThe number in the name indicates the number of layers, both fully-connected and convo-\\nlutional layers, in the neural network. ResNet-50 consists of a convolutional layer, 16 building', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 22}),\n",
       " Document(page_content='blocks with 3 convolutional layers each, and a fully-connected layer outputting 1000 probabil-\\nities for the ImageNet classes. ResNet-50 achieves an accuracy of 75.3% on ImageNet.\\nCIF AR-10 + ResNet-56\\nCIF AR-10 is a dataset of 50 thousand training and 10 thousand test images. As the name\\nimplies, the dataset contains 10 classes. F or each class there are 5 thousand training images\\nand a thousand test images. All images have the same size of 32 by 32 pixels. Note that the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 22}),\n",
       " Document(page_content='number of classes and image size are much smaller compared to ImageNet. A similar neural\\nnetwork will require significantly less FLOPs for CIF AR-10 than ImageNet. Currently , the\\nbest accuracy achieved on CIF AR-10 is 99.9% [ 57].\\nResNet-56 consists of a convolutional layer, 27 building blocks with 2 convolutional layers\\neach, and a fully-connected layer outputting 10 probabilities for the CIF AR-10 classes. ResNet-\\n56 achieves an accuracy of 88.8% on CIF AR-10.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 22}),\n",
       " Document(page_content='F or both ImageNet and CIF AR-10, ResNets do not achieve state-of-the-art accuracy . This is\\nto be expected, given that ResNets were introduced in 2015. By using a benchmark model\\nthat older compression methods already reported results for, it is easier to compare methods.\\nUsing the latest models, requires rerunning older compression methods for comparison. This\\nonly works up to a certain point. F or example, MNIST and LeNet-5 [ 58] have been used for', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 22}),\n",
       " Document(page_content='a long time as benchmark, and are still sometimes used, but LeNet-5 has been compressed to\\nless 0.5% of its weights without any loss of accuracy [ 12], making any further improvements\\ndiﬀicult to quantify .\\n17', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 22}),\n",
       " Document(page_content='3 Methods\\nIn this chapter two compression methods are selected for testing on YOLOv7-tiny . Section 3.1\\nwill answer the question: What are the best networks for comparing compression methods?\\nThis is followed by a compilation of best performing compression methods: What is the state-\\nof-the-art in neural network compression? F rom this list, two methods are selected, which will\\nnarrow down which compression method is best suited for compressing YOLOv7? The selected', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='methods are explained in Sections 3.2 -3.3 .\\n3.1 Compression Search\\nF or compressing YOLOv7, a list of the state-of-the-art compression methods must be compiled.\\nF ortunately , there are several surveys categorizing and rating compression methods. The\\nlargest comparison of compression methods is given in [ 59]. This comparison is used as a\\nstarting point for finding the best methods. The authors categorize 150 methods and count', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='how often each network and dataset is used to report the compression results. The most used\\nnetworks and datasets are: ResNet-56 on CIF AR-10, AlexNet and ResNet-50 on ImageNet,\\nand LeNet-5 on MNIST. As mentioned in Chapter 2, LeNet-5 is no longer useful as benchmark.\\nBoth LeNet-5 and AlexNet are relatively old and small networks that are less representative\\nof bigger networks like YOLOv7.\\nSo, what are the best networks for comparing compression methods? Based on how often', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='these networks are used as benchmark, ResNet-50 (on ImageNet) and ResNet-56 (on CIF AR-\\n10) are the best networks to compare and rank compression methods. Note that these networks\\nand datasets are introduced in Section 2.3.3 .\\nThe latest methods reported in [ 59] are from 2020. It is likely that since then new methods\\nhave been proposed with better performance. Therefore, a search has been performed for\\npapers from after 2020. By only including the papers that report on ResNet-50 on ImageNet', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='and ResNet-56 on CIF AR-10, the search is narrowed down considerably .\\nT able 3.1 shows the result of this search. The FLOPs pruning ratio (FPR) and parameter\\npruning ratio (PPR) are the remaining percentage of FLOPs and parameters, respectively . T o\\nlimit this table to the best performing compression methods, only those with a FPR below\\n30% for ResNet-56 or a FPR below 40% for ResNet-50 are included. If a method achieves a', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='FPR below this threshold on one, but not the other, network, the results are still given for\\nboth networks. F or most methods PPR and FPR are close together. Since the goal is to speed\\nup a neural network, the number of FLOPs is more important than the number of parameters.\\nF rom this table it can be concluded that FPFS outperforms all other methods on ResNet-\\n56. PCA-Pruner, CONPLSF, HRel-1, NNCS and GBIP are quite similar in compression, with', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='NNCS outperforming them on accuracy loss. PKSMIO and KSE are clearly at the bottom\\nbased on their compression ratio but do limit the accuracy loss or even increase accuracy . F or\\nResNet-50 on ImageNet, NNCS has slightly better compression and KSE has slightly better\\naccuracy . PKSMIO, GBIP and FPFS also have similar compression performance, with GBIP\\nhaving a slight edge on accuracy . HRel-1 is significantly outperformed by the other methods\\non ResNet-50.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='on ResNet-50.\\nAlthough the compression ratio and accuracy loss of a method are important, the ease\\n19', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 24}),\n",
       " Document(page_content='T able 3.1 :Reported results from the best performing compression methods for ResNet-56 on\\nCIF AR-10 and ResNet-50 on ImageNet. Accuracy Loss is the percentage point reduction\\nin accuracy of the pruned network. Negative accuracy loss indicates an increased accuracy\\nafter pruning. FPR and PPR refers to FLOPs Pruning Ratio and Parameter Pruning Ratio,\\nrespectively . These pruning ratios give the percentage of FLOPs or parameters remaining', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 25}),\n",
       " Document(page_content='after pruning. A lower pruning ratio is better. Only compression methods with a FPR\\nbelow 30% for ResNet-56 or a FPR below 40% for ResNet-50 are included in this table. F or\\ncompleteness, the results of methods with a FPR below this threshold for only one network\\nare still reported for both networks. This table helps answer the question: What is the\\nstate-of-the-art in neural network compression?\\nModel Method Accuracy Loss FPR PPR\\nResNet-56\\n(CIF AR-10)FPFS [ 13] 0.7 11.2 12.9', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 25}),\n",
       " Document(page_content='PCA-Pruner [ 60] 1.05 11.2 18.8\\nCONPLSF [ 61] 1.35 21.0 21.9\\nHRel-1 [ 62] 1.1 23.1 22.2\\nNNCS [ 63] 0.21 23.8 13.7\\nGBIP [ 64] 0.38 26.6 29.6\\nPKSMIO [ 65] -0.32 35.8 36.4\\nKSE [ 66] 0.15 40.0 41.7\\nResNet-50\\n(ImageNet)NNCS 1.24 21.1 24.4\\nKSE 0.84 21.3 34.5\\nPKSMIO 1.48 33.9 27.8\\nGBIP 0.47 36.7 44.6\\nFPFS 0.96 39.7 -\\nHRel-1 0.68 51.3 51.8\\nT able 3.2 :Number of training epochs that each method uses to go from a pretrained to', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 25}),\n",
       " Document(page_content='a pruned, finetuned network. ’50 per layer’ means that the network layers are pruned\\nsequentially with 50 epochs of finetuning after each layer.∗These numbers are not explicitly\\nmentioned in papers but are estimated.\\nMethod T raining Epochs\\nCIF AR-10 ImageNet\\nFPFS 50 per layer 100 per layer\\nPCA-Pruner 50 per layer -\\nCONPLSF 260 -\\nHRel-1 100 33\\nNNCS 450∗-\\nGBIP 30 20\\nKSE 200 21\\nPKSMIO 150∗-\\n20', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 25}),\n",
       " Document(page_content='of implementation is also of relevance. Specifically , the time it takes to go from a pretrained\\nmodel to a pruned and finetuned model should be within certain bounds. T able 3.2 shows the\\nnumber of training epochs each methods requires to achieve a pruned and finetuned model.\\nRegarding size, the dataset used for training YOLOv7, MS COCO, is more similar to ImageNet\\nthan CIF AR-10. Unfortunately , not all papers report training epochs on ImageNet. Since it', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='can take up to two hours on a simple GPU to train YOLOv7 for one epoch, most methods\\nare not feasible. F rom this table, the three methods that seem feasible are HRel-1, GBIP and\\nKSE.\\nGBIP outperforms KSE for ResNet-56 on CIF AR-10, and HRel-1 for ResNet-50 on Im-\\nageNet, which makes it an easy choice. The second chosen method is KSE. It significantly\\noutperforms HRel-1 on FPR with similar accuracy loss on ResNet-50. It is outperformed by', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='HRel-1 on ResNet-56, but in this case the difference in accuracy loss is clearly in favor of KSE.\\nTherefore, the answer to the question, Which compression method is best suited for com-\\npressing YOLOv7? is narrowed down to two methods: GBIP and KSE. The next subsec-\\ntions will describe the selected methods. The specific implementations of these methods for\\nYOLOv7-tiny will be discussed in Chapter 4.\\n3.2 GBIP', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='3.2 GBIP\\nGlobal balanced iterative pruning for eﬀicient convolutional neural networks [64] introduces a\\ncompression method (GBIP) based on a simple pruning strategy but with a more advanced\\napproach to recovering accuracy . It prunes entire filters (see Figure 2.9h ) from each convolu-\\ntional layer. Removing filters reduces the number of output channels of this layer as well as\\nthe input channels of the next layer.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='As criterion for pruning, the L 1norm of the output feature maps is used. The use of output\\nfeature maps, means that GBIP requires a dataset to compress a network. By normalizing the\\nL1norm for each layer, layers with large L 1norms do not dominate the compression result.\\nRather the network is compressed more evenly , resulting in a balanced pruning of the network.\\nThis balanced pruning means that the difference in pruning ratios of FLOPs and parameters\\nis small. The importance score ml', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='nis calculated for all Nloutput feature maps Yl\\nnfor all layers\\nl:\\nml\\nn=∥Yl\\nn∥1/max{∥Yl\\n1∥1,∥Yl\\n2∥1, ...,∥Yl\\nNl∥1} (3.1)\\nIt is not explicitly mentioned whether the output feature maps are pre- or post-activation.\\nHowever, other compression methods typically use post-activation [ 67,68,69]. This makes\\nsense, given that the post-activation feature maps contain the information that is passed on\\nthrough the rest of the network. Therefore, the post-activation feature maps are used for', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='computing the importance score.\\nThe pruning threshold is determined by the mean of the importance scores for each layer\\nand a global pruning factor k:\\nml\\np=k1\\nNNX\\nn=1ml\\nn, with k∈(0,1) (3.2)\\nF or all feature maps where ml\\nnis below ml\\npthe corresponding filters are removed. The\\nnext layer now has a reduced number of input channels and thus its weights can also be\\npartially pruned.\\nAs the name indicates, an iterative pruning schedule is used, where the network is trained', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='for ten epochs after each pruning step to partially restore its accuracy . This increases the\\n21', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 26}),\n",
       " Document(page_content='control over the pruning process. If the accuracy stays high, a next pruning step can be per-\\nformed. But if the accuracy starts to deteriorate too quickly , the network might be compressed\\nto its limit. The authors give no rationale for the number of pruning steps they use in their\\nexperiments, but depending on the dataset there are a total of 2 or 3 pruning steps.\\nUsing the L 1norm for pruning filter is common, but the authors of GBIP also focus on the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='finetuning stage. They add three ways to improve the accuracy recovery: knowledge transfer\\nfrom the (i) output and (ii) intermediate features of the original network, as well as an (iii)\\nadversarial network to discriminate between the two networks.\\n3.2.1 Output T ransfer\\nThe output of the original model (teacher) is used to guide the training of the compressed model\\n(student). As mentioned in Subsection 2.3.1 , the knowledge of the teacher can be transferred', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='to the student by adding the KL divergence between the output of the teacher, fT(x), and\\nthe student, fS(x), to the student loss function. In this case, a temperature T is used to\\nsoften the outputs. T o keep the magnitude of the KL loss independent of the temperature,\\nthe loss is multiplied by T2. Finally , the output transfer loss ( LOT ) is a weighted sum of the\\ncross-entropy between fS(x)and hard targets, and the KL loss ( LKL ):\\np(x) =Fsoftmax (fS(x)/T) (3.3)\\nq(x) =Fsoftmax (fT(x)/T) (3.4)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='DKL(p∥q) =nX\\ni=1[p(x) log(p(x))−p(x) log(q(x))] (3.5)\\nLKL(WS) =T2DKL(p∥q) (3.6)\\nLOT(WS) =αLKL(WS) + (1 −α)LCE(WS) (3.7)\\n3.2.2 Attention T ransfer\\nNot only the output, but also the intermediate features of the teacher can be used to guide\\nthe training of the compressed student. The implementation in GBIP is taken from [ 70]. The\\nactivations M of three different layers in the network are used to calculate spatial attention', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='maps Al. These maps are a measure of how much attention the network is paying to each pixel\\nof the feature map. This way the student can be taught to focus on the same pixel locations\\nas the teacher. The spatial attention maps are normalized by their L 2norm. The attention\\ntransfer loss LAT is given by the L 2norm of the difference between the normalized attention\\nmaps:\\nAl(Mab) =1\\nNNX\\nn=1(Mab\\nn)2(3.8)\\nLAT(WS) =3X\\nl=1\\r\\r\\r\\r\\rAl\\nS\\n∥Al\\nS∥2−Al\\nT\\n∥Al\\nT∥2\\r\\r\\r\\r\\r\\n2(3.9)\\n3.2.3 Adversarial Game', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='T o further converge the output of the student to that of the teacher, a discriminator network\\nis created to differentiate between the two networks. This network has three fully-connected\\n22', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 27}),\n",
       " Document(page_content='layers with 128-256-128 neurons. The task of this network is to get better at distinguishing\\nbetween teacher and student output, while the student network tries to fool the discriminator.\\nThe idea is taken from [ 71], where a simple fully-connected network is also used to distinguish\\nbetween an original and a compressed network. The goal of the discriminator is to output a\\nvalue close to one if the input came from the teacher and close to zero if it came from the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='student. It does this by minimizing the following loss function:\\nLG(WG) =EfT(x)∼pT(x)[log(1 −G(fT(x, W T), WG))]\\n+EfS(x)∼pS(x)[log(G(fS(x, W S), WG))](3.10)\\nwhile the discriminator is being optimized to predict a zero for student input, the student\\nis simultaneously updated to fool the discriminator and get it to output a value closer to one,\\nwith the adversarial game loss LAG :\\nLAG(WS) =EfS(x)∼pS(x)[log(1 −G(fS(x, W S), WG))] (3.11)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='Combining these losses, gives the following total loss function for the compressed network:\\nLS(WS) =LAG(WS) +LAT(WS) +LOT(WS) (3.12)\\n3.2.4 Results\\nThe authors test GBIP on several networks and datasets. F rom this, it is clear that k= 0.5is\\nthe maximum for smaller networks like VGG-16 and VGG-19, as well as for larger networks\\non more complex datasets (CIF AR-100, ImageNet). F or a large network on a smaller dataset,\\nlike ResNet-110 on CIF AR-10, kcan go up to 0.7 without accuracy loss.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='They also assess the effect of the three finetuning techniques. All three show improvements\\nwith output transfer (OT) having the largest effect, followed by the adversarial game (AG)\\nand attention transfer (A T). F or ResNet-18 on ImageNet, the authors report the best accuracy\\nimprovement of 1.24% using all three techniques. While the accuracy improvement for VGG-\\n16 on CIF AR-10 and ResNet-56 on CIF AR-100 is only 0.20% and 0.24%, respectively . The', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='difference between these accuracy improvements might be due to the initial accuracy loss of the\\nbaseline without any finetuning techniques. F or these last two networks, the baseline accuracy\\nloss is very small (0.42%) for ResNet-56 and negative (-0.34%) for VGG-16, leaving less room\\nfor improvement than ResNet-18 with 1.90% baseline accuracy loss.\\n3.3 KSE\\nExploiting Kernel Sparsity and Entropy for Interpretable CNN Compression [66] introduces', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='a compression method (KSE) focusing on the channels of the input feature maps and its\\ncorresponding 2D kernels. Instead of pruning, this method uses clustering of the 2D kernels\\ninto a few clusters to compress the network. It introduces an indicator that uses both the\\nsparsity and entropy of a kernel. Based on this indicator the number of clusters required for\\neach input feature map is calculated.\\n23', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 28}),\n",
       " Document(page_content='3.3.1 Kernel Sparsity\\nThe convolution operation can be formulated as:\\nYn=CX\\nc=1Wn,c∗Xc (3.13)\\nwithC channels in the input feature map and N channels in the output. F or each channel\\nin the input, Xc, the matching 2D kernels are given by {Wn,c}N\\nn=1 . Instead of using the sparsity\\nofXcas pruning indicator, the sparsity of its corresponding 2D kernels is used. In the paper,\\nthe authors verify the relationship between the sparsity of the input and that of the weights.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 29}),\n",
       " Document(page_content='This makes KSE a data-free compression method since it only uses the weights of the network\\nwithout having to run a dataset through it.\\nThe sparsity for channel cof the input feature map is defined as:\\nsc=NX\\nn=1∥Wn,c∥1 (3.14)\\nClearly , scis low for sparse kernels, where the weights are small.\\n3.3.2 Kernel Entropy\\nIf an input feature map contains a lot of information, its 2D kernels are less suitable for\\npruning. The amount of information increases if the 2D kernels are very diverse. The authors', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 29}),\n",
       " Document(page_content='propose kernel entropy as a measure of this information.\\nFirst, a density metric dm is calculated for all 2D kernels based on the distance between\\neach kernel and its k-nearest neighbors:\\ndm(Wi,c) =NX\\nj=1ACi,j (3.15)\\nACi,j=(\\n∥Wi,c−Wj,c∥2 ifWj,c is among k nearest neighbors of Wi,c\\n0 otherwise(3.16)\\nLarger values for dm(Wi,c)mean its closest neighbors are far away . Large differences\\nbetween kernels also result in large differences in the convolution of these kernels with the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 29}),\n",
       " Document(page_content='input feature map. Thus, if many kernels have a high dm , the input feature map contains a\\nlot of information.\\nBased on this density metric, the proposed kernel entropy is defined as:\\nec=−NX\\ni=1dm(Wi,c)\\ndclog2dm(Wi,c)\\ndc, with dc=NX\\ni=1dm(Wi,c) (3.17)\\nIf the kernels are very diverse, the kernel entropy will be lower.\\n3.3.3 Kernel Clustering\\nThe kernel sparsity and entropy (KSE) indicator combines the two metrics introduced above:\\n24', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 29}),\n",
       " Document(page_content='vc=rsc\\n1 +αec(3.18)\\nWith balancing factor αset to 1 for all experiments. As mentioned above, a lower kernel\\nsparsity refers to a higher sparseness, while a lower entropy refers to a more diverse distribution\\nof kernels. Therefore, a lower vcshould indicate higher compression. Based on vcthe number\\nof kernels for the c-th input feature map is calculated:\\nqc=8\\n><\\n>:0 if⌊vcG⌋= 0\\nN if⌈vcG⌉=G\\x06N\\n2G−⌈vcG⌉+T\\x07\\notherwise(3.19)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='otherwise(3.19)\\nWhere G andT are the hyperparameters controlling granularity and compression ratio,\\nrespectively . If G= 2 ,qcis limited to 0 or N. Larger values for G allow for finer grained\\ncompression. Treduces the number of clusters by a factor 2Tfor the third case in the equation\\nabove.\\nIn the case of qc= 0 , the entire input feature map can be ignored. And if qc=N, no\\ncompression is applied. F or the values in between 0 and N, all the original N kernels are', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='assigned to one of the qcclusters and its weights replaced by the centroid of this cluster. The\\ncentroids are denoted as {Bi,c}qc\\ni=1 , while the index set {In,c∈ {1,2, ..., q c}}N\\nn=1 links each\\nkernel to one of the qckernels. In the finetuning stage, only the cluster centroids are updated.\\nT o make full use of the induced sparsity , the 3D convolutions are split into 2D convolutions.\\nUsing the notation from above, qc2D activation maps are generated: Zi,c=Bi,c∗Xc. These', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='are then combined into the n-th output feature map:\\nYn=CX\\nc=1ZIn,c,c (3.20)\\n3.3.4 Results\\nF rom the paper, one would assume the authors use Equation 3.20 to speed up the network,\\nbut from the published code it is clear that this is not the case. Instead, a full weight matrix\\nis reconstructed from the clusters. Only in the case of qc= 0 , when there are no clusters,\\nthe size of the weight matrix is reduced. This means that the actual reduction in FLOPs is', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='minimal. The fact that an unoptimized implementation is used, is not mentioned in the paper\\nitself. They do, however, refer to the reduction in FLOPs as ’theoretical’ exactly once.\\nThey test KSE with ResNet-56, DenseNet-40 and DenseNet-100 on CIF AR-10, as well as\\nResNet-50 on ImageNet. F or CIF AR-10, Tis set to 0, while for ImageNet it is set to 1. In all\\ncases, the best compression is achieved with G= 5 or 6.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='F or ResNet-50 on ImageNet the reduction in FLOPs is about 1.5 times the reduction in\\nparameters, while for CIF AR-10 these are almost identical. And the accuracy loss is slightly\\nhigher for ImageNet, though still below 1%. This might be due to the different setting for T\\nbut is not mentioned in the paper.\\nThey also evaluate the effect of kernel sparsity and entropy . F or ResNet-56 on ImageNet,\\nthey show that using both kernel sparsity and kernel entropy outperform using either one of', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='these alone.\\n25', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 30}),\n",
       " Document(page_content='This chapter explained the search for compression methods, showing the state-of-the-art in T a-\\nble 3.1 , which methods have been selected and how these selected methods work. Specifically ,\\nit answered the question: What are the best networks for comparing compression methods?\\nNamely , ResNet-50 (on ImageNet) and ResNet-56 (on CIF AR-10). As well as answering:\\nWhat is the state-of-the-art in neural network compression? and narrowing down Which com-', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 31}),\n",
       " Document(page_content='pression method is best suited for compressing YOLOv7? to two methods: GBIP and KSE.\\n26', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 31}),\n",
       " Document(page_content='4 YOLOv7-tiny Experiments\\nThe previous chapter introduced the two selected compression methods. This chapter will go\\nover the specific implementation details of adapting these methods for compressing YOLOv7-\\ntiny on MS COCO. It will also introduce the experiments that are run to determine the best\\nperforming method for YOLOv7.\\nFirst, Section 4.1 mentions the evaluation metrics and implementation details that are', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='similar for both methods. Then, Section 4.2 and 4.3 will go into the specifics for GBIP and\\nKSE, respectively .\\n4.1 General\\nThe available implementation of both YOLOv7-tiny and KSE use PyT orch [ 72] as machine\\nlearning framework. T o avoid unnecessary complexities, all the experiments also use PyT orch.\\nAs much as possible, the hyperparameters chosen by the creators of YOLOv7 are kept the\\nsame for the compression experiments. F or example, the batch size, learning rate scheduler', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='and data augmentation, are not changed. This means a batch size of 64 and the use of cosine\\nannealing (See Subsection 2.2.3 ). The starting learning rate is determined by training the\\nnetwork for different learning rates and check how fast the network converges with each.\\nAlso from the YOLOv7 implementation, while training and evaluating the network the\\nfitness metric is used. F or the following experiments the fitness, AP and AP 50 are reported.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='See Subsection 2.2.3 how these metrics are computed. T o determine the amount of compression\\nthat is achieved, the parameter pruning ratio (PPR) and FLOPs pruning ratio (FPR) are used.\\nThese are explained in Section 2.3.2 .\\n4.2 GBIP\\n4.2.1 Experiments\\nGBIP has only one hyperparameter influencing the amount of compression, pruning threshold\\nfactor k. The authors find the best results when using k= 0.3−0.6. Since YOLOv7-tiny is', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='already relatively small, it makes sense to also test k= 0.2, so the following values will be\\ntested: k={0.2,0.3,0.4,0.5,0.6}. These experiments will run three pruning cycles, where\\neach pruning cycle consists of a pruning step followed by 10 epochs of finetuning.\\nT o increase the accuracy of the pruned network, GBIP introduces three finetuning tech-\\nniques: output transfer (OT), attention transfer (A T) and adversarial game (AG). The authors', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='do an ablation study to determine the eﬀicacy of each of these techniques. The ablation study\\nis done on three different networks, but only for image classification. Since the object detection\\ntask of YOLOv7(-tiny) is significantly different, and the fact that the effect measured in the\\nablation study differs considerably per network and dataset, before running the full experi-\\nments mentioned above an ablation study is done for YOLOv7-tiny . F or this ablation study ,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='only one pruning cycle will be tested with k= 0.4. That is, the network will be pruned once,\\nfollowed by 10 epochs of finetuning. If any of the finetuning techniques does not improve the\\nfinal accuracy , it can be omitted for the other experiments.\\n27', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 32}),\n",
       " Document(page_content='4.2.2 Implementation\\nThe authors do not provide their own implementation online. Therefore, GBIP has been\\nimplemented from scratch in PyT orch. The explanation in the paper is entirely focused on an\\napplication for image classification. Some adjustments have been made to be able to use this\\nmethod for object detection with YOLOv7-tiny on MS COCO.\\nThe pruning step is identical, but the finetuning techniques have been (slightly) changed', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='to fit the YOLOv7-tiny architecture. F or attention transfer, only the location of the attention\\nmaps must be chosen. But given the different output structure of YOLOv7-tiny , compared\\nto a typical classification network like ResNet-56, the output transfer and adversarial game\\nrequires more changes. The rest of this subsection will describe these changes.\\nAttention T ransfer\\nThe original implementation uses the activations of three layers in the network to calculate', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='an attention transfer loss. No explanation is given for the exact positioning of these layers,\\nbut they are spaced out evenly through the network. Several combinations of layers have\\nbeen tested by pruning with k= 0.5and finetuning for one epoch. The results are shown in\\nT able 4.1 . Note that due to the limited finetuning, the accuracy increase is only an indication\\nof the relative performance. So, which layers should be used for attention transfer? F rom the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='limited testing done, it seems that (i) it is better to include more layers than three, and (ii)\\nthat later layers ( [57,65,73] ) work better than earlier layers ( [0,1]). The best result (shown in\\nbold) is obtained when using the output of all ELAN blocks to create the attention maps.\\nT able 4.1 :Layer indices of at-\\ntention maps tested for At-\\ntention T ransfer. See Ap-\\npendix A.1 for the YOLOv7-\\ntiny architecture with indices.\\nYOLOv7-tiny has been pruned\\nwith k= 0.5and finetuned for', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='one epoch.Layer IndicesFitness\\nIncrease\\n37, 40, 50 0.49\\n14, 21, 28 0.67\\n7, 14, 21, 28 0.79\\n57, 65, 73 1.39\\n0, 1 -0.12\\n7, 14, 21, 28, 47, 57, 65, 73 1.77\\n0, 1, 7, 14, 21, 28, 47, 57, 65, 73 1.71\\nOutput T ransfer\\nGiven that the output of an image classification network is different from YOLOv7, the way\\nthe output transfer loss is calculated requires some changes. As explained in Subsection 3.2.1 ,\\nto align the pruned network with the teacher, the KL divergence between the (softened)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='output of both networks is added to the loss function. This makes sense for an output of\\nclass probabilities, but the output of YOLOv7 not only contains class probabilities but also\\na bounding box and objectness score (see Subsection 2.2.2 ). T o make full use of the output,\\nthe implemented transfer learning loss, LTL , consists of three parts: (i) a KL divergence loss\\nbetween the class probabilities, (ii) a binary cross-entropy loss between the objectness scores,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='and (iii) an IoU loss between the bounding boxes of the student and the teacher.\\nNote that these three additional losses match with the three loss components of YOLOv7\\n(see Subsection 2.2.3 ). The KL divergence loss, LKL(WS,class _probs , is identical to Eq 3.3 -3.6 ,\\nwhile the IoU loss, LIoU(WS,bbox), is similar to the IoU loss of YOLOv7. These six losses are\\ncombined, in the same way as Equation 3.7 , to form the output transfer loss:\\n28', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 33}),\n",
       " Document(page_content='LTL(WS) =LKL(WS,class_probs ) +LBCE (WS,objectness ) +LIoU(WS,bbox) (4.1)\\nLOT(WS) =αLTL(WS) + (1 −α)LYOLO (WS) (4.2)\\nAdversarial Game\\nAs mentioned in Subsection 3.2.3 , the discriminator for the image classification tasks is a 128-\\n256-128 fully-connected network. This network takes as input a 1D tensor with class probabil-\\nities. Given the difference in output of YOLOv7, some changes to the discriminator network', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='must be made. The output of YOLOv7 consists of three 3D tensors (see Subsection 2.2.3 ). F or\\nan input image of [640×640] , the output is ( [255×80×80],[255×40×40],[255×20×20] ). It\\nis not directly obvious how the adversarial network should deal with this output. Therefore,\\nthree different types of architecture have been tested.\\nThe first architecture uses max pooling to reduce the size of all 3D tensors to [255×1×1].', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='Then, the three tensors are concatenated and flattened, resulting in a 1D tensor with a length\\nof 765. Finally , this 1D tensor is run through a 128-256-128 fully-connected network.\\nF or the second network, max pooling is used on the second and third 3D tensors to create\\nthree tensors of size [255×20×20] . These tensors are concatenated and run through two\\nconvolutional layers, followed by one final fully-connected layer.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='The last architecture most closely resembles the original adversarial network from the\\nGBIP paper. First, each 3D tensor is reshaped from [255×80×80] to[19200 ×85] and\\nremoving the bounding box information and objectness score gives [19200 ×80] . This tensor\\ncontains 19200 probability distributions for each 80 classes in MS COCO. The original 128-\\n256-128 fully-connected network can now be used on all 19200 probability distributions. The', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='final output of the network is simply the average of all these 19200 runs.\\nSeveral tests have been done, slightly tweaking certain parts like kernel size for convolution\\nand max pooling, number of neurons in fully-connected layers and total number of layers.\\nGiven these tests, and the fact that it is most similar to the original network, the fully-\\nconnected 128-256-128 network has been chosen.\\nIn the final implementation, based on [ 71], alternately the student network is trained with', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='adversarial loss for a period while the adversarial network stays fixed, and then the adversarial\\nnetwork is updated while the student network is trained without adversarial loss.\\n4.3 KSE\\n4.3.1 Experiments\\nKSE has two hyperparameters regulating compression: G andT. A higher value for G results\\nin higher compression granularity , while higher values for Tlead to higher compression ratios.\\nT o find the best values for these hyperparameters, the same values as used in the KSE paper', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='(G={3,4,5,6}, T={0,1}) are tested on YOLOv7-tiny .\\nIn contrast to GBIP , KSE compresses the network once, followed by several epochs of\\nfinetuning. In the case of YOLOv7-tiny , the accuracy reaches its maximum after about 15\\nepochs of finetuning. T o be sure, all compressed networks will be finetuned for 20 epochs.\\n4.3.2 Implementation\\nF ortunately , the authors of KSE provide an implementation of their code online [LINK]. In', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='contrast to GBIP , which requires significant changes for YOLOv7 due to the used finetuning\\n29', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 34}),\n",
       " Document(page_content='techniques, KSE does not use any special finetuning. This makes it relatively easy to adapt\\nto YOLOv7-tiny .\\nUnfortunately , the published code contains an unoptimized implementation of their method.\\nThe output of each layer is not computed using Equation 3.20 , but rather a weight matrix is\\ncreated by matching the clusters Bi,c and indices In,c withWn,c=BIn,c,c(See Subsection 3.3.3\\nfor notation). This weight matrix has a size of [N×C′×k×k], where C′is the number of', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 35}),\n",
       " Document(page_content='input channels with qc>0. This is almost identical to the size of the original weight matrix,\\nsince only a fraction of the input channels can be removed entirely .\\nF or now, the optimization will be ignored, and revisited if the KSE outperforms GBIP .\\nThe same formula used by the authors of KSE to report the achieved acceleration will be used\\nto compare with GBIP . This formula for one layer, rewritten to match FLOPs pruning ratio\\n(FPR) definition, is given by:\\nFPR =P\\ncqc\\nNC(4.3)', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 35}),\n",
       " Document(page_content='FPR =P\\ncqc\\nNC(4.3)\\nNote that with the current unoptimized implementation, the actual FPR for each layer is\\nsimply:\\nFPR =C′\\nC(4.4)\\nThe theoretical PPR is the same as for the unoptimized implementation. Although a full\\nweight matrix is constructed, the actual parameters that are saved and tracked are the cluster\\ncentroid.\\nSimilar, to the original implementation, for YOLOv7-tiny the first and last layers are not com-', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 35}),\n",
       " Document(page_content='pressed. The reason for this is that the first layer contains all information in only three input\\nchannels (RGB), so compressing this layer might remove too much information. Compression\\nin the intermediate layers can be partly recovered in later layers, which is not the case for the\\nlast layer.\\nThis chapter listed the experiments that will be run on YOLOv7-tiny , as well as the specific\\nimplementation details of GBIP and KSE. In the next chapter, the results of these experiments', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 35}),\n",
       " Document(page_content='are given, based on which one method is selected to run on YOLOv7.\\n30', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 35}),\n",
       " Document(page_content='5 Selecting the Best Method\\nThis chapter will begin by reporting the results of running GBIP and KSE on YOLOv7-tiny in\\nSection 5.1 . Based on these results, Section 5.2 will answer the question: Which compression\\nmethod is best suited for compressing YOLOv7?\\n5.1 Compression Results\\n5.1.1 GBIP\\nAn ablation study is performed to determine the eﬀicacy of the three added finetuning tech-\\nniques: attention transfer (A T), output transfer (OT), adversarial game (AG). The results are', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='shown in T able 5.1 . A T performs significantly better than the baseline. OT only manages a\\nmarginal increase in fitness, while AG shows no improvement at all. Combining A T and OT,\\nwhich both increase fitness individually , does not result in an improvement over just using A T.\\nSimilarly , combining A T with AG does not increase the fitness. Given these results, the next\\nexperiments have been performed using only A T.\\nT able 5.1 :Results of the abla-\\ntion study on YOLOv7-tiny .', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='All experiments are performed\\nwith k= 0.6. A T: Attention\\nT ransfer, OT: Output T rans-\\nfer, AG: Adversarial Game.\\nFitness and AP are percent-\\nages. Best results are marked\\nin bold.A T OT AG Fitness AP AP 50\\n- - - 31.4 29.7 47.1\\n✓ - - 33.9 32.2 49.9\\n-✓ - 32.4 30.7 47.9\\n- -✓ 31.4 29.7 47.1\\n✓ ✓ - 33.8 32.0 49.6\\n✓ -✓ 33.9 32.2 49.9\\nThe results of running GBIP on YOLOv7-tiny for k={0.2,0.3,0.4,0.5,0.6}are shown in', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='T able 7.1 . There are two observations that can be made from this data. Firstly , the number\\nof FLOPs decreases faster than the number of parameters. The authors of GBIP show that\\nfor VGG-16 and GoogLeNet on CIF AR-10 the FPR and PPR stay within about 5%pt of each\\nother. F or k= 0.6there is a difference of 23.1%pt between FPR and PPR. The reason for\\nthis, is that the first layers, which contain the most FLOPs, are pruned slightly more than', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='the rest of the network. F or the networks tested by the authors, the FLOPs are more evenly\\ndistributed throughout the networks than is the case for YOLOv7-tiny .\\nSecondly , the FPR does not decrease linearly with increasing k. This is to be expected,\\ngiven that the network is pruned multiple times. A smaller kwill remove a small portion of\\nchannels in the first pruning cycle, and then another small portion of the remaining network in', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='the next cycle. While larger values for kwill remove a larger portion of channels the first time,\\nand another large portion of an already smaller network the second time, thus compounding\\nthe compression effect.\\n31', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 36}),\n",
       " Document(page_content='T able 5.2 :Results of GBIP ex-\\nperiments on YOLOv7-tiny .\\nFitness, AP , FLOPs Prun-\\ning Ratio (FPR) and Param-\\neter Pruning Ratio (PPR) are\\ngiven as percentages. Note k=\\n0refers to the unpruned base-\\nline.k Fitness AP AP 50 PPR FPR\\n0 39.2 37.4 55.2 100 100\\n0.2 39.0 37.2 55.4 99.8 99.6\\n0.3 38.4 36.6 54.7 97.9 93.6\\n0.4 37.6 35.8 54.0 95.2 86.8\\n0.5 34.8 33.0 50.9 85.8 68.6\\n0.6 28.0 26.3 42.9 65.4 42.2\\n5.1.2 KSE', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='5.1.2 KSE\\nThe results of running KSE on YOLOv7-tiny are shown in T able 5.3 . F or the FPR both the\\nactual and theoretical values are given, while the actual PPR is identical to the theoretical\\nvalues. The actual values refer to the used unoptimized implementation, while the theoretical\\nvalues are computed using Equation 4.3 (see Subsection 4.3.2 ).\\nRegarding the actual FPR, if granularity G gets higher, the less reduction in FLOPs is', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='achieved. This is expected, since with higher granularity , the number of clusters assigned to\\nan input channel can be very small. As mentioned in Subsection 4.3.2 , the actual FPR is\\nbased on the number of input channels that are completely ignored, that is qc= 0 . With\\nlow granularity , either no clusters or a large number of clusters is assigned, resulting in the\\nremoval of relatively unimportant input channels, which in the case of high granularity might', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='have been assigned just a few clusters.\\nIn general, increasing G andT increases the compression. It seems that the specific\\ncombination of G andT does not matter much for the resulting accuracy . F or both G=\\n5, T= 0 andG= 3, T= 1 the FPR is about 70%, and their fitness is identical. The same can\\nbe seen for G= 6, T= 0 andG= 4, T= 1 , where the FPR is around 60% and the difference\\nin fitness is 0.6%pt.\\nT able 5.3 :Results of\\nKSE experiments on\\nYOLOv7-tiny . Fit-\\nness, AP , PPR and', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='ness, AP , PPR and\\nFPR are percentages.\\nNote that for the\\nFPR both the actual\\nand theoretical val-\\nues are given, the-\\noretical PPR is the\\nsame as the actual\\nvalues (see Subsec-\\ntion 4.3.2 ).G T Fitness AP AP 50 PPR FPR FPR\\ntheoretical actual\\n- - 39.2 37.4 55.2 100 100 100\\n3 0 36.9 35.0 53.4 88.1 67.4 92.9\\n4 0 36.8 35.0 53.2 80.7 55.7 96.7\\n5 0 36.0 34.2 52.4 70.7 46.3 97.9\\n6 0 34.8 33.0 51.2 62.3 39.6 98.2\\n3 1 36.0 34.2 52.4 69.9 54.7 92.9\\n4 1 35.4 33.6 51.8 59.4 40.4 96.7', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='5 1 34.1 32.3 50.3 51.2 32.2 97.9\\n6 1 31.8 30.0 47.8 44.8 27.4 98.2\\n5.2 Best Method\\n5.2.1 Comparing GBIP and KSE\\nF rom the results of compressing YOLOv7-tiny , the most relevant information is the reduction\\nin fitness with respect to the FLOPs Pruning Ratio (FPR). This information is shown in\\n32', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 37}),\n",
       " Document(page_content='Figure 5.1 :Fitness vs FPR for GBIP , KSE (theoretical) and KSE (actual). According to the\\nKSE paper, the theoretical FLOPs should be possible with some optimization. KSE (actual)\\ngives the current FPR of the authors’ own implementation of KSE, without optimization.\\nF or GBIP the end result of each pruning cycle is shown, so 5 different values for kwith 3\\npruning cycles each gives 15 results.\\nFig. 5.1 and is taken directly from T ables 7.1 -5.3 . F or KSE it shows both the actual and', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 38}),\n",
       " Document(page_content='theoretical FPR. Because the current implementation of KSE can only ignore input channels\\nthat are fully pruned ( qc= 0 ), which is only a fraction of the total channels, the actual FPR\\nis very close to 100. However, comparing GBIP with the theoretical values of KSE shows a\\nsignificant advantage for KSE. F or all FPR the fitness is higher for KSE than GBIP . Assuming\\nKSE can indeed be optimized to achieve these theoretical values, KSE is the preferred method', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 38}),\n",
       " Document(page_content='for compressing YOLOv7. If not, clearly the unoptimized version of KSE is significantly worse\\nand GBIP is the better method.\\nKSE uses a type of unstructured pruning, which means that some additional optimization\\nis required to achieve acceleration. This is in contrast with structured pruning methods,\\nlike GBIP , that directly prune entire channels or filters. The authors of KSE detail how\\nthe computation of a compressed convolutional layer could be optimized. Unfortunately , the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 38}),\n",
       " Document(page_content='optimization of KSE is not as straightforward as the paper itself might suggest. The following\\nsubsection documents how the optimization is implemented in PyT orch.\\n5.2.2 PyT orch optimization\\nAs discussed in Subsection 4.3.2 , the implementation by the authors create a full weight matrix,\\nby copying the calculated clusters several times, which only results in a small acceleration if\\nan entire input channel was pruned. T o accelerate the pruned network, the authors suggest', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 38}),\n",
       " Document(page_content='taking the convolution of each input channel cwith all clusters Bi,c for that channel. Using\\nthe indices that matches output channels nwith the clusters, In,c , the output can be obtained:\\n33', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 38}),\n",
       " Document(page_content='Zi,c=Bi,c∗Xc (5.1)\\nYn=CX\\nc=1ZIn,c,c (5.2)\\nWhere Bi,c is the i-th cluster for input channel candIn,c is the index of the cluster that\\nconvolved with input channel cis part of output channel n.\\nThis is relatively easy to implement in PyT orch but turns out to make the network much\\nslower. One large convolution is split into a lot of small ones. Although the network is\\ncompressed, the number of small convolutions is still two orders of magnitude higher than the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='original number of convolutions. This has two main drawbacks.\\nFirstly , the optimization requires a lot of manual indexing, which is relatively slow. All\\nthose small convolutions still need to be matched and summed together (Equation 5.2 ). This\\nalso means that the intermediate results must be stored till the final output is computed. With\\na large convolution the intermediate results are only needed in the CUDA kernel, where they', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='can immediately be added to the final output without storing the result.\\nThis is part of the second problem: running a CUDA kernel has some overhead. This\\noverhead comes from launching and initializing the kernel, and moving the data from the slower,\\nglobal memory of the GPU to the fast L 1memory . So, running a lot of smaller convolutions\\nends up costing more time than a few larger convolutions.\\nT o see the difference between the two implementations, the output of the PyT orch Profiler', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='is given in T able 5.4 . This shows the total and average GPU time as well as the number of\\ncalls for the most used operators. Especially the number of calls is interesting. It shows\\n7625 convolutions for the optimized implementation compared to 58 in the original. And\\nalthough these are much smaller convolutions, the average GPU time is almost identical. The\\nother operators are almost all related to storing the intermediate results and indexing these to', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='compute the final output. Most of these operations still happen in the original implementation\\nbut are run on an optimized convolution CUDA kernel.\\nF rom the profiler output, it is clear that using the ’optimization’ in PyT orch is not an op-\\ntion. It is possible that a custom-made, optimized CUDA kernel implementation will accelerate\\nKSE. However, this falls outside the scope of this thesis.\\n5.2.3 Selecting Best Method', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='Although KSE could theoretically outperform GBIP , with the current implementation, pro-\\nvided online by the authors of KSE, this is not the case. An attempt has been made to optimize\\nthis implementation in PyT orch, but this turns out to make things worse.\\nGBIP , on the other hand, uses structured pruning which makes it easy to obtain a working,\\nfaster model without the need for optimization. GBIP also allows for more control in the size', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='of the resulting network, by varying the number of pruning cycles. So, which compression\\nmethod is best suited for compressing YOLOv7? Looking at Figure 5.1 , comparing GBIP with\\nthe actual values for KSE, it is clear that GBIP is the better method. Thus, GBIP is selected\\nfor compressing YOLOv7.\\nBased on the selection of GBIP for compressing YOLOv7, the next chapter will detail the\\nexperiments that are run on YOLOv7.\\n34', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 39}),\n",
       " Document(page_content='T able 5.4 :PyT orch profiler output for the original ( orig. ) and optimized ( opt. ) implementation\\nof KSE. The values are an average of 10 runs, with G= 3, T= 0 . This table contains all\\noperators that take up at least 1% of GPU time in the optimized implementation.\\nOperator NameT otal GPU [ms] Num. Calls A vg. GPU [us]\\norig. opt. orig. opt. orig. opt.\\naten::conv2d 20.60 2732.40 58 7625 355 358\\naten::convolution 19.99 2584.40 58 7625 345 339\\naten::_convolution 19.13 2458.60 58 7625 330 322', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 40}),\n",
       " Document(page_content='aten::cudnn_convolution 17.83 1271.70 58 7625 307 167\\naten::index 5.56 922.90 54 7621 103 121\\naten::contiguous 6.37 852.70 57 7570 112 113\\naten::clone 5.56 746.30 57 7570 97 99\\naten::select 0.27 705.90 9 22710 30 31\\naten::add 0.08 451.40 3 7624 27 59\\naten::slice 2.07 445.90 63 15197 33 29\\naten::as_strided 1.42 418.60 189 53050 8 8\\naten::item - 413.90 - 7567 - 55\\naten::_local_scalar_dense - 285.40 - 7567 - 38\\naten::empty_like 3.16 264.90 112 7625 28 35\\naten::unsqueeze - 238.00 - 7567 - 31', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 40}),\n",
       " Document(page_content='aten::reshape 1.80 231.60 57 7624 32 30\\naten::copy_ 1.62 231.30 60 7573 27 31\\naten::empty 3.76 186.10 408 15497 9 12\\ntotal 80 7187\\n35', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 40}),\n",
       " Document(page_content='6 YOLOv7 Experiments\\nBased on the experiments on YOLOv7-tiny , which are discussed in the previous chapters,\\nGBIP has been chosen for compressing YOLOv7. This chapter will explain which experiments\\nare performed in Section 6.1 . This is followed by the implementation details in Section 6.2 ,\\nwhich will answer the question: What are the optimal hyperparameters to compress YOLOv7?\\n6.1 Experiments\\nThe goal of the experiments is to compress YOLOv7 to the same number of FLOPs as', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='YOLOv7-tiny . As shown in T able 2.1 , YOLOv7 has about 7.7 times more FLOPs than\\nYOLOv7-tiny , which corresponds to a FLOPs pruning ratio (FPR) of 13.2%. Given that\\nfor YOLOv7-tiny a FPR of 42% was achieved with k= 0.6, it is expected that a large value\\nofkis needed to compress YOLOv7 to the required FPR of 13.2%. There is no way to de-\\ntermine exactly what kwill result in enough compression, but fortunately , by increasing the', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='number of pruning cycles, it is possible to keep compressing the network further. This way it\\nis always possible to achieve the required FPR, but the increased training time might make\\nthis infeasible.\\nSince it is diﬀicult to know which kwill produce the required compression, the experiments\\nstart with k= 0.9running till it reaches this compression. Based on the results of this\\nexperiment, the value of kwill be adjusted. If it takes longer than three pruning cycles to', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='reach the required compression, higher values of kwill be tested and vice versa.\\n6.2 Implementation Details\\nMost of the implementation details of GBIP for YOLOv7 are identical to YOLOv7-tiny , which\\ncan be found in Subsection 4.3.2 .\\n6.2.1 Attention T ransfer\\nGiven that the output of ELAN blocks worked well for attention transfer on YOLOv7-tiny ,\\nthese are used as starting point for YOLOv7. T able 6.1 shows which layers have been tested.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='ELAN refers to all ELAN blocks, DOWN to the downsample layers at the first bottom-to-\\ntop pathway . Layers 51, 54 and 66 link the bottom-to-top pathway with the top-to-bottom\\npathway . See Appendix A.2 for a detailed overview of the YOLOv7 architecture with indices.\\nThe best result is highlighted in bold and will be used in the experiments.\\n6.2.2 Pruning Batch Size\\nSince the importance score for each output channel is computed based on the activation values,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='instead of the weight values, data is required. The amount of data used to calculate the\\nimportance scores, called the pruning batch size, can significantly increase the pruning time.\\nThe authors of GBIP do not mention how much data is used, or if they use the entire dataset.\\n37', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 42}),\n",
       " Document(page_content='T able 6.1 :Layer indices of atten-\\ntion maps tested for Attention\\nT ransfer. See Appendix A.2\\nfor the YOLOv7 architecture\\nwith indices. YOLOv7 has\\nbeen pruned with k= 0.8\\nand finetuned for one epoch.\\nDOWN: downsample layers\\n16, 29, 42Layer IndicesFitness\\nIncrease\\nELAN, 51 1.05\\nELAN, DOWN 1.12\\nELAN, 51, 54, 66 1.51\\nELAN, 51, DOWN 0.85\\nELAN, 51, DOWN, 54, 66 1.61\\nT o find a reasonable value for the pruning batch size, the pruning step is run for increasing', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 43}),\n",
       " Document(page_content='pruning batch sizes and recording which output channels are pruned. The results for four\\ndifferent layers of the network are shown in Figure 6.1 . At small batch sizes the number and\\nlocation of pruned channels varies, this is especially clear for layer 24 (bottom left). At larger\\nbatch size there only a few channels, with importance scores close to the pruning threshold,\\nthat still show change. F rom these figures the pruning batch size has been set to 210training\\nsamples.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 43}),\n",
       " Document(page_content='samples.\\nFigure 6.1 :Pruning YOLOv7 with k= 0.9for a different number of training samples, or\\npruning batch size. Orange indicates the output channel at this index is pruned. Solid\\nhorizontal lines mean that whether this channel is pruned does not change with increasing\\nbatch size.\\n6.2.3 Ablation Study\\nGiven the change of network, an ablation study is again performed to the test the importance\\nof GBIP’s finetuning techniques: attention transfer, output transfer and adversarial game.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 43}),\n",
       " Document(page_content='Since higher values of kare expected, the ablation study will run with k= 0.9. Based on the\\nresults of the ablation study on YOLOv7-tiny , the number of epochs for each test is reduced to\\n38', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 43}),\n",
       " Document(page_content='three. At three epochs the relative difference between techniques was obvious in YOLOv7-tiny ,\\nwith only a slight increase in fitness afterwards.\\nT able 6.2 shows the results of the ablation study . Like YOLOv7-tiny , attention transfer\\nagain has the largest effect on accuracy recovery . But this time combining attention transfer\\nwith output transfer has a slight edge over just attention transfer. F or this reason, attention\\ntransfer and output transfer are both used in the experiments.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='T able 6.2 :Results of the ablation\\nstudy on YOLOv7. All exper-\\niments are performed with k=\\n0.9. A T: Attention T ransfer,\\nOT: Output T ransfer, AG: Ad-\\nversarial Game. Fitness and AP\\nare percentages. Best results are\\nmarked in bold.A T OT AG Fitness AP AP 50\\n- - - 41.2 39.5 57.3\\n✓ - - 44.2 42.3 60.7\\n-✓ - 43.4 41.6 59.3\\n- -✓ 42.0 40.2 58.0\\n✓ ✓ - 44.3 42.5 60.5\\n6.2.4 Learning Rate\\nT o decide the initial learning rate a learning rate range test [ 73] is performed. While training', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='the learning rate is slowly increased from a very low ( 10−6) to a very high (1) value while\\nlogging the loss value. The optimal learning rate, where the loss decreases fastest, is obtained\\nby taking the derivative of the loss with respect to the learning rate. A small amount of\\nsmoothing, using a moving average, is applied to filter out noise for both the loss and its\\nderivative.\\nFigure 6.2 shows the loss and its derivative for the ablation study experiments. The loss', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='decreases up to a learning rate of around 10−2. F rom the derivative plot, the optimal learning\\nrate for ’None’ (no finetuning techniques) is around 3×10−3. The other variants are all very\\nsimilar to each other (which is why only ’A T’ is shown in color) and have an optimal learning\\nrate around 6×10−4.\\nF or all experiments, including the ablation study , this approach has been taken to deter-\\nmine the optimal learning rate. Not only at the start of the experiment, but also after each', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='successive pruning step. This learning rate is still used in combination with cosine annealing\\nas described in Subsection 2.2.3 .\\nThis chapter explained how the experiments will be run. It answered the question: What\\nare the optimal hyperparameters to compress YOLOv7? The layers used for attention transfer\\nare indicated in T able 6.1 . The pruning batch size is set to 210based on Figure 6.1 . An\\nablation study (Subsection 6.2.3 ) is performed to determine that attention transfer and ouput', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='transfer will be used as finetuning techniques. And the procedure for determining the learning\\nrate is explained in Subsection 6.2.4 .\\n39', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 44}),\n",
       " Document(page_content='(a) Loss.\\n (b) Derivative of the loss.\\nFigure 6.2 :Determining learning rates for k= 0.9using different finetuning techniques. Learn-\\ning rate is slowly increased while monitoring loss. Maximum learning rate is at the point\\nwhere the loss starts increasing. The minimum of the derivative of the loss indicates the\\noptimal learning rate where the loss decreases the fastest. F or ’None’ this optimal learning\\nrate lies around 2×10−3, while for the other variants it is around 6×10−4.\\n40', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 45}),\n",
       " Document(page_content='7 Results\\nThis chapter reports the results of compressing YOLOv7 with GBIP . In Section 7.1 , these\\nresults are used to answer the main research question: Can a state-of-the-art compression\\nof YOLOv7 achieve higher accuracy than YOLOv7-tiny at the same number of floating-point\\noperations? In Section 7.2 , the different compressed models are evaluated and compared,\\nanswering the questions: Does one large pruning step work better than several smal ler pruning', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='steps? and: Why is the selected compression method not able to outperform YOLOv7-tiny?\\n7.1 Final Results\\nYOLOv7 has been compressed with different values of k, stopping when the FLOPs Pruning\\nRatio (FPR) is close to that of YOLOv7-tiny (13.2). T able 7.1 gives the results of these\\nexperiments. The best compressed network is achieved with k= 0.95 after 2 pruning steps.\\nk= 0.92 has a very small fitness increase, but a worse FPR.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-\\ntiny at the same number of floating-point operations? Surprisingly , none of the compressed\\nnetworks achieves a fitness close to that of YOLOv7-tiny . This means that it is better to use\\nthe handcrafted YOLOv7-tiny than compressing YOLOv7 with a state-of-the-art compression\\nmethod.\\nT able 7.1 :Results of GBIP\\nexperiments on YOLOv7.\\nFitness, AP , FLOPs\\nPruning Ratio (FPR)\\nand Parameter Pruning', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='Ratio (PPR) are given as\\npercentages. Note k= 0\\nrefers to the unpruned\\nbaseline.kPruning\\nStepsFitness AP AP 50 PPR FPR\\nYOLOv7-tiny 39.2 37.4 55.2 16.8 13.2\\n0.9 5 22.9 21.6 34.3 20.9 13.0\\n0.91 3 27.0 25.6 39.5 19.6 12.6\\n0.92 3 28.5 27.0 41.3 19.3 12.6\\n0.95 2 28.4 27.0 41.3 15.3 11.3\\n1.015 1 21.5 20.3 32.5 12.8 13.1\\nThe fitness and FPR after each pruning step for all experiments are shown in Figure 7.1 .\\nThe bottom left of the left plot, where the compressed models reach the required FPR, is', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='magnified on the right. The fitness degrades quicker the further the model is compressed.\\nThis is expected since at first less relevant information can be removed with little accuracy\\nloss, but at lower FPR all unimportant filters are already removed and compressing further\\nreduces the accuracy faster.\\nIt also seems that with many pruning steps, like k= 0.9with 5 steps, the accuracy\\ndecreases faster than with only two or three pruning steps. Therefore, k= 1.015 has been', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='tested to see if this extends to only one pruning step. However, after pruning once with\\nk= 1.015 the required FPR is reached, but the resulting fitness is below even that of k= 0.9.\\n41', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 46}),\n",
       " Document(page_content='Figure 7.1 :Fitness vs FPR for GBIP experiments after each pruning step for different k. Final\\nmodels are at the left bottom (within the black box) and are enlarged on the plot on the\\nright. Solid lines indicate FPR (13.2%) and fitness (39.2%) of YOLOv7-tiny .\\n7.2 Comparison\\nThis section answers the questions: Does one large pruning step work better than several\\nsmal ler pruning steps? and: Why is the selected compression method not able to outperform\\nYOLOv7-tiny?', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='YOLOv7-tiny?\\nFigure 7.2 shows the pruning ratio of each layer in YOLOv7 after the first and last pruning\\nstep for different values of k. Looking at Figure 7.2a , obviously a higher kcompresses the\\nnetwork further in one step. F or k= 0.9to 0.92 and to 0.95 the pruning ratio decreases quite\\ngradually with highs and lows in the same locations. However, for k= 1.015 the pruning ratio\\ncurve is almost flat, with only minor variation between layers. This suggests that at such high', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='kthe compression is no longer able to distinguish between layers with important filters and\\nthose with less important filters.\\nIn Figure 7.2b all models have very similar FPR. k= 1.015 is again an outlier, but the\\nother curves all follow a similar trajectory . However, the lower k, the more accentuated the\\npeaks and valleys are. This means that certain layers are compressed extremely far, while\\nothers are compressed significantly less compared to higher k.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='It seems that this is mostly caused by the number of pruning steps. Figure 7.3 shows the\\npruning ratio for all pruning steps at k= 0.9. Each successive pruning step further accentuates\\nthe peaks and valleys, resulting in an extremely low pruning ratio for certain layers, which\\nmight create a bottleneck for the information flow in the network and thus cause increased\\naccuracy degradation.\\nSo far it is found that large k, like 1.015, results in a very even pruning ratio, which', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='ignores the relative importance of the layers. At the same time, low values of krequire more\\npruning steps and thus over compress less important layers. These two facts explain why the\\nexperiments with 2 or 3 pruning steps perform better than the others.\\nIt is also interesting to look at the locations of the layers with high and low pruning ratio.\\nF rom Figure 7.2b it can be seen that layers 32 to 50, 84 to 86 and 96 to 99 have a relative', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='high pruning ratio, suggesting that these layers contain more relevant information. Figure 7.4b\\nhighlights the location of these layers in the YOLOv7 architecture.\\nSimilarly , layers 1 to 21 and 63 to 78 have an extremely low pruning ratio. The location of\\nthese layers is shown in Figure 7.4a . Comparing the locations of high and low pruning ratios,\\n42', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 47}),\n",
       " Document(page_content='(a) After first pruning step.\\n (b) After last pruning step.\\nFigure 7.2 :Pruning Ratio of each convolutional layer in YOLOv7 for different values of kafter\\n(a) the first and (b) last pruning step.\\nFigure 7.3 :Pruning Ratio of each convolutional layer in YOLOv7 after each pruning step at\\nk= 0.9.\\nthe detections at higher scales are clearly prioritized over that at the lower scale. At the higher\\nscales, the information can flow through several paths, but at the lowest scales there is only', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 48}),\n",
       " Document(page_content='one path. It is precisely this path that is compressed very heavily . And there is no way to\\nrecover the information later in the network. This explains why k= 0.9with 5 pruning steps,\\nwhich has the most extreme compression in these layers, performs worse than those with only\\ntwo or three pruning steps.\\nSo, does one large pruning step work better than several smal ler pruning steps? No,\\nfrom the comparison above it can be concluded that one large pruning step does not work', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 48}),\n",
       " Document(page_content='better than several smaller pruning steps. At the same time, too many steps also hurt the\\nperformance. In this case, the optimal number of pruning steps is found to be two or three.\\nThe results in this chapter shows that YOLOv7-tiny outperforms YOLOv7 compressed with\\nGBIP . But why is the selected compression method not able to outperform YOLOv7-tiny? One\\nreason for this seems to be the fact that GBIP can only remove filters and not entire layers.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 48}),\n",
       " Document(page_content='Especially in the lower layers, any information that is removed cannot be recovered. At the\\nsame time some layers are left with only a few percent of their weights, making it impossible to\\npass all input information through to the next layer. This means that a layer with only a few\\nweights will decrease the accuracy , and it would be better to remove the layer in its entirety .\\n43', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 48}),\n",
       " Document(page_content='(a) Blue: Low FPR.\\n (b) Red: High FPR.\\nFigure 7.4 :Location of layers with high and low FPR.\\nAnother way to avoid the loss of information in the earlier layers is to change the pruning\\nthreshold to allow less compression in the earlier layers and more in the later layers. Both\\nchanges would require a lot of extra testing and are left as recommendations for further re-\\nsearch.\\n44', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 49}),\n",
       " Document(page_content='8 Conclusion\\nIn this final chapter the research questions are answered in Section 8.1 and some recommen-\\ndations for further research are given in Section 8.2 .\\n8.1 Research Questions\\n8.1.1 Main Research Question\\nThe goal of this thesis is to see if compression of a large neural network can achieve better\\nresults than a hand-designed smaller network. Specifically:\\nCan a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 50}),\n",
       " Document(page_content='tiny at the same number of floating-point operations?\\nF rom the experiments done on YOLOv7 with GBIP , it turns out that YOLOv7-tiny outper-\\nforms the compressed YOLOv7. The best performing model resulted from k= 0.95, which\\nachieves a 11.3% FLOPs Pruning Ratio (FPR) with a fitness of 28.4%. YOLOv7-tiny (with a\\nFPR of 13.2%) achieves a significantly higher fitness of 39.2%.\\nClearly , YOLOv7-tiny is preferable over YOLOv7 compressed with GBIP .\\n8.1.2 Sub Questions', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 50}),\n",
       " Document(page_content='8.1.2 Sub Questions\\n1. What is the state-of-the-art in neural network compression?\\nSection 2.3 gives some background information about different types of compression techniques.\\nIn general, these techniques can be divided into four categories: pruning, quantization, tensor\\ndecomposition and compact architectures. In Chapter 3, based on a search of the literature, a\\nlist of best performing compression methods is compiled. The current state-of-the-art achieves', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 50}),\n",
       " Document(page_content='a FPR of 11.2% with ResNet-56 on CIF AR-10 [ 13] and 21.3 % FPR with ResNet-50 on Ima-\\ngeNet [ 66], both with an accuracy loss of less than 1%.\\n2. What are the best networks for comparing compression methods?\\nThe main reason a network is suitable for comparing compression methods, is the fact that\\nmost recent papers report results using this network. F rom the literature, Section 3.1 found\\nthat the best networks are ResNet-50 trained on ImageNet and ResNet-56 trained on CIF AR-\\n10.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 50}),\n",
       " Document(page_content='10.\\n3. Which compression method is best suited for compressing YOLOv7?\\nF rom the list of best performing compression methods (T able 3.1 ), two methods were selected:\\nGBIP and KSE. These methods were selected after first removing methods with very high\\nfinetuning requirements.\\n45', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 50}),\n",
       " Document(page_content='Both GBIP and KSE have been tested on YOLOv7-tiny . Chapter 4goes into the im-\\nplementation details of these methods for YOLOv7-tiny . Based on the results (Chapter 5),\\nGBIP is chosen as best method for compressing YOLOv7. KSE could theoretically outperform\\nGBIP , but as mentioned in Section 5.2 , these theoretical values were unattainable.\\n4. What are the optimal hyperparameters to compress YOLOv7?\\nIn Chapter 6the hyperparameters are discussed. The learning rate is determined by a learning', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='rate range test (Section 6.2.4 ). The learning rate is slowly increased from a very low to very\\nhigh values while logging the loss. Using the derivative of this loss curve, the learning rate\\nwhere the loss decreases fastest can be selected.\\nAnother hyperparameter that requires tuning is the number of training samples, or prun-\\ning batch size, used for calculating the importance score. Section 6.2.4 shows that after 210', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='training samples there is almost no more change in which filters gets pruned.\\nIn the GBIP paper, three finetuning techniques are introduced to recover the accuracy\\nof a pruned network: attention transfer, output transfer and adversarial game. An ablation\\nstudy (Section 6.2.3 ) shows that for YOLOv7 a combination of attention and output transfer\\nworks best.\\n5. Does one large pruning step work better than several smaller pruning steps?', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='F rom the experiments on YOLOv7 (Chapter 7), it shows that using only one pruning step does\\nnot achieve good results. In that case, the layers of the network are very evenly compressed,\\nignoring any difference in relevance between layers. On the other hand, with five pruning steps\\nthe compression is very uneven over the layers, to the point that less than 1% of the parameters\\nare left in several layers. F or the experiments in this thesis, two or three pruning steps results', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='in the best accuracy . Incidentally , in the GBIP paper the number of pruning steps is also set\\nto two or three, although no explanation is given for this choice.\\n6. Why is the selected compression method not able to outperform YOLOv7-tiny?\\nIt is diﬀicult to give a definitive answer to this question. One very probable answer lies in the\\nway GBIP compresses a network, namely by removing filters. YOLOv7-tiny has fewer layers', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='than YOLOv7, but GBIP cannot remove entire layers, so all the information must still flow\\nthrough each layer. This means that even if a layer is not necessary it still needs a way to pass\\nthe information on to the next layer and the weights cannot be set to zero.\\n8.2 Recommendations\\nBased on the experiments in this thesis there are several recommendations for future research.\\nOne obvious recommendation is further pursuing the optimization of KSE. In theory KSE', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='should be able to outperform GBIP , but the optimization was unsuccessful. It is very well\\npossible that with a custom CUDA implementation of KSE the results will be closer to the\\ntheory .\\nAnother recommendation is investigating a varying pruning threshold. Currently , the\\npruning threshold is the same in each layer of the network. However, if the compression in the\\nearly layers is too large, it is impossible to recover the lost information. Maybe by allowing', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='lower compression in the early layers and more in the later layers, it is possible to achieve\\nbetter accuracy at the same FPR.\\n46', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 51}),\n",
       " Document(page_content='The last recommendation is testing the removal of entire layers. Based on the performance\\nof YOLOv7-tiny it is clear that a network with fewer layers can outperform a compressed\\nnetwork with more layers. However, allowing the removal of layers while compressing might\\nresult in a model that can outperform YOLOv7-tiny .\\n47', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 52}),\n",
       " Document(page_content='A Architectures\\nA.1 YOLOv7-tiny\\nFigure A.1 :YOLOv7-tiny architecture. Name of each block is given on top of the box; output\\ndimensions are given below; numbers on the right indicate index of last layer in the block.\\nIn the CSPSPP block, the numbers under MaxPool indicate kernel and padding size.\\n49', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 54}),\n",
       " Document(page_content='A.2 YOLOv7\\nFigure A.2 :YOLOv7 architecture. Name of each block is given on top of the box; output\\ndimensions are given below; numbers on the right indicate index of last layer in the block.\\nIn the SPPFCSP block, the numbers under MaxPool indicate kernel and padding size. 2-4\\nand 1-4 ELAN blocks are show in Figure 2.6 .\\n50', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 55}),\n",
       " Document(page_content='Bibliography\\n[1] J. Steinbrener and P . Desai, “Comparison of deep learning architectures on embedded\\ndevices and generalized fixed-point conversion algorithm,” pp. 343–347, 06 2019.\\n[2] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521, pp. 436–44, 05\\n2015.\\n[3] V. Sze, Y.-H. Chen, T.-J. Y ang, and J. Emer, “Eﬀicient processing of deep neural net-\\nworks: A tutorial and survey ,” Proceedings of the IEEE , vol. 105, 03 2017.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 56}),\n",
       " Document(page_content='[4] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision and challenges,”\\nIEEE Internet of Things Journal , vol. 3, no. 5, pp. 637–646, 2016.\\n[5] R. Pilipovic, P . Bulić, and V. Risojević, “Compression of convolutional neural networks:\\nA short survey ,” pp. 1–6, 03 2018.\\n[6] R. Sanchez-Iborra and A. F. Skarmeta, “Tinyml-enabled frugal smart objects: Challenges\\nand opportunities,” IEEE Circuits and Systems Magazine , vol. 20, no. 3, pp. 4–18, 2020.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 56}),\n",
       " Document(page_content='[7] L. Dutta and S. Bharali, “Tinyml meets iot: A comprehensive survey ,” Internet of Things ,\\nvol. 16, p. 100461, 10 2021.\\n[8] F. Götz, “The data deluge: What do we do with the data\\ngenerated by avs?. ” https://blogs.sw.siemens.com/polarion/\\nthe-data-deluge-what-do-we-do-with-the-data-generated-by-avs/ , Jan 2021.\\nAccessed: 2023-11-06.\\n[9] K. Nan, S. Liu, J. Du, and H. Liu, “Deep model compression for mobile platforms: A', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 56}),\n",
       " Document(page_content='survey ,” T singhua Science and T echnology , vol. 24, pp. 677–693, 12 2019.\\n[10] Y. Cheng, D. W ang, P . Zhou, and T. Zhang, “A survey of model compression and accel-\\neration for deep neural networks,” 10 2017.\\n[11] K. Ullrich, E. Meeds, and M. W elling, “Soft weight-sharing for neural network compres-\\nsion,” 02 2017.\\n[12] J. Bi and S. Gunn, “Sparse deep neural networks for embedded intelligence,” pp. 30–38,\\n11 2018.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 56}),\n",
       " Document(page_content='11 2018.\\n[13] W. Zhang and Z. W ang, “F pfs: Filter-level pruning via distance weight measuring filter\\nsimilarity ,” Neurocomputing , vol. 512, pp. 40–51, 2022.\\n[14] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. W ang, T. W eyand, M. Andreetto,\\nand H. Adam, “Mobilenets: Eﬀicient convolutional neural networks for mobile vision\\napplications,” 04 2017.\\n[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,”\\n2015.\\n51', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 56}),\n",
       " Document(page_content='[16] C.-Y. W ang, A. Bochkovskiy , and H.-Y. M. Liao, “Y olov7: T rainable bag-of-freebies sets\\nnew state-of-the-art for real-time object detectors,” 2022.\\n[17] V. Nair and G. Hinton, “Rectified linear units improve restricted boltzmann machines,”\\nvol. 27, pp. 807–814, 06 2010.\\n[18] S. R. Dubey , S. K. Singh, and B. B. Chaudhuri, “A comprehensive survey and performance\\nanalysis of activation functions in deep learning,” CoRR , vol. abs/2109.14545, 2021.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='[19] D. Hendrycks and K. Gimpel, “Bridging nonlinearities and stochastic regularizers with\\ngaussian error linear units,” CoRR , vol. abs/1606.08415, 2016.\\n[20] R. J. T an, “Breaking down mean average precision (map). ” https://\\ntowardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52 ,\\nMar 2019. Accessed: 2023-11-06.\\n[21] M. Everingham, L. V an Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='visual object classes (voc) challenge,” International Journal of Computer Vision , vol. 88,\\npp. 303–338, June 2010.\\n[22] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P . Perona,\\nD. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft COCO: common objects in context,”\\nCoRR , vol. abs/1405.0312, 2014.\\n[23] “Object detection on pascal voc 2012. ” https://paperswithcode.com/sota/\\nobject-detection-on-pascal-voc-2012 . Accessed: 2023-11-06.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='[24] Z. Zhao, P . Zheng, S. Xu, and X. W u, “Object detection with deep learning: A review,”\\nCoRR , vol. abs/1807.05511, 2018.\\n[25] “Object detection on coco test-dev. ” https://paperswithcode.com/sota/\\nobject-detection-on-coco . Accessed: 2023-11-06.\\n[26] R. Kaur and S. Singh, “A comprehensive review of object detection with deep learning,”\\nDigital Signal Processing , vol. 132, p. 103812, 2023.\\n[27] S. Ren, K. He, R. B. Girshick, and J. Sun, “F aster R-CNN: towards real-time object', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='detection with region proposal networks,” CoRR , vol. abs/1506.01497, 2015.\\n[28] R. B. Girshick, “F ast R-CNN,” CoRR , vol. abs/1504.08083, 2015.\\n[29] A. Benali Amjoud and M. Amrouch, “Object detection using deep learning, cnns and\\nvision transformers: A review,” IEEE Access , vol. PP , pp. 1–1, 01 2023.\\n[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy , S. E. Reed, C. F u, and A. C. Berg, “SSD:\\nsingle shot multibox detector,” CoRR , vol. abs/1512.02325, 2015.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='[31] J. Redmon, S. K. Divvala, R. B. Girshick, and A. F arhadi, “Y ou only look once: Unified,\\nreal-time object detection,” CoRR , vol. abs/1506.02640, 2015.\\n[32] T. Lin, P . Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, “F eature\\npyramid networks for object detection,” CoRR , vol. abs/1612.03144, 2016.\\n[33] C.-Y. W ang, H.-Y. M. Liao, and I.-H. Y eh, “Designing network design strategies through\\ngradient path analysis,” 2022.\\n52', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 57}),\n",
       " Document(page_content='[34] A. Bochkovskiy , C. W ang, and H. M. Liao, “Y olov4: Optimal speed and accuracy of object\\ndetection,” CoRR , vol. abs/2004.10934, 2020.\\n[35] H. Zhang, M. Cissé, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk\\nminimization,” CoRR , vol. abs/1710.09412, 2017.\\n[36] T. Devries and G. W. T aylor, “Improved regularization of convolutional neural networks\\nwith cutout,” CoRR , vol. abs/1708.04552, 2017.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[37] Z. Zheng, P . W ang, W. Liu, J. Li, R. Y e, and D. Ren, “Distance-iou loss: F aster and\\nbetter learning for bounding box regression,” CoRR , vol. abs/1911.08287, 2019.\\n[38] H. Salehinejad and S. V alaee, “Ising-dropout: A regularization method for training and\\ncompression of deep neural networks,” 05 2019.\\n[39] J. Chang and S. Jin, “Prune deep neural networks with the modified l1/2 penalty ,” IEEE\\nAccess , vol. PP , pp. 1–1, 12 2018.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[40] M. Alnemari and N. Bagherzadeh, “Eﬀicient deep neural networks for edge computing,”\\npp. 1–7, 07 2019.\\n[41] P . Merolla, R. Appuswamy , J. Arthur, S. Esser, and D. Modha, “Deep neural networks\\nare robust to weight binarization and other non-linear distortions,” 06 2016.\\n[42] M. Courbariaux, I. Hubara, D. Soudry , R. El-Y aniv, and Y. Bengio, “Binarized neural\\nnetworks: T raining deep neural networks with weights and activations constrained to +1\\nor -1,” 02 2016.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='or -1,” 02 2016.\\n[43] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . MIT Press, 2016. http:\\n//www.deeplearningbook.org .\\n[44] Q. W u, W. Li, X. Lu, H. Zhang, H. Luo, and C. Lei, “A deep learning model compression\\nalgorithm based on optimal clustering,” p. 171, 05 2019.\\n[45] S. Han, H. Mao, and W. Dally , “Deep compression: Compressing deep neural networks\\nwith pruning, trained quantization and huffman coding,” 10 2016.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[46] J. Xue, J. Li, and Y. Gong, “Restructuring of deep neural network acoustic models with\\nsingular value decomposition,” Proceedings of the Annual Conference of the International\\nSpeech Communication Association, INTERSPEECH , pp. 2365–2369, 01 2013.\\n[47] Y.-D. Kim, E. Park, S. Y oo, T. Choi, L. Y ang, and D. Shin, “Compression of deep\\nconvolutional neural networks for fast and low power mobile applications,” 05 2016.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[48] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky , “Speeding-up con-\\nvolutional neural networks using fine-tuned cp-decomposition,” 12 2014.\\n[49] A. Novikov, D. Podoprikhin, A. Osokin, and D. V etrov, “T ensorizing neural networks,”\\n09 2015.\\n[50] F. Iandola, S. Han, M. Moskewicz, K. Ashraf, W. Dally , and K. Keutzer, “Squeezenet:\\nAlexnet-level accuracy with 50x fewer parameters and <0.5mb model size,” 02 2016.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[51] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 03\\n2015.\\n53', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 58}),\n",
       " Document(page_content='[52] M. V. de Carvalho and M. Pratama, “Improving shallow neural network by compressing\\ndeep neural network,” pp. 1382–1387, 11 2018.\\n[53] C.-J. Chen, K.-C. Chen, and M.-c. Martin-Kuo, “Acceleration of neural network model\\nexecution on embedded systems,” pp. 1–3, 04 2018.\\n[54] O. Russakovsky , J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy ,\\nA. Khosla, M. Bernstein, A. C. Berg, and L. F ei-F ei, “ImageNet Large Scale Visual', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='Recognition Challenge,” International Journal of Computer Vision (IJCV) , vol. 115, no. 3,\\npp. 211–252, 2015.\\n[55] A. Krizhevsky , “Learning multiple layers of features from tiny images,” University of\\nT oronto , 05 2012.\\n[56] “Image classification on imagenet. ” https://paperswithcode.com/sota/\\nimage-classification-on-imagenet . Accessed: 2023-11-06.\\n[57] “Image classification on cifar-10. ” https://paperswithcode.com/sota/\\nimage-classification-on-cifar-10 . Accessed: 2023-11-06.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='[58] Y. Lecun, L. Bottou, Y. Bengio, and P . Haffner, “Gradient-based learning applied to\\ndocument recognition,” Proceedings of the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998.\\n[59] S. V adera and S. Ameen, “Methods for pruning deep neural networks,” CoRR ,\\nvol. abs/2011.00241, 2020.\\n[60] W. Zhang and Z. W ang, “Pca-pruner: Filter pruning by principal component analysis,”\\nJ. Intel l. F uzzy Syst. , vol. 43, pp. 4803–4813, jan 2022.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='[61] M. U. Haider and M. T aj, “Comprehensive online network pruning via learnable scaling\\nfactors,” CoRR , vol. abs/2010.02623, 2020.\\n[62] C. Sarvani, M. Ghorai, S. R. Dubey , and S. S. Basha, “HRel: Filter pruning based on\\nhigh relevance between activation maps and class labels,” Neural Networks , vol. 147,\\npp. 186–197, mar 2022.\\n[63] W. Gao, Y. Guo, S. Ma, G. Li, and S. Kwong, “Eﬀicient neural network compression', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='inspired by compressive sensing,” IEEE T ransactions on Neural Networks and Learning\\nSystems , pp. 1–15, 2022.\\n[64] J. Chang, Y. Lu, P . Xue, Y. Xu, and Z. W ei, “Global balanced iterative pruning for\\neﬀicient convolutional neural networks,” Neural Computing and Applications , Jul 2022.\\n[65] J. Zhu and J. Pei, “Progressive kernel pruning cnn compression method with an adjustable\\ninput channel,” Applied Intel ligence , vol. 52, pp. 1–22, 07 2022.', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='[66] Y. Li, S. Lin, B. Zhang, J. Liu, D. S. Doermann, Y. W u, F. Huang, and R. Ji,\\n“Exploiting kernel sparsity and entropy for interpretable CNN compression,” CoRR ,\\nvol. abs/1812.04368, 2018.\\n[67] K. Zhao, A. Jain, and M. Zhao, “Iterative activation-based structured pruning,” CoRR ,\\nvol. abs/2201.09881, 2022.\\n[68] M. R. Ganesh, J. J. Corso, and S. Y. Sekeh, “MINT: deep network compression via mutual\\ninformation-based neuron trimming,” CoRR , vol. abs/2003.08472, 2020.\\n54', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 59}),\n",
       " Document(page_content='[69] H. Hu, R. Peng, Y.-W. T ai, and C.-K. T ang, “Network trimming: A data-driven neuron\\npruning approach towards eﬀicient deep architectures,” 07 2016.\\n[70] S. Zagoruyko and N. Komodakis, “Paying more attention to attention: Improv-\\ning the performance of convolutional neural networks via attention transfer,” CoRR ,\\nvol. abs/1612.03928, 2016.\\n[71] S. Lin, R. Ji, C. Y an, B. Zhang, L. Cao, Q. Y e, F. Huang, and D. S. Doermann,', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 60}),\n",
       " Document(page_content='“T owards optimal structured CNN pruning via generative adversarial learning,” CoRR ,\\nvol. abs/1903.09291, 2019.\\n[72] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury , G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Y ang, Z. De Vito, M. Raison, A. T e-\\njani, S. Chilamkurthy , B. Steiner, L. F ang, J. Bai, and S. Chintala, “Pytorch: An imper-\\native style, high-performance deep learning library ,” in Advances in Neural Information', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 60}),\n",
       " Document(page_content='Processing Systems 32 , pp. 8024–8035, Curran Associates, Inc., 2019.\\n[73] L. N. Smith, “No more pesky learning rate guessing games,” CoRR , vol. abs/1506.01186,\\n2015.\\n55', metadata={'source': 'pdfs\\\\Thesis.pdf', 'page': 60})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3b6dcf8-10ca-4695-9e38-ffbdfa3a78ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c74ae779-f515-49d7-89a4-b43df8d9b574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing YOLOv7\n",
      "Benjamin van Zwienen\n",
      "Master of Science Thesis\n",
      "November 24, 2023\n",
      "Student Number: 4471180\n",
      "Supervisor: Prof. Dr. Ir. M. Wisse\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83ceb1c-0326-4f98-9cb6-d002c749cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real-time object detectors with 30 FPS or higher on GPU\n",
      "V100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\n",
      "AP) outperforms both transformer-based detector SWIN-\n",
      "L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\n",
      "509% in speed and 2% in accuracy, and convolutional-\n",
      "based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\n",
      "FPS A100, 55.2% AP) by 551% in speed and 0.7% AP\n",
      "in accuracy, as well as YOLOv7 outperforms: YOLOR,\n",
      "YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f5f7712-6d61-40b2-a4d3-8788e3668f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "called YOLOv7-tiny , was manually designed by the authors of YOLOv7. This thesis answers\n",
      "the question: Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than\n",
      "YOLOv7-tiny at the same number of floating-point operations?\n",
      "First, two state-of-the-art compression methods are selected and compared on YOLOv7-\n",
      "tiny . Then the best performing method, GBIP , is used to compress YOLOv7 till it has the\n",
      "same number of FLOPs as YOLOv7-tiny . F rom the experiments it is determined that GBIP\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c722043c-cc24-4f66-a769-25ccd09f9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Systems 32 , pp. 8024–8035, Curran Associates, Inc., 2019.\n",
      "[73] L. N. Smith, “No more pesky learning rate guessing games,” CoRR , vol. abs/1506.01186,\n",
      "2015.\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[283].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b36c4504-cbf2-4c9c-a573-a97ac8839116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d820bc5d-cda3-4778-9ec9-9b4b3c510727",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63b6db4e-9702-42c4-b360-30271dc11351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.embed_query(\"How are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c92e374f-c926-4826-82f6-0b7c7506bfff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding.embed_query(\"hi i am fine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "343fbe13-33ec-4585-8526-936b4fcf9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b099781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6eeba7-d93b-4342-af74-9adfd7a83fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dba196af-50d0-4ed0-9106-740e65100fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca19dfdb-60b8-4093-a5e2-83d340aa0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"testing\" # put in the name of your pinecone index here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b085b338-5b84-41a6-9bae-dc9ed07b76d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index('testing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd115ec6-5a94-4274-9a28-e9c2539ea783",
   "metadata": {},
   "source": [
    "## Create Embeddings for each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc5a27c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_pinecone\n",
      "  Downloading langchain_pinecone-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_pinecone) (0.1.38)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_pinecone) (1.26.4)\n",
      "Requirement already satisfied: pinecone-client<4,>=3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_pinecone) (3.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (0.1.38)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2,>=0.1->langchain_pinecone) (8.2.3)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client<4,>=3->langchain_pinecone) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client<4,>=3->langchain_pinecone) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client<4,>=3->langchain_pinecone) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pinecone-client<4,>=3->langchain_pinecone) (2.2.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1->langchain_pinecone) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1->langchain_pinecone) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_pinecone) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2,>=0.1->langchain_pinecone) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_pinecone) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-core<0.2,>=0.1->langchain_pinecone) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\alams\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.64.1->pinecone-client<4,>=3->langchain_pinecone) (0.4.6)\n",
      "Downloading langchain_pinecone-0.0.3-py3-none-any.whl (8.3 kB)\n",
      "Installing collected packages: langchain_pinecone\n",
      "Successfully installed langchain_pinecone-0.0.3\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f56ed82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing YOLOv7\n",
      "Benjamin van Zwienen\n",
      "Master of Science Thesis\n",
      "November 24, 2023\n",
      "Student Number: 4471180\n",
      "Supervisor: Prof. Dr. Ir. M. Wisse\n",
      "Abstract\n",
      "In the literature, neural network compression can significantly reduce the number of floating-\n",
      "point operations (FLOPs) of a neural network with limited accuracy loss. At the same\n",
      "time, it is common to manually design smaller networks instead of using modern compres-\n",
      "sion techniques. This thesis will compare the two approaches for the object detection network\n",
      "YOLOv7. YOLOv7 can run in real time on a desktop GPU. F or edge GPUs a smaller version,\n",
      "called YOLOv7-tiny , was manually designed by the authors of YOLOv7. This thesis answers\n",
      "the question: Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than\n",
      "YOLOv7-tiny at the same number of floating-point operations?\n",
      "First, two state-of-the-art compression methods are selected and compared on YOLOv7-\n",
      "tiny . Then the best performing method, GBIP , is used to compress YOLOv7 till it has the\n",
      "same number of FLOPs as YOLOv7-tiny . F rom the experiments it is determined that GBIP\n",
      "is not able to achieve higher accuracy than YOLOv7-tiny at the same number of FLOPs.\n",
      "iii\n",
      "Contents\n",
      "1 Introduction 1\n",
      "1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n",
      "1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n",
      "1.3 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "2 Background 5\n",
      "2.1 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.1.2 Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
      "2.2 YOLOv7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2.1 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "2.2.3 T raining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "2.3 Compressing Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "2.3.1 Compression T echniques . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "2.3.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "2.3.3 Benchmark Datasets and Networks . . . . . . . . . . . . . . . . . . . . . 16\n",
      "3 Methods 19\n",
      "3 Methods 19\n",
      "3.1 Compression Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "3.2 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "3.2.1 Output T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "3.2.2 Attention T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "3.2.3 Adversarial Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "3.2.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "3.3 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "3.3.1 Kernel Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "3.3.2 Kernel Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "3.3.3 Kernel Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n",
      "3.3.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "4 YOLOv7-tiny Experiments 27\n",
      "4.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n",
      "4.2 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n",
      "4.2.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n",
      "4.2.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n",
      "4.3 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "4.3.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "4.3.2 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "v\n",
      "5 Selecting the Best Method 31\n",
      "5.1 Compression Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "5.1.1 GBIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "5.1.2 KSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
      "5.2 Best Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
      "5.2.1 Comparing GBIP and KSE . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
      "5.2.2 PyT orch optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n",
      "5.2.3 Selecting Best Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n",
      "6 YOLOv7 Experiments 37\n",
      "6.1 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "6.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "6.2.1 Attention T ransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "6.2.2 Pruning Batch Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "6.2.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n",
      "6.2.4 Learning Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "7 Results 41\n",
      "7.1 Final Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "7.2 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n",
      "8 Conclusion 45\n",
      "8 Conclusion 45\n",
      "8.1 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n",
      "8.1.1 Main Research Question . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n",
      "8.1.2 Sub Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n",
      "8.2 Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n",
      "A Architectures 49\n",
      "A.1 YOLOv7-tiny . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n",
      "A.2 YOLOv7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n",
      "vi\n",
      "1 Introduction\n",
      "1.1 Motivation\n",
      "In the last decade, the performance of neural networks has increased dramatically . They\n",
      "have become state of the art in a wide range of areas, such as image classification, object\n",
      "detection and localization, and speech recognition [ 1,2]. A lot of the improvements have come\n",
      "through creating larger and more complex models. Besides a larger memory footprint, complex\n",
      "neural networks take longer to train and run inference and have higher energy consumption.\n",
      "Therefore, neural networks are usually run on servers with high-end GPUs, allowing for the\n",
      "increased computational demand needed for increased accuracy [ 3].\n",
      "There are a lot of edge devices, like smartphones, home automation devices and robots,\n",
      "which could also benefit from large neural networks. There are two paradigms for providing\n",
      "machine learning to these devices: cloud computing and edge computing [ 4]. In the first case,\n",
      "the edge device sends its data to a server with powerful hardware, which runs neural network\n",
      "and sends back the result [ 5]. In the latter case, all the computation is handled on the edge\n",
      "device itself.\n",
      "There are several drawbacks to cloud computing since all data must be transmitted to\n",
      "a server [ 6,7]. Especially for edge devices with a lot of sensors, the network bandwidth will\n",
      "become a problem. F or example, an autonomous car is estimated to produce 3 Gbit of data\n",
      "per second [ 8]. F or real time applications, the latency of the connection might be too high to\n",
      "function at the desired speed. Also, in safety-critical situations, the internet connection must\n",
      "be extremely reliable. And lastly , certain edge devices, especially health monitoring devices,\n",
      "collect sensitive, personal data which people might not want uploaded to a server.\n",
      "Although cloud computing has its drawbacks, edge computing is not always the obvious\n",
      "choice. Edge devices are typically constrained in terms of memory , computation and energy\n",
      "usage. This makes running large neural networks on these devices infeasible. Assuming large\n",
      "neural networks are required, upgrading the hardware or using cloud computing are reasonable\n",
      "options. However, there has also been a lot of research into creating smaller neural networks\n",
      "that can deal with the constraints of edge devices [ 5,9,10]. The creation of smaller networks\n",
      "can be split into two categories: designing new, compact architectures and compressing existing\n",
      "architectures.\n",
      "Compression of existing neural networks is based on the observation that large networks\n",
      "only need a part of their parameters for accurate predictions [ 11]. The amount of compression\n",
      "depends on the network and dataset, with a reduction in floating-point operations (FLOPs) of\n",
      "more than 99% for LeNet-5 on MNIST [ 12] and up to 89% for ResNet-56 on CIF AR-10 [ 13],\n",
      "all while the accuracy loss stays below 1%.\n",
      "Compact architectures are diﬀicult to compare to other compressed networks since they\n",
      "typically do not have a clear baseline. F or example, MobileNet [ 14] loses less than 1% accuracy\n",
      "with only 4% of FLOPs compared to VGG-16. However, MobileNet is not a smaller version\n",
      "of VGG-16, so it might just as well be compared with any other neural network. On the other\n",
      "hand, there are some architectures that could be compared, like the different ResNets [ 15].\n",
      "Maybe compressing ResNet-110 will lead to a model with higher accuracy and lower FLOPs\n",
      "than ResNet-50. No such comparison, between compact and compressed networks, has been\n",
      "found in the literature.\n",
      "1\n",
      "1.2 Research Questions\n",
      "Given the promising results of neural network compression, and the lack of comparison between\n",
      "compressed and compact networks, this thesis will focus on comparing these two for the case\n",
      "of YOLOv7 [ 16]. YOLOv7 is one of the best performing object detection networks. But given\n",
      "its size, it is meant to run on a powerful GPU. The creators of YOLOv7 have hand-designed\n",
      "YOLOv7-tiny especially for the use on edge devices. The goal of this thesis is to see if a\n",
      "state-of-the-art compression method will be able to compress YOLOv7 more eﬀiciently than\n",
      "the hand-designed YOLOv7-tiny .\n",
      "Main Research Question\n",
      "The main research question of this thesis is:\n",
      "Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-\n",
      "tiny at the same number of floating-point operations?\n",
      "Based on the compression literature, it seems plausible that a compression method should\n",
      "be able to outperform YOLOv7-tiny .\n",
      "Sub Questions\n",
      "Sub Questions\n",
      "T o answer the main research question, several sub questions will be answered throughout this\n",
      "thesis. These sub questions are listed and motivated below:\n",
      "1. What is the state of the art in neural network compression?\n",
      "It is important to understand what compression techniques exist and how they work, as well\n",
      "as the state-of-the-art performance.\n",
      "2. What are the best networks for comparing compression methods?\n",
      "T o make a fair comparison between compression methods, they should be evaluated on the\n",
      "same networks. It is, therefore, important to determine which networks are most suited for\n",
      "this comparison.\n",
      "3. Which compression method is best suited for compressing YOLOv7?\n",
      "Based on the latest research, two of the best methods will be assessed on YOLOv7-tiny to see if\n",
      "they can be adapted to the YOLOv7 architecture and select the best performing method. An\n",
      "important constraint on the compression methods for this thesis is the amount of finetuning\n",
      "required. There is no hard limit, but the compression method should be able to work on\n",
      "a standard GPU and complete within hours or days rather than weeks. This is also why\n",
      "YOLOv7-tiny is initially used to compare methods. YOLOv7-tiny runs and trains much faster,\n",
      "while the architecture is very similar to YOLOv7. Typically , both the reduction in FLOPs\n",
      "and parameters is reported for the compression of neural networks. This is also the case in\n",
      "this thesis. However, object detection is usually run in real-time, which makes speed, and thus\n",
      "FLOPs, the key factor.\n",
      "4. What are the optimal hyperparameters to compress YOLOv7?\n",
      "The best performing method on YOLOv7-tiny will be used to compress YOLOv7 till it reaches\n",
      "the same speed as YOLOv7-tiny , as measured by the number of FLOPs. T o achieve the best\n",
      "result, the hyperparameters of this method have to be adapted to YOLOv7.\n",
      "2\n",
      "5. Does one large pruning step work better than several smal ler pruning steps?\n",
      "The selected compression method runs multiple pruning steps. Therefore, this thesis will also\n",
      "test if several small pruning steps or one large step achieves better results.\n",
      "6. Why is the selected compression method not able to outperform YOLOv7-tiny?\n",
      "Surprisingly , it turns out the compressed YOLOv7 model is not able to outperform YOLOv7-\n",
      "tiny . The results have to be examined, to see why this is the case.\n",
      "1.3 Outline\n",
      "Chapter 2will provide the background knowledge for this thesis: a short introduction to neural\n",
      "networks and YOLOv7, is followed by an extensive overview of neural network compression.\n",
      "Chapter 3will explain the search method and the selected compression methods. These meth-\n",
      "ods will then be adapted for YOLOv7-tiny , which is the topic of Chapter 4, as well as the\n",
      "proposed experiments. In Chapter 5the results of these experiments are given, followed by\n",
      "the selection of the best method for YOLOv7.\n",
      "The adaptation of the selected method to YOLOv7, including the optimization of the\n",
      "hyperparameters, and the experiments are discussed in Chapter 6. The results are given in\n",
      "Chapter 7. Based on these results, the research questions will be answered in Chapter 8, and\n",
      "recommendations for future research are given.\n",
      "3\n",
      "2 Background\n",
      "This chapter will provide the background for the rest of the thesis. It starts by introducing\n",
      "the basics of neural networks in Section 2.1 . Section 2.2 will explain YOLOv7, the neural\n",
      "network used in this thesis. A deep dive into the compression of neural networks is presented\n",
      "in Section 2.3 .\n",
      "2.1 Neural Networks\n",
      "The notation used in the rest of the thesis will be introduced in this section. F ollowed by a\n",
      "brief overview of object detection and the relevant metrics.\n",
      "2.1.1 Basics\n",
      "2.1.1 Basics\n",
      "This thesis assumes a basic understanding of neural networks. Here, the activation functions\n",
      "used in YOLOv7 are introduced, as well as the notation for a convolutional layer. Different\n",
      "papers use slightly different notations, thus it is important to explicitly specify the notation\n",
      "used in this thesis.\n",
      "Activation functions\n",
      "Activation functions introduce nonlinearity to a neural network. They are usually added after\n",
      "each fully-connected or convolution operation. F or a long time, the most common activation\n",
      "function was the Rectified Linear Unit (ReLU) [ 17]. In the past years more complex functions\n",
      "have been proposed [ 18]. One such activation function, which is used in YOLOv7, is the\n",
      "Sigmoid Linear Unit (SiLU) [ 19].\n",
      "Another common activation function is the sigmoid function. In case of YOLOv7, it is\n",
      "used in the final layer. Given that the output of the sigmoid function is between 0 and 1,\n",
      "it can be interpreted as a probability . The equations for these three activation functions are\n",
      "given in Equations 2.1 -2.3 and are plotted in Figure 2.1 .\n",
      "fReLU (x) = max(0, x) (2.1)\n",
      "fsigmoid (x) =σ(x) =1\n",
      "1 +e−x(2.2)\n",
      "fSiLU(x) =xσ(x) (2.3)\n",
      "Convolutional Layer\n",
      "The operation of a convolutional layer can be expressed as:\n",
      "Yn=f CX\n",
      "c=1Wn,c∗Xc+bn!\n",
      "(2.4)\n",
      "5\n",
      "Figure 2.1 :Activation functions.\n",
      "(a) Example 1\n",
      " (b) Example 2\n",
      "Figure 2.2 :Output of an object detection network for two images from the MS COCO dataset.\n",
      "Detected objects are classified and located with a bounding box.\n",
      "With output Y∈RN×Hout×Wout , weights W∈RN×C×Kh×Kw, input X∈RC×Hin×Win\n",
      "and bias b∈RN. Where N is the number of output channels, C the number of input channels,\n",
      "W andH the width and height of the channels, KH andKW the height and width of the\n",
      "convolution kernels, and fthe activation function.\n",
      "The input and output channels can also be referred to as input and output feature maps.\n",
      "Though in some cases the output feature maps only refer to the direct result of the convolution\n",
      "before the activation function is applied. In this thesis, the output feature maps will refer to\n",
      "the post-activation feature maps, unless explicitly mentioned otherwise.\n",
      "The weights W of a convolutional layer can be split into N 3D filters. Each of these filters\n",
      "convolved with the input X corresponds to one of the output feature maps Yn.\n",
      "2.1.2 Object Detection\n",
      "Object detection is the task of locating and classifying objects in an image. F or each object,\n",
      "the network should output a bounding box that fits closely around the object and produce a\n",
      "classification for that object. Figure 2.2 shows the output of an object detection network for\n",
      "two different images.\n",
      "This subsection provides background on the metrics, datasets and models used for object\n",
      "detection. The metrics and datasets are used for YOLOv7, while a short overview of the\n",
      "existing models provides context.\n",
      "6\n",
      "(a) Intersection\n",
      " (b) Union\n",
      " (c) IoU\n",
      "Figure 2.3 :Intersection over Union (IoU) of a predicted bounding box, bbox pred , and the\n",
      "ground truth, bbox gt.\n",
      "Metrics\n",
      "T o determine how well an object detection network is performing, first the proposed bounding\n",
      "boxes must be evaluated. This is done using the Intersection over Union (IoU) metric:\n",
      "IoU =bbox gt∩bbox pred\n",
      "bbox gt∪bbox pred(2.5)\n",
      "This measures how close the predicted bounding box, bbox pred , matches with the ground\n",
      "truth, bbox gt. Figure 2.3 visualizes the IoU equation. The IoU is zero when the bounding\n",
      "boxes have no overlap, and one when the bounding boxes are identical.\n",
      "Precision and recall are defined in terms of T rue Positives (TP), F alse Positives (FP) and F alse\n",
      "Negatives (FN):\n",
      "Precision =TP\n",
      "TP+FP(2.6)\n",
      "Recall =TP\n",
      "TP+FN(2.7)\n",
      "It is unrealistic to expect all predicted bounding boxes to match perfectly with the ground\n",
      "truth. However, they must match to some extent. This extent is measured with the IoU\n",
      "metric. T o compute the number of true and false positives for the precision and recall, only\n",
      "the predicted bounding boxes are used that have an IoU above a certain threshold. This\n",
      "threshold is typically set to a value of 0.5 or higher.\n",
      "The precision is largest when there are no false positives, while the recall is largest when\n",
      "there are no false negatives. T o avoid false positives fewer bounding boxes could be proposed,\n",
      "but this creates more false negatives. Thus, there is a tradeoff between precision and recall.\n",
      "This tradeoff can be captured in a precision-recall curve p(r). This curve is created\n",
      "by ordering all predictions from highest to lowest confidence and computing intermediate\n",
      "precision and recall from top to bottom. At high confidence, the precision will be high since\n",
      "it is unlikely to be very certain about an incorrect prediction. At the same time, the number\n",
      "of false negatives will be higher if lower confidence predictions are ignored. An example of a\n",
      "precision-recall curve is shown in Figure 2.4 .\n",
      "Finally , based on an interpolated version of the precision-recall curve (Figure 2.4 ), the most\n",
      "common metric for evaluating object detection networks, namely A verage Precision (AP), can\n",
      "be computed. The interpolated precision-recall curve pinterp (r)is given as:\n",
      "7\n",
      "Figure 2.4 :Precision-recall curve p(r)and interpolation pinterp (r). The colored area under\n",
      "pinterp (r)is the A verage Precision (AP), which is the metric used in this thesis. Adapted\n",
      "from [ 20].\n",
      "pinterp (r) = max\n",
      "˜r≥rp(˜r) (2.8)\n",
      "The AP is simply defined as the area under the interpolated precision-recall curve. The\n",
      "area is computed by sampling pinterp (r)at 101 points:\n",
      "AP=1\n",
      "101100X\n",
      "i=0pinterp (0.01i) (2.9)\n",
      "Since the definition of AP includes precision and recall, it requires an IoU threshold. F or\n",
      "example, AP 50 refers to the AP at an IoU threshold of 0.5. The AP can also be an average at\n",
      "different IoU thresholds. AP 50:95 refers to the average of AP 50, AP 55, ..., AP 95. Each object\n",
      "class has its own AP , for example APcaror APperson, but typically only the average over\n",
      "all classes is reported. This is called the mean A verage Precision, mAP . F ollowing common\n",
      "notation, in this thesis, unless specifically stated otherwise, the mean A verage Precision at\n",
      "IoUs from 0.5 to 0.95, mAP 50:95 , will be referred to as simply AP .\n",
      "Datasets\n",
      "The two most common datasets for training and benchmarking object detection networks are\n",
      "P ASCAL VOC [ 21] and MS COCO [ 22]. P ASCAL VOC is an older dataset with oﬀicial\n",
      "competitions from 2005 to 2012. The 2012 version of the dataset has 20 classes and consists\n",
      "of 6k training, 6k validation and 11k test images. The best reported result on P ASCAL VOC\n",
      "achieves an AP of 97.2% [ 23], which makes it diﬀicult to show any further improvement. Most\n",
      "newer methods do no longer report results for P ASCAL VOC.\n",
      "Instead, MS COCO has become the default benchmarking dataset for object detection,\n",
      "with competitions from 2015 to 2020. The dataset was last updated in 2017 and now con-\n",
      "tains 118k training, 5k validation and 41k test images. Objects have to be classified into 80\n",
      "categories, which include the same 20 categories as P ASCAL VOC. Compared to P ASCAL\n",
      "VOC, MS COCO contains objects at all different scales, including a large number of small ob-\n",
      "jects [ 24]. This makes MS COCO a more challenging dataset than P ASCAL VOC. Currently\n",
      "the best reported result on MS COCO is an AP of 66.0% [ 25].\n",
      "8\n",
      "Models\n",
      "There are two types of deep learning approaches for object detection: two-stage and one-stage\n",
      "object detectors [ 26]. T wo-stage networks split the detection of objects into two parts: (i)\n",
      "find regions in an image where objects may be present, (ii) classify these region proposals.\n",
      "One-stage object detectors do not use region proposals, but instead predict all bounding boxes\n",
      "and classifications in one go.\n",
      "A common example of a two-stage detector is F aster R-CNN [ 27] which builds on F ast\n",
      "R-CNN [ 28]. With F aster R-CNN the region proposal network and the classifier network\n",
      "are not separated. Firstly , a set of feature maps is generated from an input image using\n",
      "convolutional layers. Then, based on these feature maps the region proposals are generated.\n",
      "Finally , the classifier takes the feature maps and looks at the regions that were proposed.\n",
      "T wo-stage detectors are usually slower since they must first find region proposals and then run\n",
      "a classifier for all these regions. Usually , the increase in inference time will be compensated by\n",
      "an accuracy increase [ 29]. However, for real-time applications this might not be a beneficial\n",
      "tradeoff.\n",
      "F or this reason, one-stage detectors are more likely to be used in real-time applications.\n",
      "Commonly referenced one-stage detectors are SSD [ 30] and different YOLO versions. Both\n",
      "SSD and F aster R-CNN were introduced in 2015, and are no longer state-of-the-art, achieving\n",
      "28.8% and 34.9% AP on MS COCO, respectively . The original YOLO [ 31] network was also\n",
      "introduced in 2015, but newer versions, like YOLOv7 [ 16], are still in use. YOLOv7 was\n",
      "specifically designed for real-time applications and runs 10 times faster than most other non-\n",
      "YOLO methods. It achieves 51.4% AP on MS COCO. As mentioned above, currently the best\n",
      "reported result on MS COCO is 66.0% AP . Thus, YOLOv7 should not be used if very high\n",
      "accuracy is needed, and slower inference time is acceptable.\n",
      "2.2 YOLOv7\n",
      "This thesis deals with the compression of YOLOv7. An explanation of this object detection\n",
      "neural network is given in this section. Subsection 2.2.1 deals with the structure of YOLOv7\n",
      "and its different versions. Next, the computation of the output and its interpretation is dis-\n",
      "cussed in Subsection 2.2.2 . And lastly , the training of YOLOv7 is described in Subsection 2.2.3 .\n",
      "2.2.1 Structure\n",
      "Versions\n",
      "Versions\n",
      "There are three different YOLOv7 versions for three different types of GPU: YOLOv7-tiny\n",
      "for edge GPUs, YOLOv7 for normal desktop GPUs and YOLOv7-W6 for cloud GPUs. They\n",
      "all share the same type of architecture but are specifically designed to run in real-time on\n",
      "the respective hardware. T able 2.1 shows the difference in number of parameters and FLOPs,\n",
      "the speed at which each version runs on a NVIDIA V100 GPU, and the achieved AP on MS\n",
      "COCO. The tradeoff between speed and accuracy is clear, with YOLOv7-tiny optimizing for\n",
      "speed at the cost of accuracy . YOLOv7-tiny and YOLOv7 are evaluated with input images\n",
      "resized to 640x640, while YOLOv7-W6 is designed for input images of 1280x1280. F or the rest\n",
      "of this thesis all reported results are obtained using 640x640 images.\n",
      "9\n",
      "T able 2.1 :Comparison of\n",
      "the different YOLOv7\n",
      "versions [ 16]. Reported\n",
      "FPS is achieved on a\n",
      "NVIDIA V100 GPU.V ersion #Params #FLOPs FPS AP\n",
      "YOLOv7-tiny 6.2M 13.8G 286 38.7\n",
      "YOLOv7 36.9M 104.7G 161 51.4\n",
      "YOLOv7-W6 70.4M 360.0G 84 54.9\n",
      "Information Flow\n",
      "A simplified overview of the YOLOv7 architecture is shown in Figure 2.5 . The numbers on the\n",
      "left indicate the scale at that level. F or example, at the top (32x) the input image has been\n",
      "downsampled from a high resolution of 640x640 to a low resolution of 20x20. The different\n",
      "scales make it easier to focus on different sized objects. The arrows show how the information\n",
      "flows through the network to the three output layers, it has a bottom-to-top pathway as well\n",
      "as a top-to-bottom and another bottom-to-top pathway . These are called feature pyramids\n",
      "and were introduced in [ 32]. The upper layers contain more semantic information, but due to\n",
      "the lower resolution lack the precise localization. By allowing information to flow between the\n",
      "different scales, the semantic information can be combined with the more precise localization\n",
      "from the higher resolution, resulting in better object detection.\n",
      "Figure 2.5 :Simplified overview of the\n",
      "YOLOv7 architecture showing the\n",
      "flow of information. Numbers on\n",
      "the left indicate the scale: an input\n",
      "image of 640x640 will be reduced to\n",
      "20x20 at the top (640/32 = 20).\n",
      "ELAN\n",
      "ELAN\n",
      "Most of the convolutional layers are built up from ELAN (Eﬀicient Layer Aggregation Net-\n",
      "works) blocks [ 33]. Stacking more and more layers in a neural network leads to reduced\n",
      "accuracy increase, and after a point, it will actually reduce the overall accuracy . ELAN is\n",
      "designed to solve this problem by reducing the length of the shortest gradient path. Fig-\n",
      "ure 2.6 shows different configurations of an ELAN block. ELAN 2-4 and ELAN 1-4 are used\n",
      "in YOLOv7, while the smaller ELAN 1-2 is used in YOLOv7-tiny .\n",
      "Detection Head\n",
      "YOLOv7(-tiny) contains a detection head at three different scales (8x, 16x, 32x). These heads\n",
      "consist of two convolution layers, where the last layer has 255 output channels. Thus, in\n",
      "the case of a 640x640 input image, the output sizes are: 80x80x255 (8x scale), 40x40x255\n",
      "(16x scale) and 20x20x255 (32x scale). The next subsection will explain how the output is\n",
      "interpreted to get to bounding boxes and classifications of objects in the image.\n",
      "10\n",
      "(a) ELAN 2-4\n",
      " (b) ELAN 1-4\n",
      " (c) ELAN 1-2\n",
      "Figure 2.6 :Different ELAN block configurations. Numbers under ’Conv’ indicate kernel size\n",
      "and number of output channels.\n",
      "2.2.2 Output\n",
      "As mentioned in the previous subsection, the output of YOLOv7(-tiny) consists of three tensors\n",
      "with sizes: 80x80x255, 40x40x255 and 20x20x255. F or now, only 20x20x255 will be considered.\n",
      "The first two dimensions can be interpreted as a grid on the original input image. F or each\n",
      "cell in this 20x20 grid three bounding boxes are predicted. Each of these bounding boxes is\n",
      "characterized by 85 values (three bounding boxes: 3x85 = 255). The first four of these values\n",
      "determine the location of the bounding box, the fifth is the object confidence score, and the\n",
      "last 80 values are the class probabilities. Thus, if a different dataset is used, with a different\n",
      "number of classes, the number of output channels must change to match.\n",
      "At each scale three anchors are defined, that are boxes of a specific width and height.\n",
      "Instead of predicting the width, height and position of the bounding boxes, YOLOv7 predicts\n",
      "how the anchors should be scaled and translated to achieve the optimal bounding box. Fig-\n",
      "ure 2.8 shows an example of three anchors centered inside a cell. These anchors are identical\n",
      "for all cells at the same scale. The predicted values ( tx, ty, tw, th) can then be transformed into\n",
      "the actual bounding box with the following equations:\n",
      "bx= 2·σ(tx)−0.5 +cx (2.10)\n",
      "by= 2·σ(ty)−0.5 +cy (2.11)\n",
      "bw=\u0000\n",
      "2·σ(tw)\u00012·pw (2.12)\n",
      "bh=\u0000\n",
      "2·σ(th)\u00012·ph (2.13)\n",
      "Here bxandbygive the center position of the bounding box. cxandcyare the offsets of\n",
      "this cell from the top left of the grid. F or the third cell of the left at the top row, cx= 2 and\n",
      "cy= 0 . The width and height of the bounding box, bwandbh, are based on the width and\n",
      "height of the anchor, pwandph. Finally , all these values must be multiplied by the scale, also\n",
      "called stride, to go from the grid to input image position. The scale is simply the number of\n",
      "pixels of the input image that are represented in one cell.\n",
      "The 80 class probabilities for each bounding box can be interpreted in exactly the same\n",
      "way as with a standard classification network. The bounding box will be assigned to the class\n",
      "with the highest probability .\n",
      "11\n",
      "Now with three predicted bounding boxes at each cell, the total number of predictions\n",
      "is 25200 ( (20×20 + 40 ×40 + 80 ×80)×3). Clearly , most of these bounding boxes will not\n",
      "actually contain an object. That is why for each bounding box an object confidence score is\n",
      "predicted. As the name implies, the confidence score indicates how confident the network is\n",
      "that this bounding box contains an object. Multiplying the object confidence score with the\n",
      "probability of the assigned class, gives the overall confidence score or the estimated probability\n",
      "that an object with this specific class is present in the bounding box.\n",
      "Non-maximum suppression (NMS) is used to remove all low-confidence predictions and\n",
      "predictions with high overlap. First, all predictions with an overall confidence score below\n",
      "a certain threshold (typically 0.25) are removed. Next, for all bounding boxes that have an\n",
      "IoU with another box above a threshold (0.65 for YOLOv7), the lower confidence prediction\n",
      "is removed. This last step is repeated till there are no bounding boxes with an overlap above\n",
      "the threshold. Thus, NMS takes in 25200 predictions and will typically only return a handful\n",
      "(e.g., two in the case of Figure 2.2a ).\n",
      "Figure 2.7 :Cosine annealing.\n",
      "Figure 2.8 :Anchor boxes\n",
      "(dashed) centered in cell\n",
      "(solid black).\n",
      "2.2.3 T raining\n",
      "Data Augmentation\n",
      "Data Augmentation\n",
      "YOLOv7 is trained and evaluated on the MS COCO dataset. Different types of data augmenta-\n",
      "tion are used to avoid overfitting. Firstly , Mosaic augmentation (introduced by YOLOv4 [ 34])\n",
      "places 4 or 9 images side by side into one combined input image. Then MixUp [ 35] and\n",
      "Cutout [ 36] are applied with a probability of 15%. MixUp combines two images (with Mosaic\n",
      "already applied) into one using a weighted average of both images. Cutout randomly masks\n",
      "out square sections of the image. Next, 50% of the images is flipped left to right. Finally ,\n",
      "some noise is applied to the hsv color values.\n",
      "Loss\n",
      "The loss of YOLOv7 consists of three parts: box, objectness and classification loss. The box\n",
      "loss is the CIoU loss [ 37] between the target bounding box and the prediction. The CIoU loss\n",
      "is similar to IoU but also penalizes the distance between the center points of both boxes and\n",
      "the deviation of the aspect ratios. The objectness score should be zero if there is no object\n",
      "in the bounding box, one if it exactly fits the target, and have the same value as the IoU in\n",
      "case of partial overlap. The objectness loss is then calculated as the cross-entropy between\n",
      "this target value and the actual predicted score. The classification loss is also implemented as\n",
      "a cross-entropy loss between the predicted distribution and the actual class.\n",
      "12\n",
      "Evaluation\n",
      "The main evaluation metric for MS COCO is the AP (abbreviated from mAP 50:95 ). A detailed\n",
      "explanation of how the AP is calculated is given in Subsection 2.1.2 . F or YOLOv7 the main\n",
      "evaluation metric, which is used to determine if the network accuracy is increasing, is called\n",
      "fitness:\n",
      "fitness = 0.1·AP50+ 0.9·AP (2.14)\n",
      "This metric is also used in the rest of this thesis to evaluate the compressed models.\n",
      "Learning Rate Schedule\n",
      "The learning rate is slowly decreased during training using cosine annealing. Figure 2.7 shows\n",
      "the learning rate factor going from 1.0 to 0.01 in 10 epochs. The actual learning rate for each\n",
      "epoch is the start learning rate multiplied by the learning rate factor.\n",
      "2.3 Compressing Neural Networks\n",
      "Compression techniques can be categorized into four different types. Section 2.3.1 will intro-\n",
      "duce these categories, followed by an overview of each category . The evaluation metrics to\n",
      "compare different compression methods are explained in Section 2.3.2 . Section 2.3.3 will give a\n",
      "brief overview of the different datasets and networks commonly used for benchmarking. These\n",
      "datasets and networks are needed for comparing and selecting compression methods.\n",
      "2.3.1 Compression T echniques\n",
      "Different papers use different ways to categorize compression techniques, or use different sub-\n",
      "categories [ 5,9,10]. In this thesis they are categorized into: (i) pruning, (ii) quantization,\n",
      "(iii) tensor decomposition and (iv) compact architectures. T ensor decomposition includes the\n",
      "decomposition of both fully-connected and convolutional layers.\n",
      "Pruning\n",
      "Network pruning is based on the Minimal Description Length (MDL) principle, which states\n",
      "that the best model for describing a dataset is the one that leads to the highest compression [ 5].\n",
      "T ogether with the observation that large networks only need a fraction of their parameters for\n",
      "accurate prediction [ 11], pruning methods try to find unimportant parameters and remove them\n",
      "from the network or set them to zero. Besides compression, pruning also has a regularization\n",
      "effect on the network [ 38].\n",
      "A common criterion for pruning a parameter is magnitude-based, pruning parameters\n",
      "whose weights are below a certain threshold. This is generally used in combination with L 2\n",
      "or L 1regularization, which will force parameters that have a small impact on the network’s\n",
      "performance close to zero [ 5]. The L 2norm mainly pushes the value of larger weights down,\n",
      "while the L 1norm can achieve some sparsity . The L 0norm would induce even more sparsity ,\n",
      "but since it is not differentiable, cannot be used for neural network training with gradient\n",
      "descent [ 39].\n",
      "Pruning can be categorized into structured and unstructured methods. Unstructured methods\n",
      "will prune individual weights, while structured methods will prune complete neurons or entire\n",
      "13\n",
      "(a) shape-wise\n",
      " (b) row-wise\n",
      " (c) column-wise\n",
      " (d) channel-wise\n",
      "(e) stack shape-wise\n",
      " (f) stack row-wise\n",
      " (g) stack column-wise\n",
      " (h) filter-wise\n",
      "Figure 2.9 :Types of structured sparsity in 3D filters. Blue squares indicate weights to be\n",
      "pruned. Sparsity types (a)-(d) can also be used for fully-connected layers. Note: each\n",
      "convolutional layer includes many filters, but only one is shown for simplicity .\n",
      "filters. Unstructured pruning requires specific software to take full advantage of the sparsity ,\n",
      "while structured pruning completely removes certain parts of the network, resulting in a more\n",
      "compact architecture, with fewer FLOPs, which can be run directly without the need for\n",
      "specialized software [ 40]. Figure 2.9 shows the different kind of structures that can be used\n",
      "for sparsity in convolutional layers. Figure 2.9a shows no structure at all and 2.9h is fully\n",
      "structured. Figure 2.9b -2.9g could be classified as semi-structured, which does induce some\n",
      "structure but still requires additional optimization. Sparsity types 2.9a -2.9d can also be applied\n",
      "to the 2D weights of fully-connected layers. In that case, 2.9b -2.9d would be considered fully\n",
      "structured, which directly reduces the model size.\n",
      "Pruning methods using structured sparsity can lead to a more compact architecture, but\n",
      "not necessarily to a smaller file size, since they usually do not prune the individual weights in\n",
      "the remaining layers. Therefore, methods using structured sparsity may achieve less compres-\n",
      "sion but end up with a model that uses fewer FLOPs and thus runs faster.\n",
      "Quantization\n",
      "Quantization reduces the number of bits used to represent the value of each parameter. [ 41]\n",
      "shows that neural networks are resistant to certain amounts of low precision. There are a lot\n",
      "of different quantization methods. The standard way is to reduce the precision used for each\n",
      "parameter. F or example, when an 8-bit format is used, instead of the standard 32-bit format,\n",
      "the model size is reduced by a factor of four. In the extreme case the network can be binarized,\n",
      "using only one bit for each parameter [ 42].\n",
      "The previous approach uses linear quantization, but the weights of neural networks are\n",
      "not uniformly distributed [ 43]. One way to use nonlinear quantization is k-means clustering,\n",
      "which can be done during [ 11] or after [ 44] training. When one of the clusters is located at\n",
      "zero, this effectively prunes all parameters assigned to this cluster.\n",
      "14\n",
      "An important insight of [ 45] is the fact that pruning and quantization can compress neural\n",
      "networks without interfering with each other. Combining the two techniques can therefore lead\n",
      "to higher compression with little to no extra accuracy loss.\n",
      "T ensor Decomposition\n",
      "T ensor decomposition is a technique to approximate the full weight matrix of a fully-connected\n",
      "layer or the filter kernels of a convolutional layer with a low-rank approximation. T ensor de-\n",
      "composition methods include T runcated Singular V alue Decomposition [ 46], T ucker Decom-\n",
      "position [ 47], Canonical Polyadic Decomposition [ 48] and T ensor T rain Decomposition [ 49].\n",
      "There are several disadvantages to this approach [ 40]. Firstly , it is not always obvious which\n",
      "rank should be used. Secondly , the decomposition operation is computationally expensive.\n",
      "And finally , the factorized neural network converges slower, meaning that extensive retraining\n",
      "is required. However, most compression methods require fine-tuning and a lot of extra com-\n",
      "putation. Which is usually not a problem since this can be done using powerful computers.\n",
      "Compact Architectures\n",
      "The above-mentioned techniques change the original network to reduce its size. Another\n",
      "common technique is the use of compact architectures. These networks are not changed during\n",
      "or after training but are specifically designed to be compact. The most well-known networks\n",
      "are MobileNet [ 14] and SqueezeNet [ 50]. Another option is using knowledge distillation. With\n",
      "this approach a large network, or an ensemble of large networks, which acts as teacher is fully\n",
      "trained without any compression. Then, a much smaller student network is trained using both\n",
      "the actual dataset the teacher was trained on, as well as the dark knowledge [ 51] (softmax\n",
      "output) of the teacher. The extra knowledge from the teacher allows the student to minimize\n",
      "the accuracy loss [ 52].\n",
      "Typically , the softmax output of both the teacher and student are softened using temper-\n",
      "ature T before computing the loss [ 51]. The output is then calculated as:\n",
      "qi=exp(zi/T)P\n",
      "jexp(zj/T)(2.15)\n",
      "Where inputs ziare converted to qi. Note that if T= 1 , this results in the standard\n",
      "softmax function.\n",
      "2.3.2 Evaluation Metrics\n",
      "There are a lot of possible evaluation metrics, but most metrics are related to accuracy , number\n",
      "of parameters and number of floating-point operations (FLOPs) [ 5,9,10]. Since the comparison\n",
      "of compression methods is done using similar networks and datasets, it makes sense to use the\n",
      "relative metrics accuracy loss and compression ratio.\n",
      "The accuracy loss can be used as a constraint when searching for the right compression\n",
      "method. In certain applications an accuracy loss of up to 10% might be acceptable, while in\n",
      "other applications a loss of just 1% may cause significant problems [ 53].\n",
      "It is important to note that the number of parameters is not linearly correlated with the\n",
      "number of FLOPs [ 40]. Convolutional filters do not need many parameters, but depending on\n",
      "the image size, will require a lot of FLOPs. And the opposite is true for fully-connected layers.\n",
      "These layers take up a lot of the parameters but contribute little to the number of FLOPs.\n",
      "15\n",
      "Depending on the way the parameters are saved, the model size saved on disk might not\n",
      "be linearly correlated with the number of parameters. This is obvious when the parameters\n",
      "are saved with different precision but could also happen when a different data structure is used\n",
      "to manage sparse tensors. In the latter case, the differences are usually negligible, but when\n",
      "available both the number of parameters and the model size should be used for comparison\n",
      "between different methods.\n",
      "In the literature it is common to calculate the compression ratio for the number of FLOPs\n",
      "(or parameters) as the original number of FLOPs divided by the number of FLOPs in the\n",
      "compressed network. The higher the compression ratio, the fewer parameters an increase in\n",
      "compression ratio will prune. F or example, the difference between a compression ratio of 10\n",
      "and 20 is 5% of the parameters, but the difference between a compression ratio of 100 and 110\n",
      "is below 0.1%. Given this nonlinearity , a better way to represent compression ratios seems to\n",
      "be the percentage of parameters or FLOPs remaining after compression. In this thesis this will\n",
      "be referred to as the FLOPs Pruning Ratio (FPR) and Parameter Pruning Ratio (PPR). The\n",
      "way it is defined, a FPR of 100% means no compression, so a lower FPR and PPR is better.\n",
      "2.3.3 Benchmark Datasets and Networks\n",
      "T o make fair comparisons between different compression methods, as many external factors as\n",
      "possible should remain constant. The most important factors are the dataset and the network\n",
      "that are used to evaluate the compression method. F or example, using a large network will\n",
      "generally allow for more compression compared to a small network for the same task. Similarly ,\n",
      "a simple binary classification task will need a smaller network than a complicated classification\n",
      "task with a thousand classes, and thus allow for more compression.\n",
      "F ortunately , there are several common datasets and networks that are often used for\n",
      "benchmarking. In this section we discuss the ImageNet [ 54] and CIF AR-10 [ 55] datasets. Be-\n",
      "sides these datasets, we will briefly introduce the following networks: ResNet-50 and ResNet-56\n",
      "[15]. Typically , ResNet-50 is used for ImageNet and ResNet-56 for CIF AR-10.\n",
      "ImageNet + ResNet-50\n",
      "ImageNet is a large dataset of over 14 million images with almost 22 thousand classes. A\n",
      "popular subset comes from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
      "It is sometimes called ImageNet-1k or ILSVRC2017, but given its popularity is usually just\n",
      "referred to as ImageNet.\n",
      "This subset includes a thousand classes and is already split up in training, validation and\n",
      "test sets. It contains roughly 1.3 million training, 50 thousand validation and 100 thousand\n",
      "test images. The size of the images varies, but it is common to resize all images to 224 by\n",
      "224 pixels. Currently , the best accuracy achieved on ImageNet-1k (from now on referred to as\n",
      "ImageNet) is 91.1% [ 56].\n",
      "Figure 2.10 :ResNet building block [ 15].The ResNet architecture was introduced\n",
      "in 2015 as a solution to the degradation prob-\n",
      "lem. Namely , the problem that increasing\n",
      "the depth of a neural network only increases\n",
      "the accuracy up to a point, after which it de-\n",
      "grades rapidly [ 15]. ResNets consist of resid-\n",
      "ual building blocks. The input of each block\n",
      "is directly added to the output of one or more\n",
      "convolutional layers (see Figure 2.10 ), this\n",
      "way each block can learn a residual function\n",
      "16\n",
      "F(x). It is shown that this solves the degra-\n",
      "dation problem, since by pushing the weights\n",
      "of certain layers to zero, it easily mimics a smaller network if necessary . While at the same\n",
      "time more building blocks allow for more residual refinement of the network output, resulting\n",
      "in better accuracy .\n",
      "The number in the name indicates the number of layers, both fully-connected and convo-\n",
      "lutional layers, in the neural network. ResNet-50 consists of a convolutional layer, 16 building\n",
      "blocks with 3 convolutional layers each, and a fully-connected layer outputting 1000 probabil-\n",
      "ities for the ImageNet classes. ResNet-50 achieves an accuracy of 75.3% on ImageNet.\n",
      "CIF AR-10 + ResNet-56\n",
      "CIF AR-10 is a dataset of 50 thousand training and 10 thousand test images. As the name\n",
      "implies, the dataset contains 10 classes. F or each class there are 5 thousand training images\n",
      "and a thousand test images. All images have the same size of 32 by 32 pixels. Note that the\n",
      "number of classes and image size are much smaller compared to ImageNet. A similar neural\n",
      "network will require significantly less FLOPs for CIF AR-10 than ImageNet. Currently , the\n",
      "best accuracy achieved on CIF AR-10 is 99.9% [ 57].\n",
      "ResNet-56 consists of a convolutional layer, 27 building blocks with 2 convolutional layers\n",
      "each, and a fully-connected layer outputting 10 probabilities for the CIF AR-10 classes. ResNet-\n",
      "56 achieves an accuracy of 88.8% on CIF AR-10.\n",
      "F or both ImageNet and CIF AR-10, ResNets do not achieve state-of-the-art accuracy . This is\n",
      "to be expected, given that ResNets were introduced in 2015. By using a benchmark model\n",
      "that older compression methods already reported results for, it is easier to compare methods.\n",
      "Using the latest models, requires rerunning older compression methods for comparison. This\n",
      "only works up to a certain point. F or example, MNIST and LeNet-5 [ 58] have been used for\n",
      "a long time as benchmark, and are still sometimes used, but LeNet-5 has been compressed to\n",
      "less 0.5% of its weights without any loss of accuracy [ 12], making any further improvements\n",
      "diﬀicult to quantify .\n",
      "17\n",
      "3 Methods\n",
      "In this chapter two compression methods are selected for testing on YOLOv7-tiny . Section 3.1\n",
      "will answer the question: What are the best networks for comparing compression methods?\n",
      "This is followed by a compilation of best performing compression methods: What is the state-\n",
      "of-the-art in neural network compression? F rom this list, two methods are selected, which will\n",
      "narrow down which compression method is best suited for compressing YOLOv7? The selected\n",
      "methods are explained in Sections 3.2 -3.3 .\n",
      "3.1 Compression Search\n",
      "F or compressing YOLOv7, a list of the state-of-the-art compression methods must be compiled.\n",
      "F ortunately , there are several surveys categorizing and rating compression methods. The\n",
      "largest comparison of compression methods is given in [ 59]. This comparison is used as a\n",
      "starting point for finding the best methods. The authors categorize 150 methods and count\n",
      "how often each network and dataset is used to report the compression results. The most used\n",
      "networks and datasets are: ResNet-56 on CIF AR-10, AlexNet and ResNet-50 on ImageNet,\n",
      "and LeNet-5 on MNIST. As mentioned in Chapter 2, LeNet-5 is no longer useful as benchmark.\n",
      "Both LeNet-5 and AlexNet are relatively old and small networks that are less representative\n",
      "of bigger networks like YOLOv7.\n",
      "So, what are the best networks for comparing compression methods? Based on how often\n",
      "these networks are used as benchmark, ResNet-50 (on ImageNet) and ResNet-56 (on CIF AR-\n",
      "10) are the best networks to compare and rank compression methods. Note that these networks\n",
      "and datasets are introduced in Section 2.3.3 .\n",
      "The latest methods reported in [ 59] are from 2020. It is likely that since then new methods\n",
      "have been proposed with better performance. Therefore, a search has been performed for\n",
      "papers from after 2020. By only including the papers that report on ResNet-50 on ImageNet\n",
      "and ResNet-56 on CIF AR-10, the search is narrowed down considerably .\n",
      "T able 3.1 shows the result of this search. The FLOPs pruning ratio (FPR) and parameter\n",
      "pruning ratio (PPR) are the remaining percentage of FLOPs and parameters, respectively . T o\n",
      "limit this table to the best performing compression methods, only those with a FPR below\n",
      "30% for ResNet-56 or a FPR below 40% for ResNet-50 are included. If a method achieves a\n",
      "FPR below this threshold on one, but not the other, network, the results are still given for\n",
      "both networks. F or most methods PPR and FPR are close together. Since the goal is to speed\n",
      "up a neural network, the number of FLOPs is more important than the number of parameters.\n",
      "F rom this table it can be concluded that FPFS outperforms all other methods on ResNet-\n",
      "56. PCA-Pruner, CONPLSF, HRel-1, NNCS and GBIP are quite similar in compression, with\n",
      "NNCS outperforming them on accuracy loss. PKSMIO and KSE are clearly at the bottom\n",
      "based on their compression ratio but do limit the accuracy loss or even increase accuracy . F or\n",
      "ResNet-50 on ImageNet, NNCS has slightly better compression and KSE has slightly better\n",
      "accuracy . PKSMIO, GBIP and FPFS also have similar compression performance, with GBIP\n",
      "having a slight edge on accuracy . HRel-1 is significantly outperformed by the other methods\n",
      "on ResNet-50.\n",
      "on ResNet-50.\n",
      "Although the compression ratio and accuracy loss of a method are important, the ease\n",
      "19\n",
      "T able 3.1 :Reported results from the best performing compression methods for ResNet-56 on\n",
      "CIF AR-10 and ResNet-50 on ImageNet. Accuracy Loss is the percentage point reduction\n",
      "in accuracy of the pruned network. Negative accuracy loss indicates an increased accuracy\n",
      "after pruning. FPR and PPR refers to FLOPs Pruning Ratio and Parameter Pruning Ratio,\n",
      "respectively . These pruning ratios give the percentage of FLOPs or parameters remaining\n",
      "after pruning. A lower pruning ratio is better. Only compression methods with a FPR\n",
      "below 30% for ResNet-56 or a FPR below 40% for ResNet-50 are included in this table. F or\n",
      "completeness, the results of methods with a FPR below this threshold for only one network\n",
      "are still reported for both networks. This table helps answer the question: What is the\n",
      "state-of-the-art in neural network compression?\n",
      "Model Method Accuracy Loss FPR PPR\n",
      "ResNet-56\n",
      "(CIF AR-10)FPFS [ 13] 0.7 11.2 12.9\n",
      "PCA-Pruner [ 60] 1.05 11.2 18.8\n",
      "CONPLSF [ 61] 1.35 21.0 21.9\n",
      "HRel-1 [ 62] 1.1 23.1 22.2\n",
      "NNCS [ 63] 0.21 23.8 13.7\n",
      "GBIP [ 64] 0.38 26.6 29.6\n",
      "PKSMIO [ 65] -0.32 35.8 36.4\n",
      "KSE [ 66] 0.15 40.0 41.7\n",
      "ResNet-50\n",
      "(ImageNet)NNCS 1.24 21.1 24.4\n",
      "KSE 0.84 21.3 34.5\n",
      "PKSMIO 1.48 33.9 27.8\n",
      "GBIP 0.47 36.7 44.6\n",
      "FPFS 0.96 39.7 -\n",
      "HRel-1 0.68 51.3 51.8\n",
      "T able 3.2 :Number of training epochs that each method uses to go from a pretrained to\n",
      "a pruned, finetuned network. ’50 per layer’ means that the network layers are pruned\n",
      "sequentially with 50 epochs of finetuning after each layer.∗These numbers are not explicitly\n",
      "mentioned in papers but are estimated.\n",
      "Method T raining Epochs\n",
      "CIF AR-10 ImageNet\n",
      "FPFS 50 per layer 100 per layer\n",
      "PCA-Pruner 50 per layer -\n",
      "CONPLSF 260 -\n",
      "HRel-1 100 33\n",
      "NNCS 450∗-\n",
      "GBIP 30 20\n",
      "KSE 200 21\n",
      "PKSMIO 150∗-\n",
      "20\n",
      "of implementation is also of relevance. Specifically , the time it takes to go from a pretrained\n",
      "model to a pruned and finetuned model should be within certain bounds. T able 3.2 shows the\n",
      "number of training epochs each methods requires to achieve a pruned and finetuned model.\n",
      "Regarding size, the dataset used for training YOLOv7, MS COCO, is more similar to ImageNet\n",
      "than CIF AR-10. Unfortunately , not all papers report training epochs on ImageNet. Since it\n",
      "can take up to two hours on a simple GPU to train YOLOv7 for one epoch, most methods\n",
      "are not feasible. F rom this table, the three methods that seem feasible are HRel-1, GBIP and\n",
      "KSE.\n",
      "GBIP outperforms KSE for ResNet-56 on CIF AR-10, and HRel-1 for ResNet-50 on Im-\n",
      "ageNet, which makes it an easy choice. The second chosen method is KSE. It significantly\n",
      "outperforms HRel-1 on FPR with similar accuracy loss on ResNet-50. It is outperformed by\n",
      "HRel-1 on ResNet-56, but in this case the difference in accuracy loss is clearly in favor of KSE.\n",
      "Therefore, the answer to the question, Which compression method is best suited for com-\n",
      "pressing YOLOv7? is narrowed down to two methods: GBIP and KSE. The next subsec-\n",
      "tions will describe the selected methods. The specific implementations of these methods for\n",
      "YOLOv7-tiny will be discussed in Chapter 4.\n",
      "3.2 GBIP\n",
      "3.2 GBIP\n",
      "Global balanced iterative pruning for eﬀicient convolutional neural networks [64] introduces a\n",
      "compression method (GBIP) based on a simple pruning strategy but with a more advanced\n",
      "approach to recovering accuracy . It prunes entire filters (see Figure 2.9h ) from each convolu-\n",
      "tional layer. Removing filters reduces the number of output channels of this layer as well as\n",
      "the input channels of the next layer.\n",
      "As criterion for pruning, the L 1norm of the output feature maps is used. The use of output\n",
      "feature maps, means that GBIP requires a dataset to compress a network. By normalizing the\n",
      "L1norm for each layer, layers with large L 1norms do not dominate the compression result.\n",
      "Rather the network is compressed more evenly , resulting in a balanced pruning of the network.\n",
      "This balanced pruning means that the difference in pruning ratios of FLOPs and parameters\n",
      "is small. The importance score ml\n",
      "nis calculated for all Nloutput feature maps Yl\n",
      "nfor all layers\n",
      "l:\n",
      "ml\n",
      "n=∥Yl\n",
      "n∥1/max{∥Yl\n",
      "1∥1,∥Yl\n",
      "2∥1, ...,∥Yl\n",
      "Nl∥1} (3.1)\n",
      "It is not explicitly mentioned whether the output feature maps are pre- or post-activation.\n",
      "However, other compression methods typically use post-activation [ 67,68,69]. This makes\n",
      "sense, given that the post-activation feature maps contain the information that is passed on\n",
      "through the rest of the network. Therefore, the post-activation feature maps are used for\n",
      "computing the importance score.\n",
      "The pruning threshold is determined by the mean of the importance scores for each layer\n",
      "and a global pruning factor k:\n",
      "ml\n",
      "p=k1\n",
      "NNX\n",
      "n=1ml\n",
      "n, with k∈(0,1) (3.2)\n",
      "F or all feature maps where ml\n",
      "nis below ml\n",
      "pthe corresponding filters are removed. The\n",
      "next layer now has a reduced number of input channels and thus its weights can also be\n",
      "partially pruned.\n",
      "As the name indicates, an iterative pruning schedule is used, where the network is trained\n",
      "for ten epochs after each pruning step to partially restore its accuracy . This increases the\n",
      "21\n",
      "control over the pruning process. If the accuracy stays high, a next pruning step can be per-\n",
      "formed. But if the accuracy starts to deteriorate too quickly , the network might be compressed\n",
      "to its limit. The authors give no rationale for the number of pruning steps they use in their\n",
      "experiments, but depending on the dataset there are a total of 2 or 3 pruning steps.\n",
      "Using the L 1norm for pruning filter is common, but the authors of GBIP also focus on the\n",
      "finetuning stage. They add three ways to improve the accuracy recovery: knowledge transfer\n",
      "from the (i) output and (ii) intermediate features of the original network, as well as an (iii)\n",
      "adversarial network to discriminate between the two networks.\n",
      "3.2.1 Output T ransfer\n",
      "The output of the original model (teacher) is used to guide the training of the compressed model\n",
      "(student). As mentioned in Subsection 2.3.1 , the knowledge of the teacher can be transferred\n",
      "to the student by adding the KL divergence between the output of the teacher, fT(x), and\n",
      "the student, fS(x), to the student loss function. In this case, a temperature T is used to\n",
      "soften the outputs. T o keep the magnitude of the KL loss independent of the temperature,\n",
      "the loss is multiplied by T2. Finally , the output transfer loss ( LOT ) is a weighted sum of the\n",
      "cross-entropy between fS(x)and hard targets, and the KL loss ( LKL ):\n",
      "p(x) =Fsoftmax (fS(x)/T) (3.3)\n",
      "q(x) =Fsoftmax (fT(x)/T) (3.4)\n",
      "DKL(p∥q) =nX\n",
      "i=1[p(x) log(p(x))−p(x) log(q(x))] (3.5)\n",
      "LKL(WS) =T2DKL(p∥q) (3.6)\n",
      "LOT(WS) =αLKL(WS) + (1 −α)LCE(WS) (3.7)\n",
      "3.2.2 Attention T ransfer\n",
      "Not only the output, but also the intermediate features of the teacher can be used to guide\n",
      "the training of the compressed student. The implementation in GBIP is taken from [ 70]. The\n",
      "activations M of three different layers in the network are used to calculate spatial attention\n",
      "maps Al. These maps are a measure of how much attention the network is paying to each pixel\n",
      "of the feature map. This way the student can be taught to focus on the same pixel locations\n",
      "as the teacher. The spatial attention maps are normalized by their L 2norm. The attention\n",
      "transfer loss LAT is given by the L 2norm of the difference between the normalized attention\n",
      "maps:\n",
      "Al(Mab) =1\n",
      "NNX\n",
      "n=1(Mab\n",
      "n)2(3.8)\n",
      "LAT(WS) =3X\n",
      "Al1\n",
      "S\n",
      "∥Al\n",
      "S∥2−Al\n",
      "T\n",
      "∥Al\n",
      "T∥2\n",
      "2(3.9)\n",
      "3.2.3 Adversarial Game\n",
      "T o further converge the output of the student to that of the teacher, a discriminator network\n",
      "is created to differentiate between the two networks. This network has three fully-connected\n",
      "22\n",
      "layers with 128-256-128 neurons. The task of this network is to get better at distinguishing\n",
      "between teacher and student output, while the student network tries to fool the discriminator.\n",
      "The idea is taken from [ 71], where a simple fully-connected network is also used to distinguish\n",
      "between an original and a compressed network. The goal of the discriminator is to output a\n",
      "value close to one if the input came from the teacher and close to zero if it came from the\n",
      "student. It does this by minimizing the following loss function:\n",
      "LG(WG) =EfT(x)∼pT(x)[log(1 −G(fT(x, W T), WG))]\n",
      "+EfS(x)∼pS(x)[log(G(fS(x, W S), WG))](3.10)\n",
      "while the discriminator is being optimized to predict a zero for student input, the student\n",
      "is simultaneously updated to fool the discriminator and get it to output a value closer to one,\n",
      "with the adversarial game loss LAG :\n",
      "LAG(WS) =EfS(x)∼pS(x)[log(1 −G(fS(x, W S), WG))] (3.11)\n",
      "Combining these losses, gives the following total loss function for the compressed network:\n",
      "LS(WS) =LAG(WS) +LAT(WS) +LOT(WS) (3.12)\n",
      "3.2.4 Results\n",
      "The authors test GBIP on several networks and datasets. F rom this, it is clear that k= 0.5is\n",
      "the maximum for smaller networks like VGG-16 and VGG-19, as well as for larger networks\n",
      "on more complex datasets (CIF AR-100, ImageNet). F or a large network on a smaller dataset,\n",
      "like ResNet-110 on CIF AR-10, kcan go up to 0.7 without accuracy loss.\n",
      "They also assess the effect of the three finetuning techniques. All three show improvements\n",
      "with output transfer (OT) having the largest effect, followed by the adversarial game (AG)\n",
      "and attention transfer (A T). F or ResNet-18 on ImageNet, the authors report the best accuracy\n",
      "improvement of 1.24% using all three techniques. While the accuracy improvement for VGG-\n",
      "16 on CIF AR-10 and ResNet-56 on CIF AR-100 is only 0.20% and 0.24%, respectively . The\n",
      "difference between these accuracy improvements might be due to the initial accuracy loss of the\n",
      "baseline without any finetuning techniques. F or these last two networks, the baseline accuracy\n",
      "loss is very small (0.42%) for ResNet-56 and negative (-0.34%) for VGG-16, leaving less room\n",
      "for improvement than ResNet-18 with 1.90% baseline accuracy loss.\n",
      "3.3 KSE\n",
      "Exploiting Kernel Sparsity and Entropy for Interpretable CNN Compression [66] introduces\n",
      "a compression method (KSE) focusing on the channels of the input feature maps and its\n",
      "corresponding 2D kernels. Instead of pruning, this method uses clustering of the 2D kernels\n",
      "into a few clusters to compress the network. It introduces an indicator that uses both the\n",
      "sparsity and entropy of a kernel. Based on this indicator the number of clusters required for\n",
      "each input feature map is calculated.\n",
      "23\n",
      "3.3.1 Kernel Sparsity\n",
      "The convolution operation can be formulated as:\n",
      "Yn=CX\n",
      "c=1Wn,c∗Xc (3.13)\n",
      "withC channels in the input feature map and N channels in the output. F or each channel\n",
      "in the input, Xc, the matching 2D kernels are given by {Wn,c}N\n",
      "n=1 . Instead of using the sparsity\n",
      "ofXcas pruning indicator, the sparsity of its corresponding 2D kernels is used. In the paper,\n",
      "the authors verify the relationship between the sparsity of the input and that of the weights.\n",
      "This makes KSE a data-free compression method since it only uses the weights of the network\n",
      "without having to run a dataset through it.\n",
      "The sparsity for channel cof the input feature map is defined as:\n",
      "sc=NX\n",
      "n=1∥Wn,c∥1 (3.14)\n",
      "Clearly , scis low for sparse kernels, where the weights are small.\n",
      "3.3.2 Kernel Entropy\n",
      "If an input feature map contains a lot of information, its 2D kernels are less suitable for\n",
      "pruning. The amount of information increases if the 2D kernels are very diverse. The authors\n",
      "propose kernel entropy as a measure of this information.\n",
      "First, a density metric dm is calculated for all 2D kernels based on the distance between\n",
      "each kernel and its k-nearest neighbors:\n",
      "dm(Wi,c) =NX\n",
      "j=1ACi,j (3.15)\n",
      "ACi,j=(\n",
      "∥Wi,c−Wj,c∥2 ifWj,c is among k nearest neighbors of Wi,c\n",
      "0 otherwise(3.16)\n",
      "Larger values for dm(Wi,c)mean its closest neighbors are far away . Large differences\n",
      "between kernels also result in large differences in the convolution of these kernels with the\n",
      "input feature map. Thus, if many kernels have a high dm , the input feature map contains a\n",
      "lot of information.\n",
      "Based on this density metric, the proposed kernel entropy is defined as:\n",
      "ec=−NX\n",
      "i=1dm(Wi,c)\n",
      "dclog2dm(Wi,c)\n",
      "dc, with dc=NX\n",
      "i=1dm(Wi,c) (3.17)\n",
      "If the kernels are very diverse, the kernel entropy will be lower.\n",
      "3.3.3 Kernel Clustering\n",
      "The kernel sparsity and entropy (KSE) indicator combines the two metrics introduced above:\n",
      "24\n",
      "vc=rsc\n",
      "1 +αec(3.18)\n",
      "With balancing factor αset to 1 for all experiments. As mentioned above, a lower kernel\n",
      "sparsity refers to a higher sparseness, while a lower entropy refers to a more diverse distribution\n",
      "of kernels. Therefore, a lower vcshould indicate higher compression. Based on vcthe number\n",
      "of kernels for the c-th input feature map is calculated:\n",
      "qc=8\n",
      "><\n",
      ">:0 if⌊vcG⌋= 0\n",
      "N if⌈vcG⌉=G\u0006N\n",
      "2G−⌈vcG⌉+T\u0007\n",
      "otherwise(3.19)\n",
      "otherwise(3.19)\n",
      "Where G andT are the hyperparameters controlling granularity and compression ratio,\n",
      "respectively . If G= 2 ,qcis limited to 0 or N. Larger values for G allow for finer grained\n",
      "compression. Treduces the number of clusters by a factor 2Tfor the third case in the equation\n",
      "above.\n",
      "In the case of qc= 0 , the entire input feature map can be ignored. And if qc=N, no\n",
      "compression is applied. F or the values in between 0 and N, all the original N kernels are\n",
      "assigned to one of the qcclusters and its weights replaced by the centroid of this cluster. The\n",
      "centroids are denoted as {Bi,c}qc\n",
      "i=1 , while the index set {In,c∈ {1,2, ..., q c}}N\n",
      "n=1 links each\n",
      "kernel to one of the qckernels. In the finetuning stage, only the cluster centroids are updated.\n",
      "T o make full use of the induced sparsity , the 3D convolutions are split into 2D convolutions.\n",
      "Using the notation from above, qc2D activation maps are generated: Zi,c=Bi,c∗Xc. These\n",
      "are then combined into the n-th output feature map:\n",
      "Yn=CX\n",
      "c=1ZIn,c,c (3.20)\n",
      "3.3.4 Results\n",
      "F rom the paper, one would assume the authors use Equation 3.20 to speed up the network,\n",
      "but from the published code it is clear that this is not the case. Instead, a full weight matrix\n",
      "is reconstructed from the clusters. Only in the case of qc= 0 , when there are no clusters,\n",
      "the size of the weight matrix is reduced. This means that the actual reduction in FLOPs is\n",
      "minimal. The fact that an unoptimized implementation is used, is not mentioned in the paper\n",
      "itself. They do, however, refer to the reduction in FLOPs as ’theoretical’ exactly once.\n",
      "They test KSE with ResNet-56, DenseNet-40 and DenseNet-100 on CIF AR-10, as well as\n",
      "ResNet-50 on ImageNet. F or CIF AR-10, Tis set to 0, while for ImageNet it is set to 1. In all\n",
      "cases, the best compression is achieved with G= 5 or 6.\n",
      "F or ResNet-50 on ImageNet the reduction in FLOPs is about 1.5 times the reduction in\n",
      "parameters, while for CIF AR-10 these are almost identical. And the accuracy loss is slightly\n",
      "higher for ImageNet, though still below 1%. This might be due to the different setting for T\n",
      "but is not mentioned in the paper.\n",
      "They also evaluate the effect of kernel sparsity and entropy . F or ResNet-56 on ImageNet,\n",
      "they show that using both kernel sparsity and kernel entropy outperform using either one of\n",
      "these alone.\n",
      "25\n",
      "This chapter explained the search for compression methods, showing the state-of-the-art in T a-\n",
      "ble 3.1 , which methods have been selected and how these selected methods work. Specifically ,\n",
      "it answered the question: What are the best networks for comparing compression methods?\n",
      "Namely , ResNet-50 (on ImageNet) and ResNet-56 (on CIF AR-10). As well as answering:\n",
      "What is the state-of-the-art in neural network compression? and narrowing down Which com-\n",
      "pression method is best suited for compressing YOLOv7? to two methods: GBIP and KSE.\n",
      "26\n",
      "4 YOLOv7-tiny Experiments\n",
      "The previous chapter introduced the two selected compression methods. This chapter will go\n",
      "over the specific implementation details of adapting these methods for compressing YOLOv7-\n",
      "tiny on MS COCO. It will also introduce the experiments that are run to determine the best\n",
      "performing method for YOLOv7.\n",
      "First, Section 4.1 mentions the evaluation metrics and implementation details that are\n",
      "similar for both methods. Then, Section 4.2 and 4.3 will go into the specifics for GBIP and\n",
      "KSE, respectively .\n",
      "4.1 General\n",
      "The available implementation of both YOLOv7-tiny and KSE use PyT orch [ 72] as machine\n",
      "learning framework. T o avoid unnecessary complexities, all the experiments also use PyT orch.\n",
      "As much as possible, the hyperparameters chosen by the creators of YOLOv7 are kept the\n",
      "same for the compression experiments. F or example, the batch size, learning rate scheduler\n",
      "and data augmentation, are not changed. This means a batch size of 64 and the use of cosine\n",
      "annealing (See Subsection 2.2.3 ). The starting learning rate is determined by training the\n",
      "network for different learning rates and check how fast the network converges with each.\n",
      "Also from the YOLOv7 implementation, while training and evaluating the network the\n",
      "fitness metric is used. F or the following experiments the fitness, AP and AP 50 are reported.\n",
      "See Subsection 2.2.3 how these metrics are computed. T o determine the amount of compression\n",
      "that is achieved, the parameter pruning ratio (PPR) and FLOPs pruning ratio (FPR) are used.\n",
      "These are explained in Section 2.3.2 .\n",
      "4.2 GBIP\n",
      "4.2.1 Experiments\n",
      "GBIP has only one hyperparameter influencing the amount of compression, pruning threshold\n",
      "factor k. The authors find the best results when using k= 0.3−0.6. Since YOLOv7-tiny is\n",
      "already relatively small, it makes sense to also test k= 0.2, so the following values will be\n",
      "tested: k={0.2,0.3,0.4,0.5,0.6}. These experiments will run three pruning cycles, where\n",
      "each pruning cycle consists of a pruning step followed by 10 epochs of finetuning.\n",
      "T o increase the accuracy of the pruned network, GBIP introduces three finetuning tech-\n",
      "niques: output transfer (OT), attention transfer (A T) and adversarial game (AG). The authors\n",
      "do an ablation study to determine the eﬀicacy of each of these techniques. The ablation study\n",
      "is done on three different networks, but only for image classification. Since the object detection\n",
      "task of YOLOv7(-tiny) is significantly different, and the fact that the effect measured in the\n",
      "ablation study differs considerably per network and dataset, before running the full experi-\n",
      "ments mentioned above an ablation study is done for YOLOv7-tiny . F or this ablation study ,\n",
      "only one pruning cycle will be tested with k= 0.4. That is, the network will be pruned once,\n",
      "followed by 10 epochs of finetuning. If any of the finetuning techniques does not improve the\n",
      "final accuracy , it can be omitted for the other experiments.\n",
      "27\n",
      "4.2.2 Implementation\n",
      "The authors do not provide their own implementation online. Therefore, GBIP has been\n",
      "implemented from scratch in PyT orch. The explanation in the paper is entirely focused on an\n",
      "application for image classification. Some adjustments have been made to be able to use this\n",
      "method for object detection with YOLOv7-tiny on MS COCO.\n",
      "The pruning step is identical, but the finetuning techniques have been (slightly) changed\n",
      "to fit the YOLOv7-tiny architecture. F or attention transfer, only the location of the attention\n",
      "maps must be chosen. But given the different output structure of YOLOv7-tiny , compared\n",
      "to a typical classification network like ResNet-56, the output transfer and adversarial game\n",
      "requires more changes. The rest of this subsection will describe these changes.\n",
      "Attention T ransfer\n",
      "The original implementation uses the activations of three layers in the network to calculate\n",
      "an attention transfer loss. No explanation is given for the exact positioning of these layers,\n",
      "but they are spaced out evenly through the network. Several combinations of layers have\n",
      "been tested by pruning with k= 0.5and finetuning for one epoch. The results are shown in\n",
      "T able 4.1 . Note that due to the limited finetuning, the accuracy increase is only an indication\n",
      "of the relative performance. So, which layers should be used for attention transfer? F rom the\n",
      "limited testing done, it seems that (i) it is better to include more layers than three, and (ii)\n",
      "that later layers ( [57,65,73] ) work better than earlier layers ( [0,1]). The best result (shown in\n",
      "bold) is obtained when using the output of all ELAN blocks to create the attention maps.\n",
      "T able 4.1 :Layer indices of at-\n",
      "tention maps tested for At-\n",
      "tention T ransfer. See Ap-\n",
      "pendix A.1 for the YOLOv7-\n",
      "tiny architecture with indices.\n",
      "YOLOv7-tiny has been pruned\n",
      "with k= 0.5and finetuned for\n",
      "one epoch.Layer IndicesFitness\n",
      "Increase\n",
      "37, 40, 50 0.49\n",
      "14, 21, 28 0.67\n",
      "7, 14, 21, 28 0.79\n",
      "57, 65, 73 1.39\n",
      "0, 1 -0.12\n",
      "7, 14, 21, 28, 47, 57, 65, 73 1.77\n",
      "0, 1, 7, 14, 21, 28, 47, 57, 65, 73 1.71\n",
      "Output T ransfer\n",
      "Given that the output of an image classification network is different from YOLOv7, the way\n",
      "the output transfer loss is calculated requires some changes. As explained in Subsection 3.2.1 ,\n",
      "to align the pruned network with the teacher, the KL divergence between the (softened)\n",
      "output of both networks is added to the loss function. This makes sense for an output of\n",
      "class probabilities, but the output of YOLOv7 not only contains class probabilities but also\n",
      "a bounding box and objectness score (see Subsection 2.2.2 ). T o make full use of the output,\n",
      "the implemented transfer learning loss, LTL , consists of three parts: (i) a KL divergence loss\n",
      "between the class probabilities, (ii) a binary cross-entropy loss between the objectness scores,\n",
      "and (iii) an IoU loss between the bounding boxes of the student and the teacher.\n",
      "Note that these three additional losses match with the three loss components of YOLOv7\n",
      "(see Subsection 2.2.3 ). The KL divergence loss, LKL(WS,class _probs , is identical to Eq 3.3 -3.6 ,\n",
      "while the IoU loss, LIoU(WS,bbox), is similar to the IoU loss of YOLOv7. These six losses are\n",
      "combined, in the same way as Equation 3.7 , to form the output transfer loss:\n",
      "28\n",
      "LTL(WS) =LKL(WS,class_probs ) +LBCE (WS,objectness ) +LIoU(WS,bbox) (4.1)\n",
      "LOT(WS) =αLTL(WS) + (1 −α)LYOLO (WS) (4.2)\n",
      "Adversarial Game\n",
      "As mentioned in Subsection 3.2.3 , the discriminator for the image classification tasks is a 128-\n",
      "256-128 fully-connected network. This network takes as input a 1D tensor with class probabil-\n",
      "ities. Given the difference in output of YOLOv7, some changes to the discriminator network\n",
      "must be made. The output of YOLOv7 consists of three 3D tensors (see Subsection 2.2.3 ). F or\n",
      "an input image of [640×640] , the output is ( [255×80×80],[255×40×40],[255×20×20] ). It\n",
      "is not directly obvious how the adversarial network should deal with this output. Therefore,\n",
      "three different types of architecture have been tested.\n",
      "The first architecture uses max pooling to reduce the size of all 3D tensors to [255×1×1].\n",
      "Then, the three tensors are concatenated and flattened, resulting in a 1D tensor with a length\n",
      "of 765. Finally , this 1D tensor is run through a 128-256-128 fully-connected network.\n",
      "F or the second network, max pooling is used on the second and third 3D tensors to create\n",
      "three tensors of size [255×20×20] . These tensors are concatenated and run through two\n",
      "convolutional layers, followed by one final fully-connected layer.\n",
      "The last architecture most closely resembles the original adversarial network from the\n",
      "GBIP paper. First, each 3D tensor is reshaped from [255×80×80] to[19200 ×85] and\n",
      "removing the bounding box information and objectness score gives [19200 ×80] . This tensor\n",
      "contains 19200 probability distributions for each 80 classes in MS COCO. The original 128-\n",
      "256-128 fully-connected network can now be used on all 19200 probability distributions. The\n",
      "final output of the network is simply the average of all these 19200 runs.\n",
      "Several tests have been done, slightly tweaking certain parts like kernel size for convolution\n",
      "and max pooling, number of neurons in fully-connected layers and total number of layers.\n",
      "Given these tests, and the fact that it is most similar to the original network, the fully-\n",
      "connected 128-256-128 network has been chosen.\n",
      "In the final implementation, based on [ 71], alternately the student network is trained with\n",
      "adversarial loss for a period while the adversarial network stays fixed, and then the adversarial\n",
      "network is updated while the student network is trained without adversarial loss.\n",
      "4.3 KSE\n",
      "4.3.1 Experiments\n",
      "KSE has two hyperparameters regulating compression: G andT. A higher value for G results\n",
      "in higher compression granularity , while higher values for Tlead to higher compression ratios.\n",
      "T o find the best values for these hyperparameters, the same values as used in the KSE paper\n",
      "(G={3,4,5,6}, T={0,1}) are tested on YOLOv7-tiny .\n",
      "In contrast to GBIP , KSE compresses the network once, followed by several epochs of\n",
      "finetuning. In the case of YOLOv7-tiny , the accuracy reaches its maximum after about 15\n",
      "epochs of finetuning. T o be sure, all compressed networks will be finetuned for 20 epochs.\n",
      "4.3.2 Implementation\n",
      "F ortunately , the authors of KSE provide an implementation of their code online [LINK]. In\n",
      "contrast to GBIP , which requires significant changes for YOLOv7 due to the used finetuning\n",
      "29\n",
      "techniques, KSE does not use any special finetuning. This makes it relatively easy to adapt\n",
      "to YOLOv7-tiny .\n",
      "Unfortunately , the published code contains an unoptimized implementation of their method.\n",
      "The output of each layer is not computed using Equation 3.20 , but rather a weight matrix is\n",
      "created by matching the clusters Bi,c and indices In,c withWn,c=BIn,c,c(See Subsection 3.3.3\n",
      "for notation). This weight matrix has a size of [N×C′×k×k], where C′is the number of\n",
      "input channels with qc>0. This is almost identical to the size of the original weight matrix,\n",
      "since only a fraction of the input channels can be removed entirely .\n",
      "F or now, the optimization will be ignored, and revisited if the KSE outperforms GBIP .\n",
      "The same formula used by the authors of KSE to report the achieved acceleration will be used\n",
      "to compare with GBIP . This formula for one layer, rewritten to match FLOPs pruning ratio\n",
      "(FPR) definition, is given by:\n",
      "FPR =P\n",
      "cqc\n",
      "NC(4.3)\n",
      "FPR =P\n",
      "cqc\n",
      "NC(4.3)\n",
      "Note that with the current unoptimized implementation, the actual FPR for each layer is\n",
      "simply:\n",
      "FPR =C′\n",
      "C(4.4)\n",
      "The theoretical PPR is the same as for the unoptimized implementation. Although a full\n",
      "weight matrix is constructed, the actual parameters that are saved and tracked are the cluster\n",
      "centroid.\n",
      "Similar, to the original implementation, for YOLOv7-tiny the first and last layers are not com-\n",
      "pressed. The reason for this is that the first layer contains all information in only three input\n",
      "channels (RGB), so compressing this layer might remove too much information. Compression\n",
      "in the intermediate layers can be partly recovered in later layers, which is not the case for the\n",
      "last layer.\n",
      "This chapter listed the experiments that will be run on YOLOv7-tiny , as well as the specific\n",
      "implementation details of GBIP and KSE. In the next chapter, the results of these experiments\n",
      "are given, based on which one method is selected to run on YOLOv7.\n",
      "30\n",
      "5 Selecting the Best Method\n",
      "This chapter will begin by reporting the results of running GBIP and KSE on YOLOv7-tiny in\n",
      "Section 5.1 . Based on these results, Section 5.2 will answer the question: Which compression\n",
      "method is best suited for compressing YOLOv7?\n",
      "5.1 Compression Results\n",
      "5.1.1 GBIP\n",
      "An ablation study is performed to determine the eﬀicacy of the three added finetuning tech-\n",
      "niques: attention transfer (A T), output transfer (OT), adversarial game (AG). The results are\n",
      "shown in T able 5.1 . A T performs significantly better than the baseline. OT only manages a\n",
      "marginal increase in fitness, while AG shows no improvement at all. Combining A T and OT,\n",
      "which both increase fitness individually , does not result in an improvement over just using A T.\n",
      "Similarly , combining A T with AG does not increase the fitness. Given these results, the next\n",
      "experiments have been performed using only A T.\n",
      "T able 5.1 :Results of the abla-\n",
      "tion study on YOLOv7-tiny .\n",
      "All experiments are performed\n",
      "with k= 0.6. A T: Attention\n",
      "T ransfer, OT: Output T rans-\n",
      "fer, AG: Adversarial Game.\n",
      "Fitness and AP are percent-\n",
      "ages. Best results are marked\n",
      "in bold.A T OT AG Fitness AP AP 50\n",
      "- - - 31.4 29.7 47.1\n",
      "✓ - - 33.9 32.2 49.9\n",
      "-✓ - 32.4 30.7 47.9\n",
      "- -✓ 31.4 29.7 47.1\n",
      "✓ ✓ - 33.8 32.0 49.6\n",
      "✓ -✓ 33.9 32.2 49.9\n",
      "The results of running GBIP on YOLOv7-tiny for k={0.2,0.3,0.4,0.5,0.6}are shown in\n",
      "T able 7.1 . There are two observations that can be made from this data. Firstly , the number\n",
      "of FLOPs decreases faster than the number of parameters. The authors of GBIP show that\n",
      "for VGG-16 and GoogLeNet on CIF AR-10 the FPR and PPR stay within about 5%pt of each\n",
      "other. F or k= 0.6there is a difference of 23.1%pt between FPR and PPR. The reason for\n",
      "this, is that the first layers, which contain the most FLOPs, are pruned slightly more than\n",
      "the rest of the network. F or the networks tested by the authors, the FLOPs are more evenly\n",
      "distributed throughout the networks than is the case for YOLOv7-tiny .\n",
      "Secondly , the FPR does not decrease linearly with increasing k. This is to be expected,\n",
      "given that the network is pruned multiple times. A smaller kwill remove a small portion of\n",
      "channels in the first pruning cycle, and then another small portion of the remaining network in\n",
      "the next cycle. While larger values for kwill remove a larger portion of channels the first time,\n",
      "and another large portion of an already smaller network the second time, thus compounding\n",
      "the compression effect.\n",
      "31\n",
      "T able 5.2 :Results of GBIP ex-\n",
      "periments on YOLOv7-tiny .\n",
      "Fitness, AP , FLOPs Prun-\n",
      "ing Ratio (FPR) and Param-\n",
      "eter Pruning Ratio (PPR) are\n",
      "given as percentages. Note k=\n",
      "0refers to the unpruned base-\n",
      "line.k Fitness AP AP 50 PPR FPR\n",
      "0 39.2 37.4 55.2 100 100\n",
      "0.2 39.0 37.2 55.4 99.8 99.6\n",
      "0.3 38.4 36.6 54.7 97.9 93.6\n",
      "0.4 37.6 35.8 54.0 95.2 86.8\n",
      "0.5 34.8 33.0 50.9 85.8 68.6\n",
      "0.6 28.0 26.3 42.9 65.4 42.2\n",
      "5.1.2 KSE\n",
      "5.1.2 KSE\n",
      "The results of running KSE on YOLOv7-tiny are shown in T able 5.3 . F or the FPR both the\n",
      "actual and theoretical values are given, while the actual PPR is identical to the theoretical\n",
      "values. The actual values refer to the used unoptimized implementation, while the theoretical\n",
      "values are computed using Equation 4.3 (see Subsection 4.3.2 ).\n",
      "Regarding the actual FPR, if granularity G gets higher, the less reduction in FLOPs is\n",
      "achieved. This is expected, since with higher granularity , the number of clusters assigned to\n",
      "an input channel can be very small. As mentioned in Subsection 4.3.2 , the actual FPR is\n",
      "based on the number of input channels that are completely ignored, that is qc= 0 . With\n",
      "low granularity , either no clusters or a large number of clusters is assigned, resulting in the\n",
      "removal of relatively unimportant input channels, which in the case of high granularity might\n",
      "have been assigned just a few clusters.\n",
      "In general, increasing G andT increases the compression. It seems that the specific\n",
      "combination of G andT does not matter much for the resulting accuracy . F or both G=\n",
      "5, T= 0 andG= 3, T= 1 the FPR is about 70%, and their fitness is identical. The same can\n",
      "be seen for G= 6, T= 0 andG= 4, T= 1 , where the FPR is around 60% and the difference\n",
      "in fitness is 0.6%pt.\n",
      "T able 5.3 :Results of\n",
      "KSE experiments on\n",
      "YOLOv7-tiny . Fit-\n",
      "ness, AP , PPR and\n",
      "ness, AP , PPR and\n",
      "FPR are percentages.\n",
      "Note that for the\n",
      "FPR both the actual\n",
      "and theoretical val-\n",
      "ues are given, the-\n",
      "oretical PPR is the\n",
      "same as the actual\n",
      "values (see Subsec-\n",
      "tion 4.3.2 ).G T Fitness AP AP 50 PPR FPR FPR\n",
      "theoretical actual\n",
      "- - 39.2 37.4 55.2 100 100 100\n",
      "3 0 36.9 35.0 53.4 88.1 67.4 92.9\n",
      "4 0 36.8 35.0 53.2 80.7 55.7 96.7\n",
      "5 0 36.0 34.2 52.4 70.7 46.3 97.9\n",
      "6 0 34.8 33.0 51.2 62.3 39.6 98.2\n",
      "3 1 36.0 34.2 52.4 69.9 54.7 92.9\n",
      "4 1 35.4 33.6 51.8 59.4 40.4 96.7\n",
      "5 1 34.1 32.3 50.3 51.2 32.2 97.9\n",
      "6 1 31.8 30.0 47.8 44.8 27.4 98.2\n",
      "5.2 Best Method\n",
      "5.2.1 Comparing GBIP and KSE\n",
      "F rom the results of compressing YOLOv7-tiny , the most relevant information is the reduction\n",
      "in fitness with respect to the FLOPs Pruning Ratio (FPR). This information is shown in\n",
      "32\n",
      "Figure 5.1 :Fitness vs FPR for GBIP , KSE (theoretical) and KSE (actual). According to the\n",
      "KSE paper, the theoretical FLOPs should be possible with some optimization. KSE (actual)\n",
      "gives the current FPR of the authors’ own implementation of KSE, without optimization.\n",
      "F or GBIP the end result of each pruning cycle is shown, so 5 different values for kwith 3\n",
      "pruning cycles each gives 15 results.\n",
      "Fig. 5.1 and is taken directly from T ables 7.1 -5.3 . F or KSE it shows both the actual and\n",
      "theoretical FPR. Because the current implementation of KSE can only ignore input channels\n",
      "that are fully pruned ( qc= 0 ), which is only a fraction of the total channels, the actual FPR\n",
      "is very close to 100. However, comparing GBIP with the theoretical values of KSE shows a\n",
      "significant advantage for KSE. F or all FPR the fitness is higher for KSE than GBIP . Assuming\n",
      "KSE can indeed be optimized to achieve these theoretical values, KSE is the preferred method\n",
      "for compressing YOLOv7. If not, clearly the unoptimized version of KSE is significantly worse\n",
      "and GBIP is the better method.\n",
      "KSE uses a type of unstructured pruning, which means that some additional optimization\n",
      "is required to achieve acceleration. This is in contrast with structured pruning methods,\n",
      "like GBIP , that directly prune entire channels or filters. The authors of KSE detail how\n",
      "the computation of a compressed convolutional layer could be optimized. Unfortunately , the\n",
      "optimization of KSE is not as straightforward as the paper itself might suggest. The following\n",
      "subsection documents how the optimization is implemented in PyT orch.\n",
      "5.2.2 PyT orch optimization\n",
      "As discussed in Subsection 4.3.2 , the implementation by the authors create a full weight matrix,\n",
      "by copying the calculated clusters several times, which only results in a small acceleration if\n",
      "an entire input channel was pruned. T o accelerate the pruned network, the authors suggest\n",
      "taking the convolution of each input channel cwith all clusters Bi,c for that channel. Using\n",
      "the indices that matches output channels nwith the clusters, In,c , the output can be obtained:\n",
      "33\n",
      "Zi,c=Bi,c∗Xc (5.1)\n",
      "Yn=CX\n",
      "c=1ZIn,c,c (5.2)\n",
      "Where Bi,c is the i-th cluster for input channel candIn,c is the index of the cluster that\n",
      "convolved with input channel cis part of output channel n.\n",
      "This is relatively easy to implement in PyT orch but turns out to make the network much\n",
      "slower. One large convolution is split into a lot of small ones. Although the network is\n",
      "compressed, the number of small convolutions is still two orders of magnitude higher than the\n",
      "original number of convolutions. This has two main drawbacks.\n",
      "Firstly , the optimization requires a lot of manual indexing, which is relatively slow. All\n",
      "those small convolutions still need to be matched and summed together (Equation 5.2 ). This\n",
      "also means that the intermediate results must be stored till the final output is computed. With\n",
      "a large convolution the intermediate results are only needed in the CUDA kernel, where they\n",
      "can immediately be added to the final output without storing the result.\n",
      "This is part of the second problem: running a CUDA kernel has some overhead. This\n",
      "overhead comes from launching and initializing the kernel, and moving the data from the slower,\n",
      "global memory of the GPU to the fast L 1memory . So, running a lot of smaller convolutions\n",
      "ends up costing more time than a few larger convolutions.\n",
      "T o see the difference between the two implementations, the output of the PyT orch Profiler\n",
      "is given in T able 5.4 . This shows the total and average GPU time as well as the number of\n",
      "calls for the most used operators. Especially the number of calls is interesting. It shows\n",
      "7625 convolutions for the optimized implementation compared to 58 in the original. And\n",
      "although these are much smaller convolutions, the average GPU time is almost identical. The\n",
      "other operators are almost all related to storing the intermediate results and indexing these to\n",
      "compute the final output. Most of these operations still happen in the original implementation\n",
      "but are run on an optimized convolution CUDA kernel.\n",
      "F rom the profiler output, it is clear that using the ’optimization’ in PyT orch is not an op-\n",
      "tion. It is possible that a custom-made, optimized CUDA kernel implementation will accelerate\n",
      "KSE. However, this falls outside the scope of this thesis.\n",
      "5.2.3 Selecting Best Method\n",
      "Although KSE could theoretically outperform GBIP , with the current implementation, pro-\n",
      "vided online by the authors of KSE, this is not the case. An attempt has been made to optimize\n",
      "this implementation in PyT orch, but this turns out to make things worse.\n",
      "GBIP , on the other hand, uses structured pruning which makes it easy to obtain a working,\n",
      "faster model without the need for optimization. GBIP also allows for more control in the size\n",
      "of the resulting network, by varying the number of pruning cycles. So, which compression\n",
      "method is best suited for compressing YOLOv7? Looking at Figure 5.1 , comparing GBIP with\n",
      "the actual values for KSE, it is clear that GBIP is the better method. Thus, GBIP is selected\n",
      "for compressing YOLOv7.\n",
      "Based on the selection of GBIP for compressing YOLOv7, the next chapter will detail the\n",
      "experiments that are run on YOLOv7.\n",
      "34\n",
      "T able 5.4 :PyT orch profiler output for the original ( orig. ) and optimized ( opt. ) implementation\n",
      "of KSE. The values are an average of 10 runs, with G= 3, T= 0 . This table contains all\n",
      "operators that take up at least 1% of GPU time in the optimized implementation.\n",
      "Operator NameT otal GPU [ms] Num. Calls A vg. GPU [us]\n",
      "orig. opt. orig. opt. orig. opt.\n",
      "aten::conv2d 20.60 2732.40 58 7625 355 358\n",
      "aten::convolution 19.99 2584.40 58 7625 345 339\n",
      "aten::_convolution 19.13 2458.60 58 7625 330 322\n",
      "aten::cudnn_convolution 17.83 1271.70 58 7625 307 167\n",
      "aten::index 5.56 922.90 54 7621 103 121\n",
      "aten::contiguous 6.37 852.70 57 7570 112 113\n",
      "aten::clone 5.56 746.30 57 7570 97 99\n",
      "aten::select 0.27 705.90 9 22710 30 31\n",
      "aten::add 0.08 451.40 3 7624 27 59\n",
      "aten::slice 2.07 445.90 63 15197 33 29\n",
      "aten::as_strided 1.42 418.60 189 53050 8 8\n",
      "aten::item - 413.90 - 7567 - 55\n",
      "aten::_local_scalar_dense - 285.40 - 7567 - 38\n",
      "aten::empty_like 3.16 264.90 112 7625 28 35\n",
      "aten::unsqueeze - 238.00 - 7567 - 31\n",
      "aten::reshape 1.80 231.60 57 7624 32 30\n",
      "aten::copy_ 1.62 231.30 60 7573 27 31\n",
      "aten::empty 3.76 186.10 408 15497 9 12\n",
      "total 80 7187\n",
      "35\n",
      "6 YOLOv7 Experiments\n",
      "Based on the experiments on YOLOv7-tiny , which are discussed in the previous chapters,\n",
      "GBIP has been chosen for compressing YOLOv7. This chapter will explain which experiments\n",
      "are performed in Section 6.1 . This is followed by the implementation details in Section 6.2 ,\n",
      "which will answer the question: What are the optimal hyperparameters to compress YOLOv7?\n",
      "6.1 Experiments\n",
      "The goal of the experiments is to compress YOLOv7 to the same number of FLOPs as\n",
      "YOLOv7-tiny . As shown in T able 2.1 , YOLOv7 has about 7.7 times more FLOPs than\n",
      "YOLOv7-tiny , which corresponds to a FLOPs pruning ratio (FPR) of 13.2%. Given that\n",
      "for YOLOv7-tiny a FPR of 42% was achieved with k= 0.6, it is expected that a large value\n",
      "ofkis needed to compress YOLOv7 to the required FPR of 13.2%. There is no way to de-\n",
      "termine exactly what kwill result in enough compression, but fortunately , by increasing the\n",
      "number of pruning cycles, it is possible to keep compressing the network further. This way it\n",
      "is always possible to achieve the required FPR, but the increased training time might make\n",
      "this infeasible.\n",
      "Since it is diﬀicult to know which kwill produce the required compression, the experiments\n",
      "start with k= 0.9running till it reaches this compression. Based on the results of this\n",
      "experiment, the value of kwill be adjusted. If it takes longer than three pruning cycles to\n",
      "reach the required compression, higher values of kwill be tested and vice versa.\n",
      "6.2 Implementation Details\n",
      "Most of the implementation details of GBIP for YOLOv7 are identical to YOLOv7-tiny , which\n",
      "can be found in Subsection 4.3.2 .\n",
      "6.2.1 Attention T ransfer\n",
      "Given that the output of ELAN blocks worked well for attention transfer on YOLOv7-tiny ,\n",
      "these are used as starting point for YOLOv7. T able 6.1 shows which layers have been tested.\n",
      "ELAN refers to all ELAN blocks, DOWN to the downsample layers at the first bottom-to-\n",
      "top pathway . Layers 51, 54 and 66 link the bottom-to-top pathway with the top-to-bottom\n",
      "pathway . See Appendix A.2 for a detailed overview of the YOLOv7 architecture with indices.\n",
      "The best result is highlighted in bold and will be used in the experiments.\n",
      "6.2.2 Pruning Batch Size\n",
      "Since the importance score for each output channel is computed based on the activation values,\n",
      "instead of the weight values, data is required. The amount of data used to calculate the\n",
      "importance scores, called the pruning batch size, can significantly increase the pruning time.\n",
      "The authors of GBIP do not mention how much data is used, or if they use the entire dataset.\n",
      "37\n",
      "T able 6.1 :Layer indices of atten-\n",
      "tion maps tested for Attention\n",
      "T ransfer. See Appendix A.2\n",
      "for the YOLOv7 architecture\n",
      "with indices. YOLOv7 has\n",
      "been pruned with k= 0.8\n",
      "and finetuned for one epoch.\n",
      "DOWN: downsample layers\n",
      "16, 29, 42Layer IndicesFitness\n",
      "Increase\n",
      "ELAN, 51 1.05\n",
      "ELAN, DOWN 1.12\n",
      "ELAN, 51, 54, 66 1.51\n",
      "ELAN, 51, DOWN 0.85\n",
      "ELAN, 51, DOWN, 54, 66 1.61\n",
      "T o find a reasonable value for the pruning batch size, the pruning step is run for increasing\n",
      "pruning batch sizes and recording which output channels are pruned. The results for four\n",
      "different layers of the network are shown in Figure 6.1 . At small batch sizes the number and\n",
      "location of pruned channels varies, this is especially clear for layer 24 (bottom left). At larger\n",
      "batch size there only a few channels, with importance scores close to the pruning threshold,\n",
      "that still show change. F rom these figures the pruning batch size has been set to 210training\n",
      "samples.\n",
      "samples.\n",
      "Figure 6.1 :Pruning YOLOv7 with k= 0.9for a different number of training samples, or\n",
      "pruning batch size. Orange indicates the output channel at this index is pruned. Solid\n",
      "horizontal lines mean that whether this channel is pruned does not change with increasing\n",
      "batch size.\n",
      "6.2.3 Ablation Study\n",
      "Given the change of network, an ablation study is again performed to the test the importance\n",
      "of GBIP’s finetuning techniques: attention transfer, output transfer and adversarial game.\n",
      "Since higher values of kare expected, the ablation study will run with k= 0.9. Based on the\n",
      "results of the ablation study on YOLOv7-tiny , the number of epochs for each test is reduced to\n",
      "38\n",
      "three. At three epochs the relative difference between techniques was obvious in YOLOv7-tiny ,\n",
      "with only a slight increase in fitness afterwards.\n",
      "T able 6.2 shows the results of the ablation study . Like YOLOv7-tiny , attention transfer\n",
      "again has the largest effect on accuracy recovery . But this time combining attention transfer\n",
      "with output transfer has a slight edge over just attention transfer. F or this reason, attention\n",
      "transfer and output transfer are both used in the experiments.\n",
      "T able 6.2 :Results of the ablation\n",
      "study on YOLOv7. All exper-\n",
      "iments are performed with k=\n",
      "0.9. A T: Attention T ransfer,\n",
      "OT: Output T ransfer, AG: Ad-\n",
      "versarial Game. Fitness and AP\n",
      "are percentages. Best results are\n",
      "marked in bold.A T OT AG Fitness AP AP 50\n",
      "- - - 41.2 39.5 57.3\n",
      "✓ - - 44.2 42.3 60.7\n",
      "-✓ - 43.4 41.6 59.3\n",
      "- -✓ 42.0 40.2 58.0\n",
      "✓ ✓ - 44.3 42.5 60.5\n",
      "6.2.4 Learning Rate\n",
      "T o decide the initial learning rate a learning rate range test [ 73] is performed. While training\n",
      "the learning rate is slowly increased from a very low ( 10−6) to a very high (1) value while\n",
      "logging the loss value. The optimal learning rate, where the loss decreases fastest, is obtained\n",
      "by taking the derivative of the loss with respect to the learning rate. A small amount of\n",
      "smoothing, using a moving average, is applied to filter out noise for both the loss and its\n",
      "derivative.\n",
      "Figure 6.2 shows the loss and its derivative for the ablation study experiments. The loss\n",
      "decreases up to a learning rate of around 10−2. F rom the derivative plot, the optimal learning\n",
      "rate for ’None’ (no finetuning techniques) is around 3×10−3. The other variants are all very\n",
      "similar to each other (which is why only ’A T’ is shown in color) and have an optimal learning\n",
      "rate around 6×10−4.\n",
      "F or all experiments, including the ablation study , this approach has been taken to deter-\n",
      "mine the optimal learning rate. Not only at the start of the experiment, but also after each\n",
      "successive pruning step. This learning rate is still used in combination with cosine annealing\n",
      "as described in Subsection 2.2.3 .\n",
      "This chapter explained how the experiments will be run. It answered the question: What\n",
      "are the optimal hyperparameters to compress YOLOv7? The layers used for attention transfer\n",
      "are indicated in T able 6.1 . The pruning batch size is set to 210based on Figure 6.1 . An\n",
      "ablation study (Subsection 6.2.3 ) is performed to determine that attention transfer and ouput\n",
      "transfer will be used as finetuning techniques. And the procedure for determining the learning\n",
      "rate is explained in Subsection 6.2.4 .\n",
      "39\n",
      "(a) Loss.\n",
      " (b) Derivative of the loss.\n",
      "Figure 6.2 :Determining learning rates for k= 0.9using different finetuning techniques. Learn-\n",
      "ing rate is slowly increased while monitoring loss. Maximum learning rate is at the point\n",
      "where the loss starts increasing. The minimum of the derivative of the loss indicates the\n",
      "optimal learning rate where the loss decreases the fastest. F or ’None’ this optimal learning\n",
      "rate lies around 2×10−3, while for the other variants it is around 6×10−4.\n",
      "40\n",
      "7 Results\n",
      "This chapter reports the results of compressing YOLOv7 with GBIP . In Section 7.1 , these\n",
      "results are used to answer the main research question: Can a state-of-the-art compression\n",
      "of YOLOv7 achieve higher accuracy than YOLOv7-tiny at the same number of floating-point\n",
      "operations? In Section 7.2 , the different compressed models are evaluated and compared,\n",
      "answering the questions: Does one large pruning step work better than several smal ler pruning\n",
      "steps? and: Why is the selected compression method not able to outperform YOLOv7-tiny?\n",
      "7.1 Final Results\n",
      "YOLOv7 has been compressed with different values of k, stopping when the FLOPs Pruning\n",
      "Ratio (FPR) is close to that of YOLOv7-tiny (13.2). T able 7.1 gives the results of these\n",
      "experiments. The best compressed network is achieved with k= 0.95 after 2 pruning steps.\n",
      "k= 0.92 has a very small fitness increase, but a worse FPR.\n",
      "Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-\n",
      "tiny at the same number of floating-point operations? Surprisingly , none of the compressed\n",
      "networks achieves a fitness close to that of YOLOv7-tiny . This means that it is better to use\n",
      "the handcrafted YOLOv7-tiny than compressing YOLOv7 with a state-of-the-art compression\n",
      "method.\n",
      "T able 7.1 :Results of GBIP\n",
      "experiments on YOLOv7.\n",
      "Fitness, AP , FLOPs\n",
      "Pruning Ratio (FPR)\n",
      "and Parameter Pruning\n",
      "Ratio (PPR) are given as\n",
      "percentages. Note k= 0\n",
      "refers to the unpruned\n",
      "baseline.kPruning\n",
      "StepsFitness AP AP 50 PPR FPR\n",
      "YOLOv7-tiny 39.2 37.4 55.2 16.8 13.2\n",
      "0.9 5 22.9 21.6 34.3 20.9 13.0\n",
      "0.91 3 27.0 25.6 39.5 19.6 12.6\n",
      "0.92 3 28.5 27.0 41.3 19.3 12.6\n",
      "0.95 2 28.4 27.0 41.3 15.3 11.3\n",
      "1.015 1 21.5 20.3 32.5 12.8 13.1\n",
      "The fitness and FPR after each pruning step for all experiments are shown in Figure 7.1 .\n",
      "The bottom left of the left plot, where the compressed models reach the required FPR, is\n",
      "magnified on the right. The fitness degrades quicker the further the model is compressed.\n",
      "This is expected since at first less relevant information can be removed with little accuracy\n",
      "loss, but at lower FPR all unimportant filters are already removed and compressing further\n",
      "reduces the accuracy faster.\n",
      "It also seems that with many pruning steps, like k= 0.9with 5 steps, the accuracy\n",
      "decreases faster than with only two or three pruning steps. Therefore, k= 1.015 has been\n",
      "tested to see if this extends to only one pruning step. However, after pruning once with\n",
      "k= 1.015 the required FPR is reached, but the resulting fitness is below even that of k= 0.9.\n",
      "41\n",
      "Figure 7.1 :Fitness vs FPR for GBIP experiments after each pruning step for different k. Final\n",
      "models are at the left bottom (within the black box) and are enlarged on the plot on the\n",
      "right. Solid lines indicate FPR (13.2%) and fitness (39.2%) of YOLOv7-tiny .\n",
      "7.2 Comparison\n",
      "This section answers the questions: Does one large pruning step work better than several\n",
      "smal ler pruning steps? and: Why is the selected compression method not able to outperform\n",
      "YOLOv7-tiny?\n",
      "YOLOv7-tiny?\n",
      "Figure 7.2 shows the pruning ratio of each layer in YOLOv7 after the first and last pruning\n",
      "step for different values of k. Looking at Figure 7.2a , obviously a higher kcompresses the\n",
      "network further in one step. F or k= 0.9to 0.92 and to 0.95 the pruning ratio decreases quite\n",
      "gradually with highs and lows in the same locations. However, for k= 1.015 the pruning ratio\n",
      "curve is almost flat, with only minor variation between layers. This suggests that at such high\n",
      "kthe compression is no longer able to distinguish between layers with important filters and\n",
      "those with less important filters.\n",
      "In Figure 7.2b all models have very similar FPR. k= 1.015 is again an outlier, but the\n",
      "other curves all follow a similar trajectory . However, the lower k, the more accentuated the\n",
      "peaks and valleys are. This means that certain layers are compressed extremely far, while\n",
      "others are compressed significantly less compared to higher k.\n",
      "It seems that this is mostly caused by the number of pruning steps. Figure 7.3 shows the\n",
      "pruning ratio for all pruning steps at k= 0.9. Each successive pruning step further accentuates\n",
      "the peaks and valleys, resulting in an extremely low pruning ratio for certain layers, which\n",
      "might create a bottleneck for the information flow in the network and thus cause increased\n",
      "accuracy degradation.\n",
      "So far it is found that large k, like 1.015, results in a very even pruning ratio, which\n",
      "ignores the relative importance of the layers. At the same time, low values of krequire more\n",
      "pruning steps and thus over compress less important layers. These two facts explain why the\n",
      "experiments with 2 or 3 pruning steps perform better than the others.\n",
      "It is also interesting to look at the locations of the layers with high and low pruning ratio.\n",
      "F rom Figure 7.2b it can be seen that layers 32 to 50, 84 to 86 and 96 to 99 have a relative\n",
      "high pruning ratio, suggesting that these layers contain more relevant information. Figure 7.4b\n",
      "highlights the location of these layers in the YOLOv7 architecture.\n",
      "Similarly , layers 1 to 21 and 63 to 78 have an extremely low pruning ratio. The location of\n",
      "these layers is shown in Figure 7.4a . Comparing the locations of high and low pruning ratios,\n",
      "42\n",
      "(a) After first pruning step.\n",
      " (b) After last pruning step.\n",
      "Figure 7.2 :Pruning Ratio of each convolutional layer in YOLOv7 for different values of kafter\n",
      "(a) the first and (b) last pruning step.\n",
      "Figure 7.3 :Pruning Ratio of each convolutional layer in YOLOv7 after each pruning step at\n",
      "k= 0.9.\n",
      "the detections at higher scales are clearly prioritized over that at the lower scale. At the higher\n",
      "scales, the information can flow through several paths, but at the lowest scales there is only\n",
      "one path. It is precisely this path that is compressed very heavily . And there is no way to\n",
      "recover the information later in the network. This explains why k= 0.9with 5 pruning steps,\n",
      "which has the most extreme compression in these layers, performs worse than those with only\n",
      "two or three pruning steps.\n",
      "So, does one large pruning step work better than several smal ler pruning steps? No,\n",
      "from the comparison above it can be concluded that one large pruning step does not work\n",
      "better than several smaller pruning steps. At the same time, too many steps also hurt the\n",
      "performance. In this case, the optimal number of pruning steps is found to be two or three.\n",
      "The results in this chapter shows that YOLOv7-tiny outperforms YOLOv7 compressed with\n",
      "GBIP . But why is the selected compression method not able to outperform YOLOv7-tiny? One\n",
      "reason for this seems to be the fact that GBIP can only remove filters and not entire layers.\n",
      "Especially in the lower layers, any information that is removed cannot be recovered. At the\n",
      "same time some layers are left with only a few percent of their weights, making it impossible to\n",
      "pass all input information through to the next layer. This means that a layer with only a few\n",
      "weights will decrease the accuracy , and it would be better to remove the layer in its entirety .\n",
      "43\n",
      "(a) Blue: Low FPR.\n",
      " (b) Red: High FPR.\n",
      "Figure 7.4 :Location of layers with high and low FPR.\n",
      "Another way to avoid the loss of information in the earlier layers is to change the pruning\n",
      "threshold to allow less compression in the earlier layers and more in the later layers. Both\n",
      "changes would require a lot of extra testing and are left as recommendations for further re-\n",
      "search.\n",
      "44\n",
      "8 Conclusion\n",
      "In this final chapter the research questions are answered in Section 8.1 and some recommen-\n",
      "dations for further research are given in Section 8.2 .\n",
      "8.1 Research Questions\n",
      "8.1.1 Main Research Question\n",
      "The goal of this thesis is to see if compression of a large neural network can achieve better\n",
      "results than a hand-designed smaller network. Specifically:\n",
      "Can a state-of-the-art compression of YOLOv7 achieve higher accuracy than YOLOv7-\n",
      "tiny at the same number of floating-point operations?\n",
      "F rom the experiments done on YOLOv7 with GBIP , it turns out that YOLOv7-tiny outper-\n",
      "forms the compressed YOLOv7. The best performing model resulted from k= 0.95, which\n",
      "achieves a 11.3% FLOPs Pruning Ratio (FPR) with a fitness of 28.4%. YOLOv7-tiny (with a\n",
      "FPR of 13.2%) achieves a significantly higher fitness of 39.2%.\n",
      "Clearly , YOLOv7-tiny is preferable over YOLOv7 compressed with GBIP .\n",
      "8.1.2 Sub Questions\n",
      "8.1.2 Sub Questions\n",
      "1. What is the state-of-the-art in neural network compression?\n",
      "Section 2.3 gives some background information about different types of compression techniques.\n",
      "In general, these techniques can be divided into four categories: pruning, quantization, tensor\n",
      "decomposition and compact architectures. In Chapter 3, based on a search of the literature, a\n",
      "list of best performing compression methods is compiled. The current state-of-the-art achieves\n",
      "a FPR of 11.2% with ResNet-56 on CIF AR-10 [ 13] and 21.3 % FPR with ResNet-50 on Ima-\n",
      "geNet [ 66], both with an accuracy loss of less than 1%.\n",
      "2. What are the best networks for comparing compression methods?\n",
      "The main reason a network is suitable for comparing compression methods, is the fact that\n",
      "most recent papers report results using this network. F rom the literature, Section 3.1 found\n",
      "that the best networks are ResNet-50 trained on ImageNet and ResNet-56 trained on CIF AR-\n",
      "10.\n",
      "10.\n",
      "3. Which compression method is best suited for compressing YOLOv7?\n",
      "F rom the list of best performing compression methods (T able 3.1 ), two methods were selected:\n",
      "GBIP and KSE. These methods were selected after first removing methods with very high\n",
      "finetuning requirements.\n",
      "45\n",
      "Both GBIP and KSE have been tested on YOLOv7-tiny . Chapter 4goes into the im-\n",
      "plementation details of these methods for YOLOv7-tiny . Based on the results (Chapter 5),\n",
      "GBIP is chosen as best method for compressing YOLOv7. KSE could theoretically outperform\n",
      "GBIP , but as mentioned in Section 5.2 , these theoretical values were unattainable.\n",
      "4. What are the optimal hyperparameters to compress YOLOv7?\n",
      "In Chapter 6the hyperparameters are discussed. The learning rate is determined by a learning\n",
      "rate range test (Section 6.2.4 ). The learning rate is slowly increased from a very low to very\n",
      "high values while logging the loss. Using the derivative of this loss curve, the learning rate\n",
      "where the loss decreases fastest can be selected.\n",
      "Another hyperparameter that requires tuning is the number of training samples, or prun-\n",
      "ing batch size, used for calculating the importance score. Section 6.2.4 shows that after 210\n",
      "training samples there is almost no more change in which filters gets pruned.\n",
      "In the GBIP paper, three finetuning techniques are introduced to recover the accuracy\n",
      "of a pruned network: attention transfer, output transfer and adversarial game. An ablation\n",
      "study (Section 6.2.3 ) shows that for YOLOv7 a combination of attention and output transfer\n",
      "works best.\n",
      "5. Does one large pruning step work better than several smaller pruning steps?\n",
      "F rom the experiments on YOLOv7 (Chapter 7), it shows that using only one pruning step does\n",
      "not achieve good results. In that case, the layers of the network are very evenly compressed,\n",
      "ignoring any difference in relevance between layers. On the other hand, with five pruning steps\n",
      "the compression is very uneven over the layers, to the point that less than 1% of the parameters\n",
      "are left in several layers. F or the experiments in this thesis, two or three pruning steps results\n",
      "in the best accuracy . Incidentally , in the GBIP paper the number of pruning steps is also set\n",
      "to two or three, although no explanation is given for this choice.\n",
      "6. Why is the selected compression method not able to outperform YOLOv7-tiny?\n",
      "It is diﬀicult to give a definitive answer to this question. One very probable answer lies in the\n",
      "way GBIP compresses a network, namely by removing filters. YOLOv7-tiny has fewer layers\n",
      "than YOLOv7, but GBIP cannot remove entire layers, so all the information must still flow\n",
      "through each layer. This means that even if a layer is not necessary it still needs a way to pass\n",
      "the information on to the next layer and the weights cannot be set to zero.\n",
      "8.2 Recommendations\n",
      "Based on the experiments in this thesis there are several recommendations for future research.\n",
      "One obvious recommendation is further pursuing the optimization of KSE. In theory KSE\n",
      "should be able to outperform GBIP , but the optimization was unsuccessful. It is very well\n",
      "possible that with a custom CUDA implementation of KSE the results will be closer to the\n",
      "theory .\n",
      "Another recommendation is investigating a varying pruning threshold. Currently , the\n",
      "pruning threshold is the same in each layer of the network. However, if the compression in the\n",
      "early layers is too large, it is impossible to recover the lost information. Maybe by allowing\n",
      "lower compression in the early layers and more in the later layers, it is possible to achieve\n",
      "better accuracy at the same FPR.\n",
      "46\n",
      "The last recommendation is testing the removal of entire layers. Based on the performance\n",
      "of YOLOv7-tiny it is clear that a network with fewer layers can outperform a compressed\n",
      "network with more layers. However, allowing the removal of layers while compressing might\n",
      "result in a model that can outperform YOLOv7-tiny .\n",
      "47\n",
      "A Architectures\n",
      "A.1 YOLOv7-tiny\n",
      "Figure A.1 :YOLOv7-tiny architecture. Name of each block is given on top of the box; output\n",
      "dimensions are given below; numbers on the right indicate index of last layer in the block.\n",
      "In the CSPSPP block, the numbers under MaxPool indicate kernel and padding size.\n",
      "49\n",
      "A.2 YOLOv7\n",
      "Figure A.2 :YOLOv7 architecture. Name of each block is given on top of the box; output\n",
      "dimensions are given below; numbers on the right indicate index of last layer in the block.\n",
      "In the SPPFCSP block, the numbers under MaxPool indicate kernel and padding size. 2-4\n",
      "and 1-4 ELAN blocks are show in Figure 2.6 .\n",
      "50\n",
      "Bibliography\n",
      "[1] J. Steinbrener and P . Desai, “Comparison of deep learning architectures on embedded\n",
      "devices and generalized fixed-point conversion algorithm,” pp. 343–347, 06 2019.\n",
      "[2] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature , vol. 521, pp. 436–44, 05\n",
      "2015.\n",
      "[3] V. Sze, Y.-H. Chen, T.-J. Y ang, and J. Emer, “Eﬀicient processing of deep neural net-\n",
      "works: A tutorial and survey ,” Proceedings of the IEEE , vol. 105, 03 2017.\n",
      "[4] W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge computing: Vision and challenges,”\n",
      "IEEE Internet of Things Journal , vol. 3, no. 5, pp. 637–646, 2016.\n",
      "[5] R. Pilipovic, P . Bulić, and V. Risojević, “Compression of convolutional neural networks:\n",
      "A short survey ,” pp. 1–6, 03 2018.\n",
      "[6] R. Sanchez-Iborra and A. F. Skarmeta, “Tinyml-enabled frugal smart objects: Challenges\n",
      "and opportunities,” IEEE Circuits and Systems Magazine , vol. 20, no. 3, pp. 4–18, 2020.\n",
      "[7] L. Dutta and S. Bharali, “Tinyml meets iot: A comprehensive survey ,” Internet of Things ,\n",
      "vol. 16, p. 100461, 10 2021.\n",
      "[8] F. Götz, “The data deluge: What do we do with the data\n",
      "generated by avs?. ” https://blogs.sw.siemens.com/polarion/\n",
      "the-data-deluge-what-do-we-do-with-the-data-generated-by-avs/ , Jan 2021.\n",
      "Accessed: 2023-11-06.\n",
      "[9] K. Nan, S. Liu, J. Du, and H. Liu, “Deep model compression for mobile platforms: A\n",
      "survey ,” T singhua Science and T echnology , vol. 24, pp. 677–693, 12 2019.\n",
      "[10] Y. Cheng, D. W ang, P . Zhou, and T. Zhang, “A survey of model compression and accel-\n",
      "eration for deep neural networks,” 10 2017.\n",
      "[11] K. Ullrich, E. Meeds, and M. W elling, “Soft weight-sharing for neural network compres-\n",
      "sion,” 02 2017.\n",
      "[12] J. Bi and S. Gunn, “Sparse deep neural networks for embedded intelligence,” pp. 30–38,\n",
      "11 2018.\n",
      "11 2018.\n",
      "[13] W. Zhang and Z. W ang, “F pfs: Filter-level pruning via distance weight measuring filter\n",
      "similarity ,” Neurocomputing , vol. 512, pp. 40–51, 2022.\n",
      "[14] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. W ang, T. W eyand, M. Andreetto,\n",
      "and H. Adam, “Mobilenets: Eﬀicient convolutional neural networks for mobile vision\n",
      "applications,” 04 2017.\n",
      "[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,”\n",
      "2015.\n",
      "51\n",
      "[16] C.-Y. W ang, A. Bochkovskiy , and H.-Y. M. Liao, “Y olov7: T rainable bag-of-freebies sets\n",
      "new state-of-the-art for real-time object detectors,” 2022.\n",
      "[17] V. Nair and G. Hinton, “Rectified linear units improve restricted boltzmann machines,”\n",
      "vol. 27, pp. 807–814, 06 2010.\n",
      "[18] S. R. Dubey , S. K. Singh, and B. B. Chaudhuri, “A comprehensive survey and performance\n",
      "analysis of activation functions in deep learning,” CoRR , vol. abs/2109.14545, 2021.\n",
      "[19] D. Hendrycks and K. Gimpel, “Bridging nonlinearities and stochastic regularizers with\n",
      "gaussian error linear units,” CoRR , vol. abs/1606.08415, 2016.\n",
      "[20] R. J. T an, “Breaking down mean average precision (map). ” https://\n",
      "towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52 ,\n",
      "Mar 2019. Accessed: 2023-11-06.\n",
      "[21] M. Everingham, L. V an Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal\n",
      "visual object classes (voc) challenge,” International Journal of Computer Vision , vol. 88,\n",
      "pp. 303–338, June 2010.\n",
      "[22] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P . Perona,\n",
      "D. Ramanan, P . Dollár, and C. L. Zitnick, “Microsoft COCO: common objects in context,”\n",
      "CoRR , vol. abs/1405.0312, 2014.\n",
      "[23] “Object detection on pascal voc 2012. ” https://paperswithcode.com/sota/\n",
      "object-detection-on-pascal-voc-2012 . Accessed: 2023-11-06.\n",
      "[24] Z. Zhao, P . Zheng, S. Xu, and X. W u, “Object detection with deep learning: A review,”\n",
      "CoRR , vol. abs/1807.05511, 2018.\n",
      "[25] “Object detection on coco test-dev. ” https://paperswithcode.com/sota/\n",
      "object-detection-on-coco . Accessed: 2023-11-06.\n",
      "[26] R. Kaur and S. Singh, “A comprehensive review of object detection with deep learning,”\n",
      "Digital Signal Processing , vol. 132, p. 103812, 2023.\n",
      "[27] S. Ren, K. He, R. B. Girshick, and J. Sun, “F aster R-CNN: towards real-time object\n",
      "detection with region proposal networks,” CoRR , vol. abs/1506.01497, 2015.\n",
      "[28] R. B. Girshick, “F ast R-CNN,” CoRR , vol. abs/1504.08083, 2015.\n",
      "[29] A. Benali Amjoud and M. Amrouch, “Object detection using deep learning, cnns and\n",
      "vision transformers: A review,” IEEE Access , vol. PP , pp. 1–1, 01 2023.\n",
      "[30] W. Liu, D. Anguelov, D. Erhan, C. Szegedy , S. E. Reed, C. F u, and A. C. Berg, “SSD:\n",
      "single shot multibox detector,” CoRR , vol. abs/1512.02325, 2015.\n",
      "[31] J. Redmon, S. K. Divvala, R. B. Girshick, and A. F arhadi, “Y ou only look once: Unified,\n",
      "real-time object detection,” CoRR , vol. abs/1506.02640, 2015.\n",
      "[32] T. Lin, P . Dollár, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, “F eature\n",
      "pyramid networks for object detection,” CoRR , vol. abs/1612.03144, 2016.\n",
      "[33] C.-Y. W ang, H.-Y. M. Liao, and I.-H. Y eh, “Designing network design strategies through\n",
      "gradient path analysis,” 2022.\n",
      "52\n",
      "[34] A. Bochkovskiy , C. W ang, and H. M. Liao, “Y olov4: Optimal speed and accuracy of object\n",
      "detection,” CoRR , vol. abs/2004.10934, 2020.\n",
      "[35] H. Zhang, M. Cissé, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical risk\n",
      "minimization,” CoRR , vol. abs/1710.09412, 2017.\n",
      "[36] T. Devries and G. W. T aylor, “Improved regularization of convolutional neural networks\n",
      "with cutout,” CoRR , vol. abs/1708.04552, 2017.\n",
      "[37] Z. Zheng, P . W ang, W. Liu, J. Li, R. Y e, and D. Ren, “Distance-iou loss: F aster and\n",
      "better learning for bounding box regression,” CoRR , vol. abs/1911.08287, 2019.\n",
      "[38] H. Salehinejad and S. V alaee, “Ising-dropout: A regularization method for training and\n",
      "compression of deep neural networks,” 05 2019.\n",
      "[39] J. Chang and S. Jin, “Prune deep neural networks with the modified l1/2 penalty ,” IEEE\n",
      "Access , vol. PP , pp. 1–1, 12 2018.\n",
      "[40] M. Alnemari and N. Bagherzadeh, “Eﬀicient deep neural networks for edge computing,”\n",
      "pp. 1–7, 07 2019.\n",
      "[41] P . Merolla, R. Appuswamy , J. Arthur, S. Esser, and D. Modha, “Deep neural networks\n",
      "are robust to weight binarization and other non-linear distortions,” 06 2016.\n",
      "[42] M. Courbariaux, I. Hubara, D. Soudry , R. El-Y aniv, and Y. Bengio, “Binarized neural\n",
      "networks: T raining deep neural networks with weights and activations constrained to +1\n",
      "or -1,” 02 2016.\n",
      "or -1,” 02 2016.\n",
      "[43] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . MIT Press, 2016. http:\n",
      "//www.deeplearningbook.org .\n",
      "[44] Q. W u, W. Li, X. Lu, H. Zhang, H. Luo, and C. Lei, “A deep learning model compression\n",
      "algorithm based on optimal clustering,” p. 171, 05 2019.\n",
      "[45] S. Han, H. Mao, and W. Dally , “Deep compression: Compressing deep neural networks\n",
      "with pruning, trained quantization and huffman coding,” 10 2016.\n",
      "[46] J. Xue, J. Li, and Y. Gong, “Restructuring of deep neural network acoustic models with\n",
      "singular value decomposition,” Proceedings of the Annual Conference of the International\n",
      "Speech Communication Association, INTERSPEECH , pp. 2365–2369, 01 2013.\n",
      "[47] Y.-D. Kim, E. Park, S. Y oo, T. Choi, L. Y ang, and D. Shin, “Compression of deep\n",
      "convolutional neural networks for fast and low power mobile applications,” 05 2016.\n",
      "[48] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky , “Speeding-up con-\n",
      "volutional neural networks using fine-tuned cp-decomposition,” 12 2014.\n",
      "[49] A. Novikov, D. Podoprikhin, A. Osokin, and D. V etrov, “T ensorizing neural networks,”\n",
      "09 2015.\n",
      "[50] F. Iandola, S. Han, M. Moskewicz, K. Ashraf, W. Dally , and K. Keutzer, “Squeezenet:\n",
      "Alexnet-level accuracy with 50x fewer parameters and <0.5mb model size,” 02 2016.\n",
      "[51] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” 03\n",
      "2015.\n",
      "53\n",
      "[52] M. V. de Carvalho and M. Pratama, “Improving shallow neural network by compressing\n",
      "deep neural network,” pp. 1382–1387, 11 2018.\n",
      "[53] C.-J. Chen, K.-C. Chen, and M.-c. Martin-Kuo, “Acceleration of neural network model\n",
      "execution on embedded systems,” pp. 1–3, 04 2018.\n",
      "[54] O. Russakovsky , J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy ,\n",
      "A. Khosla, M. Bernstein, A. C. Berg, and L. F ei-F ei, “ImageNet Large Scale Visual\n",
      "Recognition Challenge,” International Journal of Computer Vision (IJCV) , vol. 115, no. 3,\n",
      "pp. 211–252, 2015.\n",
      "[55] A. Krizhevsky , “Learning multiple layers of features from tiny images,” University of\n",
      "T oronto , 05 2012.\n",
      "[56] “Image classification on imagenet. ” https://paperswithcode.com/sota/\n",
      "image-classification-on-imagenet . Accessed: 2023-11-06.\n",
      "[57] “Image classification on cifar-10. ” https://paperswithcode.com/sota/\n",
      "image-classification-on-cifar-10 . Accessed: 2023-11-06.\n",
      "[58] Y. Lecun, L. Bottou, Y. Bengio, and P . Haffner, “Gradient-based learning applied to\n",
      "document recognition,” Proceedings of the IEEE , vol. 86, no. 11, pp. 2278–2324, 1998.\n",
      "[59] S. V adera and S. Ameen, “Methods for pruning deep neural networks,” CoRR ,\n",
      "vol. abs/2011.00241, 2020.\n",
      "[60] W. Zhang and Z. W ang, “Pca-pruner: Filter pruning by principal component analysis,”\n",
      "J. Intel l. F uzzy Syst. , vol. 43, pp. 4803–4813, jan 2022.\n",
      "[61] M. U. Haider and M. T aj, “Comprehensive online network pruning via learnable scaling\n",
      "factors,” CoRR , vol. abs/2010.02623, 2020.\n",
      "[62] C. Sarvani, M. Ghorai, S. R. Dubey , and S. S. Basha, “HRel: Filter pruning based on\n",
      "high relevance between activation maps and class labels,” Neural Networks , vol. 147,\n",
      "pp. 186–197, mar 2022.\n",
      "[63] W. Gao, Y. Guo, S. Ma, G. Li, and S. Kwong, “Eﬀicient neural network compression\n",
      "inspired by compressive sensing,” IEEE T ransactions on Neural Networks and Learning\n",
      "Systems , pp. 1–15, 2022.\n",
      "[64] J. Chang, Y. Lu, P . Xue, Y. Xu, and Z. W ei, “Global balanced iterative pruning for\n",
      "eﬀicient convolutional neural networks,” Neural Computing and Applications , Jul 2022.\n",
      "[65] J. Zhu and J. Pei, “Progressive kernel pruning cnn compression method with an adjustable\n",
      "input channel,” Applied Intel ligence , vol. 52, pp. 1–22, 07 2022.\n",
      "[66] Y. Li, S. Lin, B. Zhang, J. Liu, D. S. Doermann, Y. W u, F. Huang, and R. Ji,\n",
      "“Exploiting kernel sparsity and entropy for interpretable CNN compression,” CoRR ,\n",
      "vol. abs/1812.04368, 2018.\n",
      "[67] K. Zhao, A. Jain, and M. Zhao, “Iterative activation-based structured pruning,” CoRR ,\n",
      "vol. abs/2201.09881, 2022.\n",
      "[68] M. R. Ganesh, J. J. Corso, and S. Y. Sekeh, “MINT: deep network compression via mutual\n",
      "information-based neuron trimming,” CoRR , vol. abs/2003.08472, 2020.\n",
      "54\n",
      "[69] H. Hu, R. Peng, Y.-W. T ai, and C.-K. T ang, “Network trimming: A data-driven neuron\n",
      "pruning approach towards eﬀicient deep architectures,” 07 2016.\n",
      "[70] S. Zagoruyko and N. Komodakis, “Paying more attention to attention: Improv-\n",
      "ing the performance of convolutional neural networks via attention transfer,” CoRR ,\n",
      "vol. abs/1612.03928, 2016.\n",
      "[71] S. Lin, R. Ji, C. Y an, B. Zhang, L. Cao, Q. Y e, F. Huang, and D. S. Doermann,\n",
      "“T owards optimal structured CNN pruning via generative adversarial learning,” CoRR ,\n",
      "vol. abs/1903.09291, 2019.\n",
      "[72] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury , G. Chanan, T. Killeen, Z. Lin,\n",
      "N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Y ang, Z. De Vito, M. Raison, A. T e-\n",
      "jani, S. Chilamkurthy , B. Steiner, L. F ang, J. Bai, and S. Chintala, “Pytorch: An imper-\n",
      "ative style, high-performance deep learning library ,” in Advances in Neural Information\n",
      "Processing Systems 32 , pp. 8024–8035, Curran Associates, Inc., 2019.\n",
      "[73] L. N. Smith, “No more pesky learning rate guessing games,” CoRR , vol. abs/1506.01186,\n",
      "2015.\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for t in text_chunks:\n",
    "    print(t.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66bc7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [t.page_content for t in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f187b04b-7158-4982-a7e3-d12304200f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# docsearch = PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embedding, index_name=index_name)\n",
    "docsearch = PineconeVectorStore.from_documents(text_chunks, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c883031-0c96-4e52-b11a-6ab22a669147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1e69db232c0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a787292-68bc-463c-b370-83dcea91fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001E69DB232C0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0683401a-f63a-4aa5-8526-7a6c8c186e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae8bb955-37fc-4dd7-a0f2-7779f1517de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02b3b0a8-e97a-431a-976c-dc7444786d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%'),\n",
       " Document(page_content='YOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%'),\n",
       " Document(page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known'),\n",
       " Document(page_content='YOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c10b8de-1f56-4e87-949d-2e6d65c80bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alams\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ee5c366-ac15-4c0d-90f7-9bd08e47feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ff998c5-1012-4c3d-a24a-ce0f01f05740",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"YOLOv7 outperforms which models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd11b268-34d7-4bdb-93fd-9acf2a874499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alams\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' YOLOv7 outperforms YOLOv7-tiny and YOLOv7-W6, as well as the original YOLO network.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b67833-a1e9-4a76-834f-0504ca92d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "while True:\n",
    "  user_input = input(f\"Input Prompt: \")\n",
    "  if user_input == 'exit':\n",
    "    print('Exiting')\n",
    "    sys.exit()\n",
    "  if user_input == '':\n",
    "    continue\n",
    "  result = qa({'query': user_input})\n",
    "  print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef2ee-a82f-499c-8bee-24e7972ae586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
